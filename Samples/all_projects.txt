################################################################
0_Introduction/UnifiedMemoryStreams
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Executing tasks on host / device
Task [0], thread [0] executing on device (396)
Task [2], thread [1] executing on host (64)
Task [3], thread [3] executing on device (304)
Task [1], thread [2] executing on device (990)
Task [4], thread [1] executing on device (972)
Task [5], thread [3] executing on device (550)
Task [6], thread [1] executing on device (881)
Task [7], thread [0] executing on device (226)
Task [8], thread [1] executing on host (93)
Task [9], thread [2] executing on device (589)
Task [10], thread [1] executing on device (309)
Task [11], thread [2] executing on device (703)
Task [12], thread [1] executing on host (64)
Task [14], thread [3] executing on device (722)
Task [13], thread [1] executing on device (777)
Task [15], thread [2] executing on device (777)
Task [16], thread [1] executing on device (731)
Task [17], thread [3] executing on device (185)
Task [18], thread [2] executing on device (815)
Task [20], thread [1] executing on host (71)
Task [19], thread [2] executing on device (923)
Task [21], thread [1] executing on device (113)
Task [22], thread [1] executing on device (362)
Task [23], thread [2] executing on device (825)
Task [24], thread [2] executing on device (986)
Task [25], thread [1] executing on device (185)
Task [26], thread [2] executing on device (145)
Task [27], thread [0] executing on host (64)
Task [28], thread [1] executing on device (740)
Task [29], thread [2] executing on device (683)
Task [30], thread [3] executing on device (234)
Task [31], thread [1] executing on device (179)
Task [32], thread [2] executing on device (210)
Task [35], thread [2] executing on device (957)
Task [33], thread [3] executing on device (985)
Task [36], thread [3] executing on device (741)
Task [37], thread [3] executing on device (638)
Task [38], thread [2] executing on device (965)
Task [39], thread [3] executing on device (335)
Task [34], thread [0] executing on device (233)
All Done!
****************************************************************
################################################################
0_Introduction/asyncAPI
################################################################
make: Nothing to be done for 'all'.
[./asyncAPI] - Starting...
GPU Device 0: "Ampere" with compute capability 8.6

CUDA device [NVIDIA GeForce RTX 3060]
time spent executing by the GPU: 11.41
time spent by CPU in CUDA calls: 8.76
CPU executed 56851 iterations while waiting for GPU to finish
****************************************************************
################################################################
0_Introduction/c++11_cuda
################################################################
>>> GCC Version is greater or equal to 4.7.0 <<<
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Read 3223503 byte corpus from ./warandpeace.txt
counted 107310 instances of 'x', 'y', 'z', or 'w' in "./warandpeace.txt"
****************************************************************
################################################################
0_Introduction/clock
################################################################
make: Nothing to be done for 'all'.
CUDA Clock sample
GPU Device 0: "Ampere" with compute capability 8.6

Average clocks/block = 2094.921875
****************************************************************
################################################################
0_Introduction/clock_nvrtc
################################################################
make: Nothing to be done for 'all'.
CUDA Clock sample
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> GPU Device has SM 8.6 compute capability
Average clocks/block = 2098.296875
****************************************************************
################################################################
0_Introduction/concurrentKernels
################################################################
make: Nothing to be done for 'all'.
[./concurrentKernels] - Starting...
GPU Device 0: "Ampere" with compute capability 8.6

> Detected Compute SM 8.6 hardware with 28 multi-processors
Expected time for serial execution of 8 kernels = 0.080s
Expected time for concurrent execution of 8 kernels = 0.010s
Measured time for sample = 0.012s
Test passed
****************************************************************
################################################################
0_Introduction/cppIntegration
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Hello World.
Hello World.
****************************************************************
################################################################
0_Introduction/cppOverload
################################################################
make: Nothing to be done for 'all'.
C++ Function Overloading starting...
Device Count: 1
GPU Device 0: "Ampere" with compute capability 8.6

Shared Size:   1024
Constant Size: 0
Local Size:    0
Max Threads Per Block: 1024
Number of Registers: 12
PTX Version: 86
Binary Version: 86
simple_kernel(const int *pIn, int *pOut, int a) PASSED

Shared Size:   2048
Constant Size: 0
Local Size:    0
Max Threads Per Block: 1024
Number of Registers: 12
PTX Version: 86
Binary Version: 86
simple_kernel(const int2 *pIn, int *pOut, int a) PASSED

Shared Size:   2048
Constant Size: 0
Local Size:    0
Max Threads Per Block: 1024
Number of Registers: 16
PTX Version: 86
Binary Version: 86
simple_kernel(const int *pIn1, const int *pIn2, int *pOut, int a) PASSED

****************************************************************
################################################################
0_Introduction/cudaOpenMP
################################################################
make: Nothing to be done for 'all'.
./cudaOpenMP Starting...

number of host CPUs:	8
number of CUDA devices:	1
   0: NVIDIA GeForce RTX 3060
---------------------------
CPU thread 0 (of 1) uses CUDA device 0
---------------------------
****************************************************************
################################################################
0_Introduction/fp16ScalarProduct
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Result native operators	: 647041.000000 
Result intrinsics	: 647041.000000 
&&&& fp16ScalarProduct PASSED
****************************************************************
################################################################
0_Introduction/matrixMul
################################################################
make: Nothing to be done for 'all'.
[Matrix Multiply Using CUDA] - Starting...
GPU Device 0: "Ampere" with compute capability 8.6

MatrixA(320,320), MatrixB(640,320)
Computing result using CUDA Kernel...
done
Performance= 947.94 GFlop/s, Time= 0.138 msec, Size= 131072000 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
****************************************************************
################################################################
0_Introduction/matrixMulDrv
################################################################
make: Nothing to be done for 'all'.
[ matrixMulDrv (Driver API) ]
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> GPU Device has SM 8.6 compute capability
  Total amount of global memory:     12884246528 bytes
> findModulePath found file at <./matrixMul_kernel64.fatbin>
> initCUDA loading module: <./matrixMul_kernel64.fatbin>
> 16 block size selected
Processing time: 3.122000 (ms)
Checking computed result for correctness: Result = PASS

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
****************************************************************
################################################################
0_Introduction/matrixMulDynlinkJIT
################################################################
make: Nothing to be done for 'all'.
[ matrixMulDynlinkJIT (CUDA dynamic linking) ]
> Device 0: "NVIDIA GeForce RTX 3060" with Compute 8.6 capability
> Compiling CUDA module
> PTX JIT log:

Test run success!
****************************************************************
################################################################
0_Introduction/matrixMul_nvrtc
################################################################
cp "/usr/local/cuda/include/cooperative_groups.h" .
cp -r "/usr/local/cuda/include/cooperative_groups" .
[Matrix Multiply Using CUDA] - Starting...
MatrixA(320,320), MatrixB(640,320)
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060

 compilation log ---
./cooperative_groups/details/info.h(50): catastrophic error: cannot open source file "nv/target"
  #include <nv/target>
                      ^

1 catastrophic error detected in the compilation of "./matrixMul_kernel.cu".
Compilation terminated.

 end log ---

error: nvrtcCompileProgram failed with error NVRTC_ERROR_COMPILATION****************************************************************
################################################################
0_Introduction/mergeSort
################################################################
make: Nothing to be done for 'all'.
./mergeSort Starting...

GPU Device 0: "Ampere" with compute capability 8.6

Allocating and initializing host arrays...

Allocating and initializing CUDA arrays...

Initializing GPU merge sort...
Running GPU merge sort...
Time: 6.250000 ms
Reading back GPU merge sort results...
Inspecting the results...
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: stable!
Shutting down...
****************************************************************
################################################################
0_Introduction/simpleAWBarrier
################################################################
>>> GCC Version is greater or equal to  5.1.0 <<<
make: Nothing to be done for 'all'.
./simpleAWBarrier starting...
GPU Device 0: "Ampere" with compute capability 8.6

Launching normVecByDotProductAWBarrier kernel with numBlocks = 56 blockSize = 768
Result = PASSED
./simpleAWBarrier completed, returned OK
****************************************************************
################################################################
0_Introduction/simpleAssert
################################################################
make: Nothing to be done for 'all'.
simpleAssert.cu:63: void testKernel(int): block: [1,0,0], thread: [28,0,0] Assertion `gtid < N` failed.
simpleAssert.cu:63: void testKernel(int): block: [1,0,0], thread: [29,0,0] Assertion `gtid < N` failed.
simpleAssert.cu:63: void testKernel(int): block: [1,0,0], thread: [30,0,0] Assertion `gtid < N` failed.
simpleAssert.cu:63: void testKernel(int): block: [1,0,0], thread: [31,0,0] Assertion `gtid < N` failed.
simpleAssert starting...
OS_System_Type.release = 5.15.153.1-microsoft-standard-WSL2
OS Info: <#1 SMP Fri Mar 29 23:14:13 UTC 2024>

GPU Device 0: "Ampere" with compute capability 8.6

Launch kernel to generate assertion failures

-- Begin assert output


-- End assert output

Device assert failed as expected, CUDA error message is: device-side assert triggered

simpleAssert completed, returned OK
****************************************************************
################################################################
0_Introduction/simpleAssert_nvrtc
################################################################
make: Nothing to be done for 'all'.
./simpleAssert_kernel.cu:37: void testKernel(int): block: [1,0,0], thread: [28,0,0] Assertion `gtid < N` failed.
./simpleAssert_kernel.cu:37: void testKernel(int): block: [1,0,0], thread: [29,0,0] Assertion `gtid < N` failed.
./simpleAssert_kernel.cu:37: void testKernel(int): block: [1,0,0], thread: [30,0,0] Assertion `gtid < N` failed.
./simpleAssert_kernel.cu:37: void testKernel(int): block: [1,0,0], thread: [31,0,0] Assertion `gtid < N` failed.
simpleAssert_nvrtc starting...
Launch kernel to generate assertion failures
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> GPU Device has SM 8.6 compute capability

-- Begin assert output


-- End assert output

Device assert failed as expected
****************************************************************
################################################################
0_Introduction/simpleAtomicIntrinsics
################################################################
make: Nothing to be done for 'all'.
simpleAtomicIntrinsics starting...
GPU Device 0: "Ampere" with compute capability 8.6

Processing time: 6.184000 (ms)
simpleAtomicIntrinsics completed, returned OK
****************************************************************
################################################################
0_Introduction/simpleAtomicIntrinsics_nvrtc
################################################################
make: Nothing to be done for 'all'.
simpleAtomicIntrinsics_nvrtc starting...
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> GPU Device has SM 8.6 compute capability
Processing time: 11.040000 (ms)
simpleAtomicIntrinsics_nvrtc completed, returned OK
****************************************************************
################################################################
0_Introduction/simpleAttributes
################################################################
make: Nothing to be done for 'all'.
./simpleAttributes Starting...

GPU Device 0: "Ampere" with compute capability 8.6

Processing time: 29504.806641 (ms)
****************************************************************
################################################################
0_Introduction/simpleCUDA2GL
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at main.cpp:174 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer(pbo_resource, *pbo, cudaGraphicsMapFlagsNone)" 
./simpleCUDA2GL Starting...

(Interactive OpenGL Demo)
GPU Device 0: "Ampere" with compute capability 8.6

****************************************************************
################################################################
0_Introduction/simpleCallback
################################################################
make: Nothing to be done for 'all'.
Starting simpleCallback
Found 1 CUDA capable GPUs
GPU[0] NVIDIA GeForce RTX 3060 supports SM 8.6, capable GPU Callback Functions
1 GPUs available to run Callback Functions
Starting 8 heterogeneous computing workloads
Total of 8 workloads finished:
Success
****************************************************************
################################################################
0_Introduction/simpleCooperativeGroups
################################################################
make: Nothing to be done for 'all'.

Launching a single block with 64 threads...

 Sum of all ranks 0..63 in threadBlockGroup is 2016 (expected 2016)

 Now creating 4 groups, each of size 16 threads:

   Sum of all ranks 0..15 in this tiledPartition16 group is 120 (expected 120)
   Sum of all ranks 0..15 in this tiledPartition16 group is 120 (expected 120)
   Sum of all ranks 0..15 in this tiledPartition16 group is 120 (expected 120)
   Sum of all ranks 0..15 in this tiledPartition16 group is 120 (expected 120)

...Done.

****************************************************************
################################################################
0_Introduction/simpleCubemapTexture
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

CUDA device [NVIDIA GeForce RTX 3060] has 28 Multi-Processors SM 8.6
Covering Cubemap data array of 64~3 x 1: Grid size is 8 x 8, each block has 8 x 8 threads
Processing time: 0.086 msec
285.77 Mtexlookups/sec
Comparing kernel output to expected data
****************************************************************
################################################################
0_Introduction/simpleDrvRuntime
################################################################
make: Nothing to be done for 'all'.
simpleDrvRuntime..
GPU Device 0: "Ampere" with compute capability 8.6

> findModulePath found file at <./vectorAdd_kernel64.fatbin>
> initCUDA loading module: <./vectorAdd_kernel64.fatbin>
Result = PASS
****************************************************************
################################################################
0_Introduction/simpleHyperQ
################################################################
make: Nothing to be done for 'all'.
starting hyperQ...
GPU Device 0: "Ampere" with compute capability 8.6

> Detected Compute SM 8.6 hardware with 28 multi-processors
Expected time for serial execution of 32 sets of kernels is between approx. 0.330s and 0.640s
Expected time for fully concurrent execution of 32 sets of kernels is approx. 0.020s
Measured time for sample = 0.062s
****************************************************************
################################################################
0_Introduction/simpleIPC
################################################################
make: Nothing to be done for 'all'.
Process 0: Starting on device 0...
Step 0 done
Process 0: verifying...
Process 0 complete!
****************************************************************
################################################################
0_Introduction/simpleLayeredTexture
################################################################
make: Nothing to be done for 'all'.
[simpleLayeredTexture] - Starting...
GPU Device 0: "Ampere" with compute capability 8.6

CUDA device [NVIDIA GeForce RTX 3060] has 28 Multi-Processors SM 8.6
Covering 2D data array of 512 x 512: Grid size is 64 x 64, each block has 8 x 8 threads
Processing time: 0.081 msec
16181.73 Mtexlookups/sec
Comparing kernel output to expected data
****************************************************************
################################################################
0_Introduction/simpleMPI
################################################################
make: Nothing to be done for 'all'.
Running on 1 nodes
Average of square roots is: 0.667242
PASSED
****************************************************************
################################################################
0_Introduction/simpleMultiCopy
################################################################
make: Nothing to be done for 'all'.
[simpleMultiCopy] - Starting...
> Using CUDA device [0]: NVIDIA GeForce RTX 3060
[NVIDIA GeForce RTX 3060] has 28 MP(s) x 128 (Cores/MP) = 3584 (Cores)
> Device name: NVIDIA GeForce RTX 3060
> CUDA Capability 8.6 hardware with 28 multi-processors
> scale_factor = 1.00
> array_size   = 4194304


Relevant properties of this CUDA device
(X) Can overlap one CPU<>GPU data transfer with GPU kernel execution (device property "deviceOverlap")
(X) Can overlap two CPU<>GPU data transfers with GPU kernel execution
    (Compute Capability >= 2.0 AND (Tesla product OR Quadro 4000/5000/6000/K5000)

Measured timings (throughput):
 Memcpy host to device	: 1.294336 ms (12.962026 GB/s)
 Memcpy device to host	: 1.277952 ms (13.128206 GB/s)
 Kernel			: 0.110592 ms (1517.037034 GB/s)

Theoretical limits for speedup gained from overlapped data transfers:
No overlap at all (transfer-kernel-transfer): 2.682880 ms 
Compute can overlap with one transfer: 2.572288 ms
Compute can overlap with both data transfers: 1.294336 ms

Average measured timings over 10 repetitions:
 Avg. time when execution fully serialized	: 2.839245 ms
 Avg. time when overlapped using 4 streams	: 2.619187 ms
 Avg. speedup gained (serialized - overlapped)	: 0.220058 ms

Measured throughput:
 Fully serialized execution		: 11.818083 GB/s
 Overlapped using 4 streams		: 12.811010 GB/s
****************************************************************
################################################################
0_Introduction/simpleMultiGPU
################################################################
make: Nothing to be done for 'all'.
Starting simpleMultiGPU
CUDA-capable device count: 1
Generating input data...

Computing with 1 GPUs...
  GPU Processing time: 18.157000 (ms)

Computing with Host CPU...

Comparing GPU and Host CPU results...
  GPU sum: 16777296.000000
  CPU sum: 16777294.395033
  Relative difference: 9.566307E-08 

****************************************************************
################################################################
0_Introduction/simpleOccupancy
################################################################
make: Nothing to be done for 'all'.
starting Simple Occupancy

[ Manual configuration with 32 threads per block ]
Potential occupancy: 33.3333%
Elapsed time: 1.93024ms

[ Automatic, occupancy-based configuration ]
Suggested block size: 768
Minimum grid size for maximum occupancy: 56
Potential occupancy: 100%
Elapsed time: 0.02816ms

Test PASSED

****************************************************************
################################################################
0_Introduction/simpleP2P
################################################################
make: Nothing to be done for 'all'.
[./simpleP2P] - Starting...
Checking for multiple GPUs...
CUDA-capable device count: 1
Two or more GPUs with Peer-to-Peer access capability are required for ./simpleP2P.
Waiving test.
****************************************************************
################################################################
0_Introduction/simplePitchLinearTexture
################################################################
make: Nothing to be done for 'all'.
simplePitchLinearTexture starting...

GPU Device 0: "Ampere" with compute capability 8.6


Bandwidth (GB/s) for pitch linear: 2.26e+02; for array: 3.13e+02

Texture fetch rate (Mpix/s) for pitch linear: 2.83e+04; for array: 3.91e+04

simplePitchLinearTexture completed, returned OK
****************************************************************
################################################################
0_Introduction/simplePrintf
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Device 0: "NVIDIA GeForce RTX 3060" with Compute 8.6 capability
printf() is called. Output:

[3, 0]:		Value is:10
[3, 1]:		Value is:10
[3, 2]:		Value is:10
[3, 3]:		Value is:10
[3, 4]:		Value is:10
[3, 5]:		Value is:10
[3, 6]:		Value is:10
[3, 7]:		Value is:10
[2, 0]:		Value is:10
[2, 1]:		Value is:10
[2, 2]:		Value is:10
[2, 3]:		Value is:10
[2, 4]:		Value is:10
[2, 5]:		Value is:10
[2, 6]:		Value is:10
[2, 7]:		Value is:10
[0, 0]:		Value is:10
[0, 1]:		Value is:10
[0, 2]:		Value is:10
[0, 3]:		Value is:10
[0, 4]:		Value is:10
[0, 5]:		Value is:10
[0, 6]:		Value is:10
[0, 7]:		Value is:10
[1, 0]:		Value is:10
[1, 1]:		Value is:10
[1, 2]:		Value is:10
[1, 3]:		Value is:10
[1, 4]:		Value is:10
[1, 5]:		Value is:10
[1, 6]:		Value is:10
[1, 7]:		Value is:10
****************************************************************
################################################################
0_Introduction/simpleSeparateCompilation
################################################################
make: Nothing to be done for 'all'.
simpleSeparateCompilation starting...
GPU Device 0: "Ampere" with compute capability 8.6

simpleSeparateCompilation completed, returned OK
****************************************************************
################################################################
0_Introduction/simpleStreams
################################################################
make: Nothing to be done for 'all'.
[ simpleStreams ]

Device synchronization method set to = 0 (Automatic Blocking)
Setting reps to 100 to demonstrate steady state

> GPU Device 0: "Ampere" with compute capability 8.6

Device: <NVIDIA GeForce RTX 3060> canMapHostMemory: Yes
> CUDA Capable: SM 8.6 hardware
> 28 Multiprocessor(s) x 128 (Cores/Multiprocessor) = 3584 (Cores)
> scale_factor = 1.0000
> array_size   = 16777216

> Using CPU/GPU Device Synchronization method (cudaDeviceScheduleAuto)
> mmap() allocating 64.00 Mbytes (generic page-aligned system memory)
> cudaHostRegister() registering 64.00 Mbytes of generic allocated system memory

Starting Test
memcopy:	5.21
kernel:		4.54
non-streamed:	5.66
4 streams:	5.74
-------------------------------
****************************************************************
################################################################
0_Introduction/simpleSurfaceWrite
################################################################
make: Nothing to be done for 'all'.
simpleSurfaceWrite starting...
GPU Device 0: "Ampere" with compute capability 8.6

CUDA device [NVIDIA GeForce RTX 3060] has 28 Multi-Processors, SM 8.6
Loaded 'teapot512.pgm', 512 x 512 pixels
Processing time: 0.056000 (ms)
4681.14 Mpixels/sec
Wrote 'output.pgm'
Comparing files
	output:    <output.pgm>
	reference: <./data/ref_rotated.pgm>
simpleSurfaceWrite completed, returned OK
****************************************************************
################################################################
0_Introduction/simpleTemplates
################################################################
make: Nothing to be done for 'all'.
> runTest<float,32>
GPU Device 0: "Ampere" with compute capability 8.6

CUDA device [NVIDIA GeForce RTX 3060] has 28 Multi-Processors
Processing time: 5.231000 (ms)
Compare OK

> runTest<int,64>
GPU Device 0: "Ampere" with compute capability 8.6

CUDA device [NVIDIA GeForce RTX 3060] has 28 Multi-Processors
Processing time: 0.406000 (ms)
Compare OK


[simpleTemplates] -> Test Results: 0 Failures
****************************************************************
################################################################
0_Introduction/simpleTemplates_nvrtc
################################################################
make: Nothing to be done for 'all'.
> runTest<float,32>
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> GPU Device has SM 8.6 compute capability
Processing time: 11.179000 (ms)
Compare OK

> runTest<int,64>
Processing time: 0.545000 (ms)
Compare OK


[simpleTemplates_nvrtc] -> Test Results: 0 Failures
****************************************************************
################################################################
0_Introduction/simpleTexture
################################################################
make: Nothing to be done for 'all'.
simpleTexture starting...
GPU Device 0: "Ampere" with compute capability 8.6

Loaded 'teapot512.pgm', 512 x 512 pixels
Processing time: 0.042000 (ms)
6241.52 Mpixels/sec
Wrote './data/teapot512_out.pgm'
Comparing files
	output:    <./data/teapot512_out.pgm>
	reference: <./data/ref_rotated.pgm>
simpleTexture completed, returned OK
****************************************************************
################################################################
0_Introduction/simpleTexture3D
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at simpleTexture3D.cpp:254 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer( &cuda_pbo_resource, pbo, cudaGraphicsMapFlagsWriteDiscard)" 
simpleTexture3D Starting...

GPU Device 0: "Ampere" with compute capability 8.6

****************************************************************
################################################################
0_Introduction/simpleTextureDrv
################################################################
make: Nothing to be done for 'all'.
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> GPU Device has SM 8.6 compute capability
> findModulePath found file at <./simpleTexture_kernel64.fatbin>
> initCUDA loading module: <./simpleTexture_kernel64.fatbin>
Loaded 'teapot512.pgm', 512 x 512 pixels
Processing time: 0.050000 (ms)
5242.88 Mpixels/sec
Wrote './data/teapot512_out.pgm'
Comparing files
	output:    <./data/teapot512_out.pgm>
	reference: <./data/ref_rotated.pgm>
****************************************************************
################################################################
0_Introduction/simpleVoteIntrinsics
################################################################
make: Nothing to be done for 'all'.
[simpleVoteIntrinsics]
GPU Device 0: "Ampere" with compute capability 8.6

> GPU device has 28 Multi-Processors, SM 8.6 compute capabilities

[VOTE Kernel Test 1/3]
	Running <<Vote.Any>> kernel1 ...
	OK

[VOTE Kernel Test 2/3]
	Running <<Vote.All>> kernel2 ...
	OK

[VOTE Kernel Test 3/3]
	Running <<Vote.Any>> kernel3 ...
	OK
	Shutting down...
****************************************************************
################################################################
0_Introduction/simpleVoteIntrinsics_nvrtc
################################################################
make: Nothing to be done for 'all'.
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> GPU Device has SM 8.6 compute capability
[simpleVoteIntrinsics_nvrtc]
[VOTE Kernel Test 1/3]
	Running <<Vote.Any>> kernel1 ...
	OK

[VOTE Kernel Test 2/3]
	Running <<Vote.All>> kernel2 ...
	OK

[VOTE Kernel Test 3/3]
	Running <<Vote.Any>> kernel3 ...
	OK
	Shutting down...
****************************************************************
################################################################
0_Introduction/simpleZeroCopy
################################################################
make: Nothing to be done for 'all'.
  Device 0: <          Ampere >, Compute SM 8.6 detected
> Using CUDA Host Allocated (cudaHostAlloc)
> vectorAddGPU kernel will add vectors using mapped CPU memory...
> Checking the results from vectorAddGPU() ...
> Releasing CPU memory...
****************************************************************
################################################################
0_Introduction/systemWideAtomics
################################################################
make: Nothing to be done for 'all'.
./all_projects.sh: line 382: 290745 Segmentation fault      ./systemWideAtomics
****************************************************************
################################################################
0_Introduction/template
################################################################
make: Nothing to be done for 'all'.
./template Starting...

GPU Device 0: "Ampere" with compute capability 8.6

Processing time: 5.644000 (ms)
****************************************************************
################################################################
0_Introduction/vectorAdd
################################################################
make: Nothing to be done for 'all'.
[Vector addition of 50000 elements]
Copy input data from the host memory to the CUDA device
CUDA kernel launch with 196 blocks of 256 threads
Copy output data from the CUDA device to the host memory
Test PASSED
Done
****************************************************************
################################################################
0_Introduction/vectorAddDrv
################################################################
make: Nothing to be done for 'all'.
Vector Addition (Driver API)
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> findModulePath found file at <./vectorAdd_kernel64.fatbin>
> initCUDA loading module: <./vectorAdd_kernel64.fatbin>
Result = PASS
****************************************************************
################################################################
0_Introduction/vectorAddMMAP
################################################################
make: Nothing to be done for 'all'.
Vector Addition (Driver API)
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
Device 0 VIRTUAL ADDRESS MANAGEMENT SUPPORTED = 1.
> findModulePath found file at <./vectorAdd_kernel64.fatbin>
> initCUDA loading module: <./vectorAdd_kernel64.fatbin>
Result = PASS
****************************************************************
################################################################
0_Introduction/vectorAdd_nvrtc
################################################################
make: Nothing to be done for 'all'.
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> GPU Device has SM 8.6 compute capability
[Vector addition of 50000 elements]
Copy input data from the host memory to the CUDA device
CUDA kernel launch with 196 blocks of 256 threads
Copy output data from the CUDA device to the host memory
Test PASSED
Done
****************************************************************
################################################################
1_Utilities/bandwidthTest
################################################################
make: Nothing to be done for 'all'.
[CUDA Bandwidth Test] - Starting...
Running on...

 Device 0: NVIDIA GeForce RTX 3060
 Quick Mode

 Host to Device Bandwidth, 1 Device(s)
 PINNED Memory Transfers
   Transfer Size (Bytes)	Bandwidth(GB/s)
   32000000			12.6

 Device to Host Bandwidth, 1 Device(s)
 PINNED Memory Transfers
   Transfer Size (Bytes)	Bandwidth(GB/s)
   32000000			13.1

 Device to Device Bandwidth, 1 Device(s)
 PINNED Memory Transfers
   Transfer Size (Bytes)	Bandwidth(GB/s)
   32000000			287.6

Result = PASS

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
****************************************************************
################################################################
1_Utilities/deviceQuery
################################################################
make: Nothing to be done for 'all'.
./deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: "NVIDIA GeForce RTX 3060"
  CUDA Driver Version / Runtime Version          12.6 / 12.6
  CUDA Capability Major/Minor version number:    8.6
  Total amount of global memory:                 12287 MBytes (12884246528 bytes)
  (028) Multiprocessors, (128) CUDA Cores/MP:    3584 CUDA Cores
  GPU Max Clock rate:                            1852 MHz (1.85 GHz)
  Memory Clock rate:                             7501 Mhz
  Memory Bus Width:                              192-bit
  L2 Cache Size:                                 2359296 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total shared memory per multiprocessor:        102400 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  1536
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 5 copy engine(s)
  Run time limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Managed Memory:                Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      No
  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.6, CUDA Runtime Version = 12.6, NumDevs = 1
Result = PASS
****************************************************************
################################################################
1_Utilities/deviceQueryDrv
################################################################
make: Nothing to be done for 'all'.
./deviceQueryDrv Starting...

CUDA Device Query (Driver API) statically linked version 
Detected 1 CUDA Capable device(s)

Device 0: "NVIDIA GeForce RTX 3060"
  CUDA Driver Version:                           12.6
  CUDA Capability Major/Minor version number:    8.6
  Total amount of global memory:                 12287 MBytes (12884246528 bytes)
  (28) Multiprocessors, (128) CUDA Cores/MP:     3584 CUDA Cores
  GPU Max Clock rate:                            1852 MHz (1.85 GHz)
  Memory Clock rate:                             7501 Mhz
  Memory Bus Width:                              192-bit
  L2 Cache Size:                                 2359296 bytes
  Max Texture Dimension Sizes                    1D=(131072) 2D=(131072, 65536) 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  1536
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size (x,y,z):    (2147483647, 65535, 65535)
  Texture alignment:                             512 bytes
  Maximum memory pitch:                          2147483647 bytes
  Concurrent copy and kernel execution:          Yes with 5 copy engine(s)
  Run time limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Concurrent kernel execution:                   Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Managed Memory:                Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      No
  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >
Result = PASS
****************************************************************
################################################################
1_Utilities/topologyQuery
################################################################
make: Nothing to be done for 'all'.
GPU0 <-> CPU:
  * Atomic Supported: no
****************************************************************
################################################################
2_Concepts_and_Techniques/EGLStream_CUDA_CrossGPU
################################################################
make: Nothing to be done for 'all'.
./EGLStream_CUDA_CrossGPU: symbol lookup error: ./EGLStream_CUDA_CrossGPU: undefined symbol: cuEGLStreamProducerDisconnect
****************************************************************
################################################################
2_Concepts_and_Techniques/EGLStream_CUDA_Interop
################################################################
make: Nothing to be done for 'all'.
./EGLStream_CUDA_Interop: symbol lookup error: ./EGLStream_CUDA_Interop: undefined symbol: cuEGLStreamProducerConnect
****************************************************************
################################################################
2_Concepts_and_Techniques/EGLSync_CUDAEvent_Interop
################################################################
>>> WARNING - EGLSync_CUDAEvent_Interop is not supported on Linux x86_64 - waiving sample <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o EGLSync_CUDAEvent_Interop.o -c EGLSync_CUDAEvent_Interop.cu
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o EGLSync_CUDAEvent_Interop EGLSync_CUDAEvent_Interop.o -lEGL -L/usr/local/cuda/lib64/stubs -lcuda -lX11 -lGLESv2
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp EGLSync_CUDAEvent_Interop ../../../bin/x86_64/linux/release
./all_projects.sh: line 478: ./EGLSync_CUDAEvent_Interop: No such file or directory
****************************************************************
################################################################
2_Concepts_and_Techniques/FunctionPointers
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at FunctionPointers.cpp:321 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer( &cuda_pbo_resource, pbo_buffer, cudaGraphicsMapFlagsWriteDiscard)" 
./FunctionPointers Starting...

GPU Device 0: "Ampere" with compute capability 8.6

Reading image: teapot512.pgm
****************************************************************
################################################################
2_Concepts_and_Techniques/MC_EstimatePiInlineP
################################################################
make: Nothing to be done for 'all'.
Monte Carlo Estimate Pi (with inline PRNG)
==========================================

Estimating Pi on GPU (NVIDIA GeForce RTX 3060)

Precision:      single
Number of sims: 100000
Tolerance:      1.000000e-02
GPU result:     3.140440e+00
Expected:       3.141593e+00
Absolute error: 1.152754e-03
Relative error: 3.669329e-04

MonteCarloEstimatePiInlineP, Performance = 672639.68 sims/s, Time = 148.67(ms), NumDevsUsed = 1, Blocksize = 128
****************************************************************
################################################################
2_Concepts_and_Techniques/MC_EstimatePiInlineQ
################################################################
make: Nothing to be done for 'all'.
Monte Carlo Estimate Pi (with inline QRNG)
==========================================

Estimating Pi on GPU (NVIDIA GeForce RTX 3060)

Precision:      single
Number of sims: 100000
Tolerance:      1.000000e-02
GPU result:     3.142520e+00
Expected:       3.141593e+00
Absolute error: 9.272099e-04
Relative error: 2.951401e-04

MonteCarloEstimatePiInlineQ, Performance = 689312.83 sims/s, Time = 145.07(ms), NumDevsUsed = 1, Blocksize = 128
****************************************************************
################################################################
2_Concepts_and_Techniques/MC_EstimatePiP
################################################################
make: Nothing to be done for 'all'.
Monte Carlo Estimate Pi (with batch PRNG)
=========================================

Estimating Pi on GPU (NVIDIA GeForce RTX 3060)

Precision:      single
Number of sims: 100000
Tolerance:      1.000000e-02
GPU result:     3.136320e+00
Expected:       3.141593e+00
Absolute error: 5.272627e-03
Relative error: 1.678329e-03

MonteCarloEstimatePiP, Performance = 691027.69 sims/s, Time = 144.71(ms), NumDevsUsed = 1, Blocksize = 128
****************************************************************
################################################################
2_Concepts_and_Techniques/MC_EstimatePiQ
################################################################
make: Nothing to be done for 'all'.
Monte Carlo Estimate Pi (with batch QRNG)
=========================================

Estimating Pi on GPU (NVIDIA GeForce RTX 3060)

Precision:      single
Number of sims: 100000
Tolerance:      1.000000e-02
GPU result:     3.141840e+00
Expected:       3.141593e+00
Absolute error: 2.472401e-04
Relative error: 7.869895e-05

MonteCarloEstimatePiQ, Performance = 632111.27 sims/s, Time = 158.20(ms), NumDevsUsed = 1, Blocksize = 128
****************************************************************
################################################################
2_Concepts_and_Techniques/MC_SingleAsianOptionP
################################################################
make: Nothing to be done for 'all'.
Monte Carlo Single Asian Option (with PRNG)
===========================================

Pricing option on GPU (NVIDIA GeForce RTX 3060)

Precision:      single
Number of sims: 100000

   Spot    |   Strike   |     r      |   sigma    |   tenor    |  Call/Put  |   Value    |  Expected  |
-----------|------------|------------|------------|------------|------------|------------|------------|
        40 |         35 |       0.03 |        0.2 |   0.333333 |       Call |    5.17636 |    5.16253 |

MonteCarloSingleAsianOptionP, Performance = 701272.09 sims/s, Time = 142.60(ms), NumDevsUsed = 1, Blocksize = 128
****************************************************************
################################################################
2_Concepts_and_Techniques/boxFilter
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at boxFilter.cpp:363 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer( &cuda_pbo_resource, pbo, cudaGraphicsMapFlagsWriteDiscard)" 
./boxFilter Starting...

Loaded './data/teapot1024.ppm', 1024 x 1024 pixels
GPU Device 0: "Ampere" with compute capability 8.6


****************************************************************
################################################################
2_Concepts_and_Techniques/convolutionSeparable
################################################################
make: Nothing to be done for 'all'.
[./convolutionSeparable] - Starting...
GPU Device 0: "Ampere" with compute capability 8.6

Image Width x Height = 3072 x 3072

Allocating and initializing host arrays...
Allocating and initializing CUDA arrays...
Running GPU convolution (16 identical iterations)...

convolutionSeparable, Throughput = 19896.5528 MPixels/sec, Time = 0.00047 s, Size = 9437184 Pixels, NumDevsUsed = 1, Workgroup = 0

Reading back GPU results...

Checking the results...
 ...running convolutionRowCPU()
 ...running convolutionColumnCPU()
 ...comparing the results
 ...Relative L2 norm: 0.000000E+00

Shutting down...
Test passed
****************************************************************
################################################################
2_Concepts_and_Techniques/convolutionTexture
################################################################
make: Nothing to be done for 'all'.
[./convolutionTexture] - Starting...
GPU Device 0: "Ampere" with compute capability 8.6

Initializing data...
Running GPU rows convolution (10 identical iterations)...
Average convolutionRowsGPU() time: 0.571100 msecs; //8262.286865 Mpix/s
Copying convolutionRowGPU() output back to the texture...
cudaMemcpyToArray() time: 0.214000 msecs; //22049.495167 Mpix/s
Running GPU columns convolution (10 iterations)
Average convolutionColumnsGPU() time: 0.401000 msecs; //11767.061673 Mpix/s
Reading back GPU results...
Checking the results...
...running convolutionRowsCPU()
...running convolutionColumnsCPU()
Relative L2 norm: 0.000000E+00
Shutting down...
Test passed
****************************************************************
################################################################
2_Concepts_and_Techniques/cuHook
################################################################
>>> Waiving build. GLIBC > 2.33 is not supported<<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o cuHook.o -c cuHook.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o cuHook cuHook.o -L/usr/local/cuda/lib64/stubs -lcuda -L/usr/local/cuda/lib -L/usr/local/cuda/lib64 -ldl
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp cuHook ../../../bin/x86_64/linux/release
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --threads 0 --std=c++11 --compiler-options -fPIC -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o libcuhook.o -c libcuhook.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -shared -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o libcuhook.so.1 libcuhook.o -L/usr/local/cuda/lib64/stubs -lcuda -L/usr/local/cuda/lib -L/usr/local/cuda/lib64 -ldl
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp libcuhook.so.1 ../../../bin/x86_64/linux/release
./all_projects.sh: line 558: ./cuHook: No such file or directory
****************************************************************
################################################################
2_Concepts_and_Techniques/dct8x8
################################################################
make: Nothing to be done for 'all'.
./dct8x8 Starting...

GPU Device 0: "Ampere" with compute capability 8.6

CUDA sample DCT/IDCT implementation
===================================
Loading test image: teapot512.bmp... [512 x 512]... Success
Running Gold 1 (CPU) version... Success
Running Gold 2 (CPU) version... Success
Running CUDA 1 (GPU) version... Success
Running CUDA 2 (GPU) version... 39718.785292 MPix/s //0.006600 ms
Success
Running CUDA short (GPU) version... Success
Dumping result to teapot512_gold1.bmp... Success
Dumping result to teapot512_gold2.bmp... Success
Dumping result to teapot512_cuda1.bmp... Success
Dumping result to teapot512_cuda2.bmp... Success
Dumping result to teapot512_cuda_short.bmp... Success
Processing time (CUDA 1)    : 0.397100 ms 
Processing time (CUDA 2)    : 0.006600 ms 
Processing time (CUDA short): 0.175000 ms 
PSNR Original    <---> CPU(Gold 1)    : 32.527462
PSNR Original    <---> CPU(Gold 2)    : 32.527309
PSNR Original    <---> GPU(CUDA 1)    : 32.527184
PSNR Original    <---> GPU(CUDA 2)    : 32.527054
PSNR Original    <---> GPU(CUDA short): 32.501888
PSNR CPU(Gold 1) <---> GPU(CUDA 1)    : 62.845787
PSNR CPU(Gold 2) <---> GPU(CUDA 2)    : 66.982300
PSNR CPU(Gold 2) <---> GPU(CUDA short): 40.958466

Test Summary...
Test passed
****************************************************************
################################################################
2_Concepts_and_Techniques/eigenvalues
################################################################
make: Nothing to be done for 'all'.
Starting eigenvalues
GPU Device 0: "Ampere" with compute capability 8.6

Matrix size: 2048 x 2048
Precision: 0.000010
Iterations to be timed: 100
Result filename: 'eigenvalues.dat'
Gerschgorin interval: -2.894310 / 2.923303
Average time step 1: 0.988050 ms
Average time step 2, one intervals: 1.067910 ms
Average time step 2, mult intervals: 2.341770 ms
Average time TOTAL: 4.589700 ms
Test Succeeded!
****************************************************************
################################################################
2_Concepts_and_Techniques/histogram
################################################################
make: Nothing to be done for 'all'.
[[histogram]] - Starting...
GPU Device 0: "Ampere" with compute capability 8.6

CUDA device [NVIDIA GeForce RTX 3060] has 28 Multi-Processors, Compute 8.6
Initializing data...
...allocating CPU memory.
...generating input data
...allocating GPU memory and copying input data

Starting up 64-bin histogram...

Running 64-bin GPU histogram for 67108864 bytes (16 runs)...

histogram64() time (average) : 0.00023 sec, 291381.7749 MB/sec

histogram64, Throughput = 291381.7749 MB/s, Time = 0.00023 s, Size = 67108864 Bytes, NumDevsUsed = 1, Workgroup = 64

Validating GPU results...
 ...reading back GPU results
 ...histogram64CPU()
 ...comparing the results...
 ...64-bin histograms match

Shutting down 64-bin histogram...


Initializing 256-bin histogram...
Running 256-bin GPU histogram for 67108864 bytes (16 runs)...

histogram256() time (average) : 0.00030 sec, 225339.3188 MB/sec

histogram256, Throughput = 225339.3188 MB/s, Time = 0.00030 s, Size = 67108864 Bytes, NumDevsUsed = 1, Workgroup = 192

Validating GPU results...
 ...reading back GPU results
 ...histogram256CPU()
 ...comparing the results
 ...256-bin histograms match

Shutting down 256-bin histogram...


Shutting down...

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

[histogram] - Test Summary
Test passed
****************************************************************
################################################################
2_Concepts_and_Techniques/imageDenoising
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at imageDenoisingGL.cpp:395 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer( &cuda_pbo_resource, gl_PBO, cudaGraphicsMapFlagsWriteDiscard)" 
CUDA error at imageDenoisingGL.cpp:422 code=4(cudaErrorCudartUnloading) "CUDA_FreeArray()" 
CUDA ImageDenoising Starting...

[CUDA ImageDenoising]
Allocating host and CUDA memory and loading image file...
Loading ./data/portrait_noise.bmp...
BMP width: 320
BMP height: 408
BMP file loaded successfully!
Data init done.
Initializing GLUT...
OpenGL window created.
GPU Device 0: "Ampere" with compute capability 8.6

Creating GL texture...
Texture created.
Creating PBO...
****************************************************************
################################################################
2_Concepts_and_Techniques/inlinePTX
################################################################
make: Nothing to be done for 'all'.
CUDA inline PTX assembler sample
GPU Device 0: "Ampere" with compute capability 8.6

Test Successful.
****************************************************************
################################################################
2_Concepts_and_Techniques/inlinePTX_nvrtc
################################################################
make: Nothing to be done for 'all'.
CUDA inline PTX assembler sample
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> GPU Device has SM 8.6 compute capability
Test Successful.
****************************************************************
################################################################
2_Concepts_and_Techniques/interval
################################################################
make: Nothing to be done for 'all'.
[Interval Computing]  starting ...

GPU Device 0: "Ampere" with compute capability 8.6

> GPU Device has Compute Capabilities SM 8.6

GPU naive implementation
Searching for roots in [0.01, 4]...
Found 2 intervals that may contain the root(s)
 i[0] = [0.999655515093009, 1.00011722206639]
 i[1] = [1.00011907576551, 1.00044661086269]
Number of equations solved: 65536
Time per equation: 13.7330961227417 us

Check against Host computation...

****************************************************************
################################################################
2_Concepts_and_Techniques/particles
################################################################
>>> GCC Version is greater or equal to 5.0.0 <<<
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at particleSystem_cuda.cu:85 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer(cuda_vbo_resource, vbo, cudaGraphicsMapFlagsNone)" 
CUDA Particles Simulation Starting...

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

grid: 64 x 64 x 64 = 262144 cells
particles: 16384
GPU Device 0: "Ampere" with compute capability 8.6

****************************************************************
################################################################
2_Concepts_and_Techniques/radixSortThrust
################################################################
>>> GCC Version is greater or equal to 5.0.0 <<<
make: Nothing to be done for 'all'.
./radixSortThrust Starting...

GPU Device 0: "Ampere" with compute capability 8.6


Sorting 1048576 32-bit unsigned int keys and values

radixSortThrust, Throughput = 1853.4250 MElements/s, Time = 0.00057 s, Size = 1048576 elements
Test passed
****************************************************************
################################################################
2_Concepts_and_Techniques/reduction
################################################################
>>> GCC Version is greater or equal to 4.7.0 <<<
make: Nothing to be done for 'all'.
./reduction Starting...

GPU Device 0: "Ampere" with compute capability 8.6

Using Device 0: NVIDIA GeForce RTX 3060

Reducing array of type int

16777216 elements
256 threads (max)
64 blocks

Reduction, Throughput = 258.2203 GB/s, Time = 0.00026 s, Size = 16777216 Elements, NumDevsUsed = 1, Workgroup = 256

GPU result = 2139353471
CPU result = 2139353471

Test passed
****************************************************************
################################################################
2_Concepts_and_Techniques/reductionMultiBlockCG
################################################################
>>> GCC Version is greater or equal to 4.7.0 <<<
make: Nothing to be done for 'all'.
reductionMultiBlockCG Starting...

GPU Device 0: "Ampere" with compute capability 8.6

33554432 elements
numThreads: 768
numBlocks: 28

Launching SinglePass Multi Block Cooperative Groups kernel
Average time: 0.831230 ms
Bandwidth:    161.468835 GB/s

GPU result = 1.992401361465
CPU result = 1.992401361465
****************************************************************
################################################################
2_Concepts_and_Techniques/scalarProd
################################################################
make: Nothing to be done for 'all'.
./scalarProd Starting...

GPU Device 0: "Ampere" with compute capability 8.6

Initializing data...
...allocating CPU memory.
...allocating GPU memory.
...generating input data in CPU mem.
...copying input data to GPU mem.
Data init done.
Executing GPU kernel...
GPU time: 2.994000 msecs.
Reading back GPU result...
Checking GPU results...
..running CPU scalar product calculation
...comparing the results
Shutting down...
L1 error: 2.745062E-08
Test passed
****************************************************************
################################################################
2_Concepts_and_Techniques/scan
################################################################
make: Nothing to be done for 'all'.
./scan Starting...

GPU Device 0: "Ampere" with compute capability 8.6

Allocating and initializing host arrays...
Allocating and initializing CUDA arrays...
Initializing CUDA-C scan...

*** Running GPU scan for short arrays (100 identical iterations)...

Running scan for 4 elements (1703936 arrays)...
Validating the results...
...reading back GPU results
 ...scanExclusiveHost()
 ...comparing the results
 ...Results Match

Running scan for 8 elements (851968 arrays)...
Validating the results...
...reading back GPU results
 ...scanExclusiveHost()
 ...comparing the results
 ...Results Match

Running scan for 16 elements (425984 arrays)...
Validating the results...
...reading back GPU results
 ...scanExclusiveHost()
 ...comparing the results
 ...Results Match

Running scan for 32 elements (212992 arrays)...
Validating the results...
...reading back GPU results
 ...scanExclusiveHost()
 ...comparing the results
 ...Results Match

Running scan for 64 elements (106496 arrays)...
Validating the results...
...reading back GPU results
 ...scanExclusiveHost()
 ...comparing the results
 ...Results Match

Running scan for 128 elements (53248 arrays)...
Validating the results...
...reading back GPU results
 ...scanExclusiveHost()
 ...comparing the results
 ...Results Match

Running scan for 256 elements (26624 arrays)...
Validating the results...
...reading back GPU results
 ...scanExclusiveHost()
 ...comparing the results
 ...Results Match

Running scan for 512 elements (13312 arrays)...
Validating the results...
...reading back GPU results
 ...scanExclusiveHost()
 ...comparing the results
 ...Results Match

Running scan for 1024 elements (6656 arrays)...
Validating the results...
...reading back GPU results
 ...scanExclusiveHost()
 ...comparing the results
 ...Results Match


scan, Throughput = 6.3062 MElements/s, Time = 0.00016 s, Size = 1024 Elements, NumDevsUsed = 1, Workgroup = 256

***Running GPU scan for large arrays (100 identical iterations)...

Running scan for 2048 elements (3328 arrays)...
Validating the results...
...reading back GPU results
...scanExclusiveHost()
 ...comparing the results
 ...Results Match

Running scan for 4096 elements (1664 arrays)...
Validating the results...
...reading back GPU results
...scanExclusiveHost()
 ...comparing the results
 ...Results Match

Running scan for 8192 elements (832 arrays)...
Validating the results...
...reading back GPU results
...scanExclusiveHost()
 ...comparing the results
 ...Results Match

Running scan for 16384 elements (416 arrays)...
Validating the results...
...reading back GPU results
...scanExclusiveHost()
 ...comparing the results
 ...Results Match

Running scan for 32768 elements (208 arrays)...
Validating the results...
...reading back GPU results
...scanExclusiveHost()
 ...comparing the results
 ...Results Match

Running scan for 65536 elements (104 arrays)...
Validating the results...
...reading back GPU results
...scanExclusiveHost()
 ...comparing the results
 ...Results Match

Running scan for 131072 elements (52 arrays)...
Validating the results...
...reading back GPU results
...scanExclusiveHost()
 ...comparing the results
 ...Results Match

Running scan for 262144 elements (26 arrays)...
Validating the results...
...reading back GPU results
...scanExclusiveHost()
 ...comparing the results
 ...Results Match


scan, Throughput = 794.3998 MElements/s, Time = 0.00033 s, Size = 262144 Elements, NumDevsUsed = 1, Workgroup = 256

Shutting down...
****************************************************************
################################################################
2_Concepts_and_Techniques/segmentationTreeThrust
################################################################
>>> GCC Version is greater or equal to 5.0.0 <<<
make: Nothing to be done for 'all'.
./segmentationTreeThrust Starting...

GPU Device 0: "Ampere" with compute capability 8.6

* Building segmentation tree... done in 116.973 (ms)
* Dumping levels for each tree...

****************************************************************
################################################################
2_Concepts_and_Techniques/shfl_scan
################################################################
>>> GCC Version is greater or equal to 4.7.0 <<<
make: Nothing to be done for 'all'.
Starting shfl_scan
GPU Device 0: "Ampere" with compute capability 8.6

> Detected Compute SM 8.6 hardware with 28 multi-processors
Starting shfl_scan
GPU Device 0: "Ampere" with compute capability 8.6

> Detected Compute SM 8.6 hardware with 28 multi-processors
Computing Simple Sum test
---------------------------------------------------
Initialize test data [1, 1, 1...]
Scan summation for 65536 elements, 256 partial sums
Partial summing 256 elements with 1 blocks of size 256
Test Sum: 65536
Time (ms): 1.847296
65536 elements scanned in 1.847296 ms -> 35.476715 MegaElements/s
CPU verify result diff (GPUvsCPU) = 0
CPU sum (naive) took 0.023180 ms

Computing Integral Image Test on size 1920 x 1080 synthetic data
---------------------------------------------------
Method: Fast  Time (GPU Timer): 0.036864 ms Diff = 0
Method: Vertical Scan  Time (GPU Timer): 0.094208 ms 
CheckSum: 2073600, (expect 1920x1080=2073600)
****************************************************************
################################################################
2_Concepts_and_Techniques/sortingNetworks
################################################################
make: Nothing to be done for 'all'.
./sortingNetworks Starting...

Starting up CUDA context...
GPU Device 0: "Ampere" with compute capability 8.6

Allocating and initializing host arrays...

Allocating and initializing CUDA arrays...

Running GPU bitonic sort (1 identical iterations)...

Testing array length 64 (16384 arrays per batch)...
Average time: 2.190000 ms


Validating the results...
...reading back GPU results
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: NOT stable

Testing array length 128 (8192 arrays per batch)...
Average time: 0.349000 ms


Validating the results...
...reading back GPU results
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: NOT stable

Testing array length 256 (4096 arrays per batch)...
Average time: 0.403000 ms


Validating the results...
...reading back GPU results
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: NOT stable

Testing array length 512 (2048 arrays per batch)...
Average time: 0.464000 ms


Validating the results...
...reading back GPU results
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: NOT stable

Testing array length 1024 (1024 arrays per batch)...
Average time: 0.508000 ms


Validating the results...
...reading back GPU results
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: NOT stable

Testing array length 2048 (512 arrays per batch)...
Average time: 0.572000 ms


Validating the results...
...reading back GPU results
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: NOT stable

Testing array length 4096 (256 arrays per batch)...
Average time: 0.696000 ms


Validating the results...
...reading back GPU results
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: NOT stable

Testing array length 8192 (128 arrays per batch)...
Average time: 0.925000 ms


Validating the results...
...reading back GPU results
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: NOT stable

Testing array length 16384 (64 arrays per batch)...
Average time: 1.255000 ms


Validating the results...
...reading back GPU results
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: NOT stable

Testing array length 32768 (32 arrays per batch)...
Average time: 1.588000 ms


Validating the results...
...reading back GPU results
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: NOT stable

Testing array length 65536 (16 arrays per batch)...
Average time: 1.886000 ms


Validating the results...
...reading back GPU results
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: NOT stable

Testing array length 131072 (8 arrays per batch)...
Average time: 2.297000 ms


Validating the results...
...reading back GPU results
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: NOT stable

Testing array length 262144 (4 arrays per batch)...
Average time: 2.757000 ms


Validating the results...
...reading back GPU results
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: NOT stable

Testing array length 524288 (2 arrays per batch)...
Average time: 3.870000 ms


Validating the results...
...reading back GPU results
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: NOT stable

Testing array length 1048576 (1 arrays per batch)...
Average time: 3.930000 ms

sortingNetworks-bitonic, Throughput = 266.8132 MElements/s, Time = 0.00393 s, Size = 1048576 elements, NumDevsUsed = 1, Workgroup = 512

Validating the results...
...reading back GPU results
...inspecting keys array: OK
...inspecting keys and values array: OK
...stability property: NOT stable

Shutting down...
****************************************************************
################################################################
2_Concepts_and_Techniques/streamOrderedAllocation
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Starting basicStreamOrderedAllocation()
> Checking the results from vectorAddGPU() ...
basicStreamOrderedAllocation PASSED
Starting streamOrderedAllocationPostSync()
Total elapsed time = 41.642624 ms over 20 iterations
> Checking the results from vectorAddGPU() ...
streamOrderedAllocationPostSync PASSED
****************************************************************
################################################################
2_Concepts_and_Techniques/streamOrderedAllocationIPC
################################################################
make: Nothing to be done for 'all'.
CUDA error at streamOrderedAllocationIPC.cu:360 code=1(cudaErrorInvalidValue) "cudaMemPoolCreate(&pools[i], &poolProps)" 
****************************************************************
################################################################
2_Concepts_and_Techniques/streamOrderedAllocationP2P
################################################################
make: Nothing to be done for 'all'.
No Two or more GPUs with same architecture capable of cuda Memory Pools found.
Waiving the sample
****************************************************************
################################################################
2_Concepts_and_Techniques/threadFenceReduction
################################################################
make: Nothing to be done for 'all'.
threadFenceReduction Starting...

GPU Device 0: "Ampere" with compute capability 8.6

GPU Device supports SM 8.6 compute capability

1048576 elements
128 threads (max)
64 blocks
Average time: 0.093090 ms
Bandwidth:    45.056448 GB/s

GPU result = 0.062298238277
CPU result = 0.062298242003
****************************************************************
################################################################
2_Concepts_and_Techniques/threadMigration
################################################################
make: Nothing to be done for 'all'.
Starting threadMigration
[ threadMigration ] API test...
> 1 CUDA device(s), 2 Thread(s)/device to launched

Device 0: "NVIDIA GeForce RTX 3060" (Compute 8.6)
	sharedMemPerBlock: 49152
	constantMemory   : 65536
	regsPerBlock     : 65536
	clockRate        : 1852000

> findModulePath found file at <./threadMigration_kernel64.fatbin>
> initCUDA loading module: <./threadMigration_kernel64.fatbin>
<CUDA Device=0, Context=0x55eae99cc8d0, Thread=0> - ThreadProc() Launched...
<CUDA Device=0, Context=0x55eae99cc8d0, Thread=1> - ThreadProc() Launched...
<CUDA Device=0, Context=0x55eae99cc8d0, Thread=1> - ThreadProc() Finished!

<CUDA Device=0, Context=0x55eae99cc8d0, Thread=0> - ThreadProc() Finished!

****************************************************************
################################################################
3_CUDA_Features/StreamPriorities
################################################################
make: Nothing to be done for 'all'.
Starting [./StreamPriorities]...
GPU Device 0: "Ampere" with compute capability 8.6

CUDA stream priority range: LOW: 0 to HIGH: -5
elapsed time of kernels launched to LOW priority stream: 6.793 ms
elapsed time of kernels launched to HI  priority stream: 5.331 ms
****************************************************************
################################################################
3_CUDA_Features/bf16TensorCoreGemm
################################################################
>>> GCC Version is greater or equal to  5.1.0 <<<
make: Nothing to be done for 'all'.
Initializing...
GPU Device 0: "Ampere" with compute capability 8.6

M: 8192 (16 x 512)
N: 8192 (16 x 512)
K: 8192 (16 x 512)
Preparing data for GPU...
Required shared memory size: 72 Kb
Computing using high performance kernel = 0 - compute_bf16gemm_async_copy
Time: 58.263554 ms
TFLOPS: 18.87
****************************************************************
################################################################
3_CUDA_Features/binaryPartitionCG
################################################################
>>> GCC Version is greater or equal to 4.7.0 <<<
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6


Launching 56 blocks with 768 threads...

Array size = 102400 Num of Odds = 50945 Sum of Odds = 1272565 Sum of Evens 1233938

...Done.

****************************************************************
################################################################
3_CUDA_Features/bindlessTexture
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at bindlessTexture.cpp:246 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer( &cuda_pbo_resource, pbo, cudaGraphicsMapFlagsWriteDiscard)" 
CUDA bindlessTexture Starting...

GPU Device 0: "Ampere" with compute capability 8.6

****************************************************************
################################################################
3_CUDA_Features/cdpAdvancedQuicksort
################################################################
>>> GCC Version is greater or equal to 5.0.0 <<<
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

GPU device NVIDIA GeForce RTX 3060 has compute capabilities (SM 8.6)
Running qsort on 1000000 elements with seed 0, on NVIDIA GeForce RTX 3060
    cdpAdvancedQuicksort PASSED
Sorted 1000000 elems in 13.145 ms (76.074 Melems/sec)
****************************************************************
################################################################
3_CUDA_Features/cdpBezierTessellation
################################################################
make: Nothing to be done for 'all'.
Running on GPU 0 (NVIDIA GeForce RTX 3060)
Computing Bezier Lines (CUDA Dynamic Parallelism Version) ... Done!
****************************************************************
################################################################
3_CUDA_Features/cdpQuadtree
################################################################
>>> GCC Version is greater or equal to 5.0.0 <<<
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

GPU device NVIDIA GeForce RTX 3060 has compute capabilities (SM 8.6)
Launching CDP kernel to build the quadtree
Results: OK
****************************************************************
################################################################
3_CUDA_Features/cdpSimplePrint
################################################################
make: Nothing to be done for 'all'.
starting Simple Print (CUDA Dynamic Parallelism)
GPU Device 0: "Ampere" with compute capability 8.6

***************************************************************************
The CPU launches 2 blocks of 2 threads each. On the device each thread will
launch 2 blocks of 2 threads each. The GPU we will do that recursively
until it reaches max_depth=2

In total 2+8=10 blocks are launched!!! (8 from the GPU)
***************************************************************************

Launching cdp_kernel() with CUDA Dynamic Parallelism:

BLOCK 0 launched by the host
BLOCK 1 launched by the host
|  BLOCK 3 launched by thread 0 of block 0
|  BLOCK 5 launched by thread 0 of block 1
|  BLOCK 2 launched by thread 0 of block 0
|  BLOCK 4 launched by thread 0 of block 1
|  BLOCK 7 launched by thread 1 of block 0
|  BLOCK 6 launched by thread 1 of block 0
|  BLOCK 8 launched by thread 1 of block 1
|  BLOCK 9 launched by thread 1 of block 1
****************************************************************
################################################################
3_CUDA_Features/cdpSimpleQuicksort
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Initializing data:
Running quicksort on 128 elements
Launching kernel on the GPU
Validating results: OK
****************************************************************
################################################################
3_CUDA_Features/cudaCompressibleMemory
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Generic memory compression support is available
allocating non-compressible Z buffer
Running saxpy on 167772160 bytes of Compressible memory
Running saxpy with 56 blocks x 768 threads = 1.512 ms 0.333 TB/s
Running saxpy on 167772160 bytes of Non-Compressible memory
Running saxpy with 56 blocks x 768 threads = 1.511 ms 0.333 TB/s

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
****************************************************************
################################################################
3_CUDA_Features/cudaTensorCoreGemm
################################################################
make: Nothing to be done for 'all'.
Initializing...
GPU Device 0: "Ampere" with compute capability 8.6

M: 4096 (16 x 256)
N: 4096 (16 x 256)
K: 4096 (16 x 256)
Preparing data for GPU...
Required shared memory size: 64 Kb
Computing... using high performance kernel compute_gemm 
Time: 9.499232 ms
TFLOPS: 14.47
****************************************************************
################################################################
3_CUDA_Features/dmmaTensorCoreGemm
################################################################
>>> GCC Version is greater or equal to  5.1.0 <<<
make: Nothing to be done for 'all'.
Initializing...
GPU Device 0: "Ampere" with compute capability 8.6

M: 8192 (8 x 1024)
N: 8192 (8 x 1024)
K: 4096 (4 x 1024)
Preparing data for GPU...
Required shared memory size: 68 Kb
Computing using high performance kernel = 0 - compute_dgemm_async_copy
Time: 3040.034912 ms
FP64 TFLOPS: 0.18
****************************************************************
################################################################
3_CUDA_Features/globalToShmemAsyncCopy
################################################################
>>> GCC Version is greater or equal to  5.1.0 <<<
make: Nothing to be done for 'all'.
[globalToShmemAsyncCopy] - Starting...
GPU Device 0: "Ampere" with compute capability 8.6

MatrixA(1280,1280), MatrixB(1280,1280)
Running kernel = 0 - AsyncCopyMultiStageLargeChunk
Computing result using CUDA Kernel...
done
Performance= 950.33 GFlop/s, Time= 4.414 msec, Size= 4194304000 Ops, WorkgroupSize= 256 threads/block
Checking computed result for correctness: Result = PASS

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
****************************************************************
################################################################
3_CUDA_Features/graphConditionalNodes
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Driver version is: 12.6
simpleIfGraph: Building graph...
Host: Launching graph with conditional value set to false
GPU: Handle set to 0
Host: Launching graph with conditional value set to true
GPU: Handle set to 1
GPU: Hello from the GPU!
simpleIfGraph: Complete

simpleDoWhileGraph: Building graph...
Host: Launching graph with loop counter set to 10
GPU: doWhileEmptyKernel()
GPU: doWhileEmptyKernel()
GPU: counter = 9
GPU: doWhileEmptyKernel()
GPU: doWhileEmptyKernel()
GPU: counter = 8
GPU: doWhileEmptyKernel()
GPU: doWhileEmptyKernel()
GPU: counter = 7
GPU: doWhileEmptyKernel()
GPU: doWhileEmptyKernel()
GPU: counter = 6
GPU: doWhileEmptyKernel()
GPU: doWhileEmptyKernel()
GPU: counter = 5
GPU: doWhileEmptyKernel()
GPU: doWhileEmptyKernel()
GPU: counter = 4
GPU: doWhileEmptyKernel()
GPU: doWhileEmptyKernel()
GPU: counter = 3
GPU: doWhileEmptyKernel()
GPU: doWhileEmptyKernel()
GPU: counter = 2
GPU: doWhileEmptyKernel()
GPU: doWhileEmptyKernel()
GPU: counter = 1
GPU: doWhileEmptyKernel()
GPU: doWhileEmptyKernel()
GPU: counter = 0
simpleDoWhileGraph: Complete

capturedWhileGraph: Building graph...
Host: Launching graph with loop counter set to 0
GPU: counter = 0
GPU: capturedWhileEmptyKernel()
Host: Launching graph with loop counter set to 10
GPU: counter = 10
GPU: counter = 9
GPU: counter = 8
GPU: counter = 7
GPU: counter = 6
GPU: counter = 5
GPU: counter = 4
GPU: counter = 3
GPU: counter = 2
GPU: counter = 1
GPU: capturedWhileEmptyKernel()
capturedWhileGraph: Complete

****************************************************************
################################################################
3_CUDA_Features/graphMemoryFootprint
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Driver version is: 12.6
Running sample.
================================
Running virtual address reuse example.
Sequential allocations & frees within a single graph enable CUDA to reuse virtual addresses.

Check confirms that d_a and d_b share a virtual address.
    FOOTPRINT: 67108864 bytes

Cleaning up example by trimming device memory.
    FOOTPRINT: 0 bytes

================================
Running physical memory reuse example.
CUDA reuses the same physical memory for allocations from separate graphs when the allocation lifetimes don't overlap.

Creating the graph execs does not reserve any physical memory.
    FOOTPRINT: 0 bytes

The first graph launched reserves the memory it needs.
    FOOTPRINT: 67108864 bytes
A subsequent launch of the same graph in the same stream reuses the same physical memory. Thus the memory footprint does not grow here.
    FOOTPRINT: 67108864 bytes

Subsequent launches of other graphs in the same stream also reuse the physical memory. Thus the memory footprint does not grow here.
01:     FOOTPRINT: 67108864 bytes
02:     FOOTPRINT: 67108864 bytes
03:     FOOTPRINT: 67108864 bytes
04:     FOOTPRINT: 67108864 bytes
05:     FOOTPRINT: 67108864 bytes
06:     FOOTPRINT: 67108864 bytes
07:     FOOTPRINT: 67108864 bytes

Check confirms all graphs use a different virtual address.

Cleaning up example by trimming device memory.
    FOOTPRINT: 0 bytes

================================
Running simultaneous streams example.
Graphs that can run concurrently need separate physical memory. In this example, each graph launched in a separate stream increases the total memory footprint.

When launching a new graph, CUDA may reuse physical memory from a graph whose execution has already finished -- even if the new graph is being launched in a different stream from the completed graph. Therefore, a kernel node is added to the graphs to increase runtime.

Initial footprint:
    FOOTPRINT: 0 bytes

Each graph launch in a seperate stream grows the memory footprint:
01:     FOOTPRINT: 67108864 bytes
02:     FOOTPRINT: 134217728 bytes
03:     FOOTPRINT: 201326592 bytes
04:     FOOTPRINT: 268435456 bytes
05:     FOOTPRINT: 335544320 bytes
06:     FOOTPRINT: 402653184 bytes
07:     FOOTPRINT: 469762048 bytes

Cleaning up example by trimming device memory.
    FOOTPRINT: 0 bytes

================================
Running unfreed streams example.
CUDA cannot reuse phyiscal memory from graphs which do not free their allocations.

Despite being launched in the same stream, each graph launch grows the memory footprint. Since the allocation is not freed, CUDA keeps the memory valid for use.
00:     FOOTPRINT: 67108864 bytes
01:     FOOTPRINT: 134217728 bytes
02:     FOOTPRINT: 201326592 bytes
03:     FOOTPRINT: 268435456 bytes
04:     FOOTPRINT: 335544320 bytes
05:     FOOTPRINT: 402653184 bytes
06:     FOOTPRINT: 469762048 bytes
07:     FOOTPRINT: 536870912 bytes

Trimming does not impact the memory footprint since the un-freed allocations are still holding onto the memory.
    FOOTPRINT: 536870912 bytes

Freeing the allocations does not shrink the footprint.
    FOOTPRINT: 536870912 bytes

Since the allocations are now freed, trimming does reduce the footprint even when the graph execs are not yet destroyed.
    FOOTPRINT: 0 bytes

Cleaning up example by trimming device memory.
    FOOTPRINT: 0 bytes

================================
Sample complete.
****************************************************************
################################################################
3_CUDA_Features/graphMemoryNodes
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Driver version is: 12.6
Setting up sample.
Setup complete.

Running negateSquares in a stream.
Validating negateSquares in a stream...
Validation PASSED!

Running negateSquares in a stream-captured graph.
Validating negateSquares in a stream-captured graph...
Validation PASSED!

Running negateSquares in an explicitly constructed graph.
Check verified that d_negSquare and d_input share a virtual address.
Validating negateSquares in an explicitly constructed graph...
Validation PASSED!

Running negateSquares with d_negSquare freed outside the stream.
Check verified that d_negSquare and d_input share a virtual address.
Validating negateSquares with d_negSquare freed outside the stream...
Validation PASSED!

Running negateSquares with d_negSquare freed outside the graph.
Validating negateSquares with d_negSquare freed outside the graph...
Validation PASSED!

Running negateSquares with d_negSquare freed in a different graph.
Validating negateSquares with d_negSquare freed in a different graph...
Validation PASSED!

Cleaning up sample.
Cleanup complete. Exiting sample.
****************************************************************
################################################################
3_CUDA_Features/immaTensorCoreGemm
################################################################
make: Nothing to be done for 'all'.
Initializing...
GPU Device 0: "Ampere" with compute capability 8.6

M: 4096 (16 x 256)
N: 4096 (16 x 256)
K: 4096 (16 x 256)
Preparing data for GPU...
Required shared memory size: 64 Kb
Computing... using high performance kernel compute_gemm_imma 
Time: 5.509344 ms
TOPS: 24.95
****************************************************************
################################################################
3_CUDA_Features/jacobiCudaGraphs
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

CPU iterations : 2954
CPU error : 4.988e-03
CPU Processing time: 1907.171997 (ms)
GPU iterations : 2954
GPU error : 4.988e-03
GPU Processing time: 316.752014 (ms)
&&&& jacobiCudaGraphs PASSED
****************************************************************
################################################################
3_CUDA_Features/memMapIPCDrv
################################################################
make: Nothing to be done for 'all'.
> findModulePath found file at <./memMapIpc_kernel64.ptx>
> initCUDA loading module: <./memMapIpc_kernel64.ptx>
> PTX JIT log:

Step 0 done
Process 0: verifying...
****************************************************************
################################################################
3_CUDA_Features/newdelete
################################################################
make: Nothing to be done for 'all'.
newdelete Starting...

GPU Device 0: "Ampere" with compute capability 8.6

 > Container = Vector test OK

 > Container = Vector, using placement new on SMEM buffer test OK

 > Container = Vector, with user defined datatype test OK

Test Summary: 3/3 succesfully run
****************************************************************
################################################################
3_CUDA_Features/ptxjit
################################################################
make: Nothing to be done for 'all'.
[PTX Just In Time (JIT) Compilation (no-qatest)] - Starting...
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> findModulePath <./ptxjit_kernel64.ptx>
> initCUDA loading module: <./ptxjit_kernel64.ptx>
Loading ptxjit_kernel[] program
CUDA Link Completed in 0.000000ms. Linker Output:
info    : 0 bytes gmem
info    : Function properties for 'myKernel':
info    : used 8 registers, used 0 barriers, 0 stack, 0 bytes smem, 360 bytes cmem[0], 0 bytes lmem
CUDA kernel launched
****************************************************************
################################################################
3_CUDA_Features/simpleCudaGraphs
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

16777216 elements
threads per block  = 512
Graph Launch iterations = 3

Num of nodes in the graph created manually = 7
[cudaGraphsManual] Host callback final reduced sum = 0.996214
[cudaGraphsManual] Host callback final reduced sum = 0.996214
[cudaGraphsManual] Host callback final reduced sum = 0.996214
Cloned Graph Output.. 
[cudaGraphsManual] Host callback final reduced sum = 0.996214
[cudaGraphsManual] Host callback final reduced sum = 0.996214
[cudaGraphsManual] Host callback final reduced sum = 0.996214

Num of nodes in the graph created using stream capture API = 7
[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214
[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214
[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214
Cloned Graph Output.. 
[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214
[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214
[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214
****************************************************************
################################################################
3_CUDA_Features/tf32TensorCoreGemm
################################################################
>>> GCC Version is greater or equal to  5.1.0 <<<
make: Nothing to be done for 'all'.
Initializing...
GPU Device 0: "Ampere" with compute capability 8.6

M: 8192 (16 x 512)
N: 8192 (16 x 512)
K: 4096 (8 x 512)
Preparing data for GPU...
Required shared memory size: 72 Kb
Computing using high performance kernel = 0 - compute_tf32gemm_async_copy
Time: 396.013580 ms
TFLOPS: 1.39
****************************************************************
################################################################
3_CUDA_Features/warpAggregatedAtomicsCG
################################################################
>>> GCC Version is greater or equal to 4.7.0 <<<
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

CPU max matches GPU max

Warp Aggregated Atomics PASSED 
****************************************************************
################################################################
4_CUDA_Libraries/FilterBorderControlNPP
################################################################
test.c:1:10: fatal error: FreeImage.h: No such file or directory
    1 | #include "FreeImage.h"
      |          ^~~~~~~~~~~~~
compilation terminated.
>>> WARNING - FreeImage is not set up correctly. Please ensure FreeImage is set up correctly. <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -I../../../Common/UtilNPP -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=compute_50 -o FilterBorderControlNPP.o -c FilterBorderControlNPP.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=compute_50 -o FilterBorderControlNPP FilterBorderControlNPP.o -lnppisu_static -lnppif_static -lnppitc_static -lnppidei_static -lnppial_static -lnppc_static -lculibos -lfreeimage
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp FilterBorderControlNPP ../../../bin/x86_64/linux/release
./all_projects.sh: line 926: ./FilterBorderControlNPP: No such file or directory
****************************************************************
################################################################
4_CUDA_Libraries/MersenneTwisterGP11213
################################################################
make: Nothing to be done for 'all'.
./MersenneTwisterGP11213 Starting...

GPU Device 0: "Ampere" with compute capability 8.6

Allocating data for 2400000 samples...
Seeding with 777 ...
Generating random numbers on GPU...


Reading back the results...
Generating random numbers on CPU...

Comparing CPU/GPU random numbers...

Max absolute error: 0.000000E+00
L1 norm: 0.000000E+00

MersenneTwisterGP11213, Throughput = 28.2021 GNumbers/s, Time = 0.00009 s, Size = 2400000 Numbers
Shutting down...
****************************************************************
################################################################
4_CUDA_Libraries/batchCUBLAS
################################################################
make: Nothing to be done for 'all'.
batchCUBLAS Starting...

GPU Device 0: "Ampere" with compute capability 8.6


 ==== Running single kernels ==== 

Testing sgemm
#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0xbf800000, -1) beta= (0x40000000, 2)
#### args: lda=128 ldb=128 ldc=128
^^^^ elapsed = 0.04416108 sec  GFLOPS=0.0949774
@@@@ sgemm test OK
Testing dgemm
#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0x0000000000000000, 0) beta= (0x0000000000000000, 0)
#### args: lda=128 ldb=128 ldc=128
^^^^ elapsed = 0.00024891 sec  GFLOPS=16.8508
@@@@ dgemm test OK

 ==== Running N=10 without streams ==== 

Testing sgemm
#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0xbf800000, -1) beta= (0x00000000, 0)
#### args: lda=128 ldb=128 ldc=128
^^^^ elapsed = 0.00015783 sec  GFLOPS=265.743
@@@@ sgemm test OK
Testing dgemm
#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0xbff0000000000000, -1) beta= (0x0000000000000000, 0)
#### args: lda=128 ldb=128 ldc=128
^^^^ elapsed = 0.02390313 sec  GFLOPS=1.75471
@@@@ dgemm test OK

 ==== Running N=10 with streams ==== 

Testing sgemm
#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0x40000000, 2) beta= (0x40000000, 2)
#### args: lda=128 ldb=128 ldc=128
^^^^ elapsed = 0.00014901 sec  GFLOPS=281.475
@@@@ sgemm test OK
Testing dgemm
#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0xbff0000000000000, -1) beta= (0x0000000000000000, 0)
#### args: lda=128 ldb=128 ldc=128
^^^^ elapsed = 0.00037193 sec  GFLOPS=112.77
@@@@ dgemm test OK

 ==== Running N=10 batched ==== 

Testing sgemm
#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0x3f800000, 1) beta= (0xbf800000, -1)
#### args: lda=128 ldb=128 ldc=128
^^^^ elapsed = 0.00020409 sec  GFLOPS=205.516
@@@@ sgemm test OK
Testing dgemm
#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0xbff0000000000000, -1) beta= (0x4000000000000000, 2)
#### args: lda=128 ldb=128 ldc=128
^^^^ elapsed = 0.00042510 sec  GFLOPS=98.6662
@@@@ dgemm test OK

Test Summary
0 error(s)
****************************************************************
################################################################
4_CUDA_Libraries/batchedLabelMarkersAndLabelCompressionNPP
################################################################
make: Nothing to be done for 'all'.
NPP Library Version 12.3.1
CUDA Driver  Version: 12.6
CUDA Runtime Version: 12.6

Input file load succeeded.
teapot_CompressedMarkerLabelsUF_8Way_512x512_32u succeeded, compressed label count is 155343.
Input file load succeeded.
CT_Skull_CompressedMarkerLabelsUF_8Way_512x512_32u succeeded, compressed label count is 436.
Input file load succeeded.
PCB_METAL_CompressedMarkerLabelsUF_8Way_509x335_32u succeeded, compressed label count is 3733.
Input file load succeeded.
PCB2_CompressedMarkerLabelsUF_8Way_1024x683_32u succeeded, compressed label count is 1274.
Input file load succeeded.
PCB_CompressedMarkerLabelsUF_8Way_1280x720_32u succeeded, compressed label count is 1467.


teapot_CompressedMarkerLabelsUFBatch_8Way_512x512_32u succeeded, compressed label count is 155342.
CT_Skull_CompressedMarkerLabelsUFBatch_8Way_512x512_32u succeeded, compressed label count is 414.
PCB_METAL_CompressedMarkerLabelsUFBatch_8Way_509x335_32u succeeded, compressed label count is 3732.
PCB2_CompressedMarkerLabelsUFBatch_8Way_1024x683_32u succeeded, compressed label count is 1220.
PCB_CompressedMarkerLabelsUFBatch_8Way_1280x720_32u succeeded, compressed label count is 1458.
****************************************************************
################################################################
4_CUDA_Libraries/boxFilterNPP
################################################################
test.c:1:10: fatal error: FreeImage.h: No such file or directory
    1 | #include "FreeImage.h"
      |          ^~~~~~~~~~~~~
compilation terminated.
>>> WARNING - FreeImage is not set up correctly. Please ensure FreeImage is set up correctly. <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -I../../../Common/UtilNPP -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=compute_50 -o boxFilterNPP.o -c boxFilterNPP.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=compute_50 -o boxFilterNPP boxFilterNPP.o -lnppisu_static -lnppif_static -lnppc_static -lculibos -lfreeimage
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp boxFilterNPP ../../../bin/x86_64/linux/release
./all_projects.sh: line 958: ./boxFilterNPP: No such file or directory
****************************************************************
################################################################
4_CUDA_Libraries/cannyEdgeDetectorNPP
################################################################
test.c:1:10: fatal error: FreeImage.h: No such file or directory
    1 | #include "FreeImage.h"
      |          ^~~~~~~~~~~~~
compilation terminated.
>>> WARNING - FreeImage is not set up correctly. Please ensure FreeImage is set up correctly. <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -I../../../Common/UtilNPP -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=compute_50 -o cannyEdgeDetectorNPP.o -c cannyEdgeDetectorNPP.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=compute_50 -o cannyEdgeDetectorNPP cannyEdgeDetectorNPP.o -lnppisu_static -lnppif_static -lnppc_static -lculibos -lfreeimage
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp cannyEdgeDetectorNPP ../../../bin/x86_64/linux/release
./all_projects.sh: line 966: ./cannyEdgeDetectorNPP: No such file or directory
****************************************************************
################################################################
4_CUDA_Libraries/conjugateGradient
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

> GPU device has 28 Multi-Processors, SM 8.6 compute capabilities

iteration =   1, residual = 4.449883e+01
iteration =   2, residual = 3.245218e+00
iteration =   3, residual = 2.690220e-01
iteration =   4, residual = 2.307639e-02
iteration =   5, residual = 1.993140e-03
iteration =   6, residual = 1.846193e-04
iteration =   7, residual = 1.693379e-05
iteration =   8, residual = 1.600115e-06
Test Summary:  Error amount = 0.000000
****************************************************************
################################################################
4_CUDA_Libraries/conjugateGradientCudaGraphs
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

> GPU device has 28 Multi-Processors, SM 8.6 compute capabilities

iteration =   1, residual = 4.449883e+01
iteration =   2, residual = 3.245218e+00
iteration =   3, residual = 2.690220e-01
iteration =   4, residual = 2.307639e-02
iteration =   5, residual = 1.993140e-03
iteration =   6, residual = 1.846193e-04
iteration =   7, residual = 1.693379e-05
iteration =   8, residual = 1.600115e-06
Test Summary:  Error amount = 0.000000
****************************************************************
################################################################
4_CUDA_Libraries/conjugateGradientMultiBlockCG
################################################################
>>> GCC Version is greater or equal to 4.7.0 <<<
make: Nothing to be done for 'all'.
Starting [conjugateGradientMultiBlockCG]...
GPU Device 0: "Ampere" with compute capability 8.6

> GPU device has 28 Multi-Processors, SM 8.6 compute capabilities

GPU Final, residual = 1.600115e-06, kernel execution time = 57.222336 ms
Test Summary:  Error amount = 0.000000 
&&&& conjugateGradientMultiBlockCG PASSED
****************************************************************
################################################################
4_CUDA_Libraries/conjugateGradientMultiDeviceCG
################################################################
>>> GCC Version is greater or equal to 4.7.0 <<<
make: Nothing to be done for 'all'.
Starting [conjugateGradientMultiDeviceCG]...
GPU Device 0: "NVIDIA GeForce RTX 3060" with compute capability 8.6
No two or more GPUs with same architecture capable of concurrentManagedAccess found. 
Waiving the sample
****************************************************************
################################################################
4_CUDA_Libraries/conjugateGradientPrecond
################################################################
make: Nothing to be done for 'all'.
conjugateGradientPrecond starting...
GPU Device 0: "Ampere" with compute capability 8.6

GPU selected Device ID = 0 
> GPU device has 28 Multi-Processors, SM 8.6 compute capabilities

laplace dimension = 128
Convergence of CG without preconditioning: 
  iteration = 564, residual = 9.039019e-13 
  Convergence Test: OK 

Convergence of CG using ILU(0) preconditioning: 
  iteration = 188, residual = 9.067234e-13 
  Convergence Test: OK 

Test Summary:
   Counted total of 0 errors
   qaerr1 = 0.000005 qaerr2 = 0.000003

****************************************************************
################################################################
4_CUDA_Libraries/conjugateGradientUM
################################################################
make: Nothing to be done for 'all'.
Starting [conjugateGradientUM]...
GPU Device 0: "Ampere" with compute capability 8.6

> GPU device has 28 Multi-Processors, SM 8.6 compute capabilities

iteration =   1, residual = 4.449883e+01
iteration =   2, residual = 3.245218e+00
iteration =   3, residual = 2.690220e-01
iteration =   4, residual = 2.307639e-02
iteration =   5, residual = 1.993140e-03
iteration =   6, residual = 1.846193e-04
iteration =   7, residual = 1.693379e-05
iteration =   8, residual = 1.600115e-06
Final residual: 1.600115e-06
&&&& conjugateGradientUM PASSED
Test Summary:  Error amount = 0.000000, result = SUCCESS
****************************************************************
################################################################
4_CUDA_Libraries/cuDLAErrorReporting
################################################################
>>> WARNING - cuDLAErrorReporting is not supported on Linux x86_64 - waiving sample <<<
>>> GCC Version is greater or equal to 4.7.0 <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o main.o -c main.cu
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o cuDLAErrorReporting main.o -lcudla
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp cuDLAErrorReporting ../../../bin/x86_64/linux/release
./all_projects.sh: line 1022: ./cuDLAErrorReporting: No such file or directory
****************************************************************
################################################################
4_CUDA_Libraries/cuDLAHybridMode
################################################################
>>> WARNING - cuDLAHybridMode is not supported on Linux x86_64 - waiving sample <<<
>>> GCC Version is greater or equal to 4.7.0 <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o main.o -c main.cu
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o cuDLAHybridMode main.o -lcudla
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp cuDLAHybridMode ../../../bin/x86_64/linux/release
./all_projects.sh: line 1030: ./cuDLAHybridMode: No such file or directory
****************************************************************
################################################################
4_CUDA_Libraries/cuDLALayerwiseStatsHybrid
################################################################
>>> WARNING - cuDLALayerwiseStatsHybrid is not supported on Linux x86_64 - waiving sample <<<
>>> GCC Version is greater or equal to 4.7.0 <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o main.o -c main.cu
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o cuDLALayerwiseStatsHybrid main.o -lcudla
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp cuDLALayerwiseStatsHybrid ../../../bin/x86_64/linux/release
./all_projects.sh: line 1038: ./cuDLALayerwiseStatsHybrid: No such file or directory
****************************************************************
################################################################
4_CUDA_Libraries/cuDLALayerwiseStatsStandalone
################################################################
>>> WARNING - cuDLALayerwiseStatsStandalone is not supported on Linux x86_64 - waiving sample <<<
>>> WARNING - libnvscibuf.so not found, Waiving the sample <<<
>>> WARNING - libnvscisync.so not found, Waiving the sample <<<
>>> WARNING - nvscibuf.h not found, Waiving the sample <<<
>>> WARNING - nvscisync.h not found, Waiving the sample <<<
>>> GCC Version is greater or equal to 4.7.0 <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o main.o -c main.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o cuDLALayerwiseStatsStandalone main.o -L/usr/lib/aarch64-linux-gnu/nvidia -lcudla -lnvscibuf -lnvscisync
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp cuDLALayerwiseStatsStandalone ../../../bin/x86_64/linux/release
./all_projects.sh: line 1046: ./cuDLALayerwiseStatsStandalone: No such file or directory
****************************************************************
################################################################
4_CUDA_Libraries/cuDLAStandaloneMode
################################################################
>>> WARNING - cuDLAStandaloneMode is not supported on Linux x86_64 - waiving sample <<<
>>> WARNING - libnvscibuf.so not found, Waiving the sample <<<
>>> WARNING - libnvscisync.so not found, Waiving the sample <<<
>>> WARNING - nvscibuf.h not found, Waiving the sample <<<
>>> WARNING - nvscisync.h not found, Waiving the sample <<<
>>> GCC Version is greater or equal to 4.7.0 <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o main.o -c main.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o cuDLAStandaloneMode main.o -L/usr/lib/aarch64-linux-gnu/nvidia -lcudla -lnvscibuf -lnvscisync
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp cuDLAStandaloneMode ../../../bin/x86_64/linux/release
./all_projects.sh: line 1054: ./cuDLAStandaloneMode: No such file or directory
****************************************************************
################################################################
4_CUDA_Libraries/cuSolverDn_LinearSolver
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

step 1: read matrix market format
Using default input file [./gr_900_900_crg.mtx]
sparse matrix A is 900 x 900 with 7744 nonzeros, base=1
step 2: convert CSR(A) to dense matrix
step 3: set right hand side vector (b) to 1
step 4: prepare data on device
step 5: solve A*x = b 
timing: cholesky =   0.013473 sec
step 6: evaluate residual
|b - A*x| = 1.278977E-13 
|A| = 1.600000E+01 
|x| = 2.357708E+01 
|b - A*x|/(|A|*|x|) = 3.390413E-16 
****************************************************************
################################################################
4_CUDA_Libraries/cuSolverRf
################################################################
make: Nothing to be done for 'all'.
step 1.1: preparation
step 1.1: read matrix market format
GPU Device 0: "Ampere" with compute capability 8.6

Using default input file [./lap2D_5pt_n100.mtx]
WARNING: cusolverRf only works for base-0 
sparse matrix A is 10000 x 10000 with 49600 nonzeros, base=0
step 1.2: set right hand side vector (b) to 1
step 2: reorder the matrix to reduce zero fill-in
        Q = symrcm(A) or Q = symamd(A) 
step 3: B = Q*A*Q^T
step 4: solve A*x = b by LU(B) in cusolverSp
step 4.1: create opaque info structure
step 4.2: analyze LU(B) to know structure of Q and R, and upper bound for nnz(L+U)
step 4.3: workspace for LU(B)
step 4.4: compute Ppivot*B = L*U 
step 4.5: check if the matrix is singular 
step 4.6: solve A*x = b 
    i.e.  solve B*(Qx) = Q*b 
step 4.7: evaluate residual r = b - A*x (result on CPU)
(CPU) |b - A*x| = 4.547474E-12 
(CPU) |A| = 8.000000E+00 
(CPU) |x| = 7.513384E+02 
(CPU) |b - A*x|/(|A|*|x|) = 7.565621E-16 
step 5: extract P, Q, L and U from P*B*Q^T = L*U 
        L has implicit unit diagonal
nnzL = 671550, nnzU = 681550
step 6: form P*A*Q^T = L*U
step 6.1: P = Plu*Qreroder
step 6.2: Q = Qlu*Qreorder 
step 7: create cusolverRf handle
step 8: set parameters for cusolverRf 
step 9: assemble P*A*Q = L*U 
step 10: analyze to extract parallelism 
step 11: import A to cusolverRf 
step 12: refactorization 
step 13: solve A*x = b 
step 14: evaluate residual r = b - A*x (result on GPU)
(GPU) |b - A*x| = 4.320100E-12 
(GPU) |A| = 8.000000E+00 
(GPU) |x| = 7.513384E+02 
(GPU) |b - A*x|/(|A|*|x|) = 7.187340E-16 
===== statistics 
 nnz(A) = 49600, nnz(L+U) = 1353100, zero fill-in ratio = 27.280242

===== timing profile 
 reorder A   : 0.003769 sec
 B = Q*A*Q^T : 0.000843 sec

 cusolverSp LU analysis: 0.000222 sec
 cusolverSp LU factor  : 0.083953 sec
 cusolverSp LU solve   : 0.002775 sec
 cusolverSp LU extract : 0.010378 sec

 cusolverRf assemble : 0.012687 sec
 cusolverRf reset    : 0.000200 sec
 cusolverRf refactor : 0.101585 sec
 cusolverRf solve    : 0.136594 sec
****************************************************************
################################################################
4_CUDA_Libraries/cuSolverSp_LinearSolver
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Using default input file [./lap2D_5pt_n100.mtx]
step 1: read matrix market format
sparse matrix A is 10000 x 10000 with 49600 nonzeros, base=1
step 2: reorder the matrix A to minimize zero fill-in
        if the user choose a reordering by -P=symrcm, -P=symamd or -P=metis
step 2.1: no reordering is chosen, Q = 0:n-1 
step 2.2: B = A(Q,Q) 
step 3: b(j) = 1 + j/n 
step 4: prepare data on device
step 5: solve A*x = b on CPU 
step 6: evaluate residual r = b - A*x (result on CPU)
(CPU) |b - A*x| = 5.393685E-12 
(CPU) |A| = 8.000000E+00 
(CPU) |x| = 1.136492E+03 
(CPU) |b| = 1.999900E+00 
(CPU) |b - A*x|/(|A|*|x| + |b|) = 5.931079E-16 
step 7: solve A*x = b on GPU
step 8: evaluate residual r = b - A*x (result on GPU)
(GPU) |b - A*x| = 1.970424E-12 
(GPU) |A| = 8.000000E+00 
(GPU) |x| = 1.136492E+03 
(GPU) |b| = 1.999900E+00 
(GPU) |b - A*x|/(|A|*|x| + |b|) = 2.166745E-16 
timing chol: CPU =   0.116898 sec , GPU =   0.116724 sec
show last 10 elements of solution vector (GPU) 
consistent result for different reordering and solver 
x[9990] = 3.000016E+01
x[9991] = 2.807343E+01
x[9992] = 2.601354E+01
x[9993] = 2.380285E+01
x[9994] = 2.141866E+01
x[9995] = 1.883070E+01
x[9996] = 1.599668E+01
x[9997] = 1.285365E+01
x[9998] = 9.299423E+00
x[9999] = 5.147265E+00
****************************************************************
################################################################
4_CUDA_Libraries/cuSolverSp_LowlevelCholesky
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Using default input file [./lap2D_5pt_n100.mtx]
step 1: read matrix market format
sparse matrix A is 10000 x 10000 with 49600 nonzeros, base=1
step 2: create opaque info structure
step 3: analyze chol(A) to know structure of L
step 4: workspace for chol(A)
step 5: compute A = L*L^T 
step 6: check if the matrix is singular 
step 7: solve A*x = b 
step 8: evaluate residual r = b - A*x (result on CPU)
(CPU) |b - A*x| = 3.637979E-12 
(CPU) |A| = 8.000000E+00 
(CPU) |x| = 7.513384E+02 
(CPU) |b - A*x|/(|A|*|x|) = 6.052497E-16 
step 9: create opaque info structure
step 10: analyze chol(A) to know structure of L
step 11: workspace for chol(A)
step 12: compute A = L*L^T 
step 13: check if the matrix is singular 
step 14: solve A*x = b 
(GPU) |b - A*x| = 1.477929E-12 
(GPU) |b - A*x|/(|A|*|x|) = 2.458827E-16 
****************************************************************
################################################################
4_CUDA_Libraries/cuSolverSp_LowlevelQR
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Using default input file [./lap2D_5pt_n32.mtx]
step 1: read matrix market format
sparse matrix A is 1024 x 1024 with 3008 nonzeros, base=1
step 2: create opaque info structure
step 3: analyze qr(A) to know structure of L
step 4: workspace for qr(A)
step 5: compute A = L*L^T 
step 6: check if the matrix is singular 
step 7: solve A*x = b 
step 8: evaluate residual r = b - A*x (result on CPU)
(CPU) |b - A*x| = 5.329071E-15 
(CPU) |A| = 6.000000E+00 
(CPU) |x| = 5.000000E-01 
(CPU) |b - A*x|/(|A|*|x|) = 1.776357E-15 
step 9: create opaque info structure
step 10: analyze qr(A) to know structure of L
step 11: workspace for qr(A)
GPU buffer size = 698112 bytes
step 12: compute A = L*L^T 
step 13: check if the matrix is singular 
step 14: solve A*x = b 
(GPU) |b - A*x| = 4.218847E-15 
(GPU) |b - A*x|/(|A|*|x|) = 1.406282E-15 
****************************************************************
################################################################
4_CUDA_Libraries/cudaNvSci
################################################################
>>> WARNING - libnvscibuf.so not found, Waiving the sample <<<
>>> WARNING - libnvscisync.so not found, Waiving the sample <<<
>>> WARNING - nvscibuf.h not found, Waiving the sample <<<
>>> WARNING - nvscisync.h not found, Waiving the sample <<<
>>> GCC Version is greater or equal to 4.7.0 <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o cudaNvSci.o -c cudaNvSci.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o imageKernels.o -c imageKernels.cu
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o main.o -c main.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o cudaNvSci cudaNvSci.o imageKernels.o main.o -L/usr/local/cuda/lib64/stubs -lcuda -L/usr/lib/aarch64-linux-gnu/nvidia -lnvscibuf -lnvscisync
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp cudaNvSci ../../../bin/x86_64/linux/release
./all_projects.sh: line 1102: ./cudaNvSci: No such file or directory
****************************************************************
################################################################
4_CUDA_Libraries/cudaNvSciNvMedia
################################################################
>>> WARNING - cudaNvSciNvMedia is not supported on Linux x86_64 - waiving sample <<<
>>> WARNING - libnvscibuf.so not found, Waiving the sample <<<
>>> WARNING - libnvscisync.so not found, Waiving the sample <<<
>>> WARNING - nvscibuf.h not found, Waiving the sample <<<
>>> WARNING - nvscisync.h not found, Waiving the sample <<<
>>> WARNING - libnvmedia.so not found, Waiving the sample <<<
>>> WARNING - nvmedia headers not found, Waiving the sample <<<
>>> GCC Version is greater or equal to 4.7.0 <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o cuda_consumer.o -c cuda_consumer.cu
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o main.o -c main.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o nvmedia_producer.o -c nvmedia_producer.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o cmdline.o -c nvmedia_utils/cmdline.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o config_parser.o -c nvmedia_utils/config_parser.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o image_utils.o -c nvmedia_utils/image_utils.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o log_utils.o -c nvmedia_utils/log_utils.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o misc_utils.o -c nvmedia_utils/misc_utils.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o nvsci_setup.o -c nvsci_setup.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o cudaNvSciNvMedia cuda_consumer.o main.o nvmedia_producer.o cmdline.o config_parser.o image_utils.o log_utils.o misc_utils.o nvsci_setup.o -L/usr/local/cuda/lib64/stubs -lcuda -L/usr/lib/aarch64-linux-gnu/nvidia -lnvscibuf -lnvscisync -lnvmedia
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp cudaNvSciNvMedia ../../../bin/x86_64/linux/release
./all_projects.sh: line 1110: ./cudaNvSciNvMedia: No such file or directory
****************************************************************
################################################################
4_CUDA_Libraries/freeImageInteropNPP
################################################################
test.c:1:10: fatal error: FreeImage.h: No such file or directory
    1 | #include "FreeImage.h"
      |          ^~~~~~~~~~~~~
compilation terminated.
>>> WARNING - FreeImage is not set up correctly. Please ensure FreeImage is set up correctly. <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -I../../../Common/UtilNPP -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=compute_50 -o freeImageInteropNPP.o -c freeImageInteropNPP.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=compute_50 -o freeImageInteropNPP freeImageInteropNPP.o -lnppisu -lnppif -lnppc -lfreeimage
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp freeImageInteropNPP ../../../bin/x86_64/linux/release
./all_projects.sh: line 1118: ./freeImageInteropNPP: No such file or directory
****************************************************************
################################################################
4_CUDA_Libraries/histEqualizationNPP
################################################################
test.c:1:10: fatal error: FreeImage.h: No such file or directory
    1 | #include "FreeImage.h"
      |          ^~~~~~~~~~~~~
compilation terminated.
>>> WARNING - FreeImage is not set up correctly. Please ensure FreeImage is set up correctly. <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -I../../../Common/UtilNPP -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=compute_50 -o histEqualizationNPP.o -c histEqualizationNPP.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=compute_50 -o histEqualizationNPP histEqualizationNPP.o -lnppisu -lnppist -lnppicc -lnppc -lfreeimage
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp histEqualizationNPP ../../../bin/x86_64/linux/release
./all_projects.sh: line 1126: ./histEqualizationNPP: No such file or directory
****************************************************************
################################################################
4_CUDA_Libraries/jitLto
################################################################
make: Nothing to be done for 'all'.
CUDA 12.6


5.1 * 0 + 0 = 0
5.1 * 1 + 2 = 7.1
5.1 * 2 + 4 = 14.2
5.1 * 3 + 6 = 21.3
5.1 * 4 + 8 = 28.4
5.1 * 5 + 10 = 35.5
5.1 * 6 + 12 = 42.6
5.1 * 7 + 14 = 49.7
5.1 * 8 + 16 = 56.8
5.1 * 9 + 18 = 63.9
5.1 * 10 + 20 = 71
5.1 * 11 + 22 = 78.1
5.1 * 12 + 24 = 85.2
5.1 * 13 + 26 = 92.3
5.1 * 14 + 28 = 99.4
5.1 * 15 + 30 = 106.5
5.1 * 16 + 32 = 113.6
5.1 * 17 + 34 = 120.7
5.1 * 18 + 36 = 127.8
5.1 * 19 + 38 = 134.9
5.1 * 20 + 40 = 142
5.1 * 21 + 42 = 149.1
5.1 * 22 + 44 = 156.2
5.1 * 23 + 46 = 163.3
5.1 * 24 + 48 = 170.4
5.1 * 25 + 50 = 177.5
5.1 * 26 + 52 = 184.6
5.1 * 27 + 54 = 191.7
5.1 * 28 + 56 = 198.8
5.1 * 29 + 58 = 205.9
5.1 * 30 + 60 = 213
5.1 * 31 + 62 = 220.1
5.1 * 32 + 64 = 227.2
5.1 * 33 + 66 = 234.3
5.1 * 34 + 68 = 241.4
5.1 * 35 + 70 = 248.5
5.1 * 36 + 72 = 255.6
5.1 * 37 + 74 = 262.7
5.1 * 38 + 76 = 269.8
5.1 * 39 + 78 = 276.9
5.1 * 40 + 80 = 284
5.1 * 41 + 82 = 291.1
5.1 * 42 + 84 = 298.2
5.1 * 43 + 86 = 305.3
5.1 * 44 + 88 = 312.4
5.1 * 45 + 90 = 319.5
5.1 * 46 + 92 = 326.6
5.1 * 47 + 94 = 333.7
5.1 * 48 + 96 = 340.8
5.1 * 49 + 98 = 347.9
5.1 * 50 + 100 = 355
5.1 * 51 + 102 = 362.1
5.1 * 52 + 104 = 369.2
5.1 * 53 + 106 = 376.3
5.1 * 54 + 108 = 383.4
5.1 * 55 + 110 = 390.5
5.1 * 56 + 112 = 397.6
5.1 * 57 + 114 = 404.7
5.1 * 58 + 116 = 411.8
5.1 * 59 + 118 = 418.9
5.1 * 60 + 120 = 426
5.1 * 61 + 122 = 433.1
5.1 * 62 + 124 = 440.2
5.1 * 63 + 126 = 447.3
5.1 * 64 + 128 = 454.4
5.1 * 65 + 130 = 461.5
5.1 * 66 + 132 = 468.6
5.1 * 67 + 134 = 475.7
5.1 * 68 + 136 = 482.8
5.1 * 69 + 138 = 489.9
5.1 * 70 + 140 = 497
5.1 * 71 + 142 = 504.1
5.1 * 72 + 144 = 511.2
5.1 * 73 + 146 = 518.3
5.1 * 74 + 148 = 525.4
5.1 * 75 + 150 = 532.5
5.1 * 76 + 152 = 539.6
5.1 * 77 + 154 = 546.7
5.1 * 78 + 156 = 553.8
5.1 * 79 + 158 = 560.9
5.1 * 80 + 160 = 568
5.1 * 81 + 162 = 575.1
5.1 * 82 + 164 = 582.2
5.1 * 83 + 166 = 589.3
5.1 * 84 + 168 = 596.4
5.1 * 85 + 170 = 603.5
5.1 * 86 + 172 = 610.6
5.1 * 87 + 174 = 617.7
5.1 * 88 + 176 = 624.8
5.1 * 89 + 178 = 631.9
5.1 * 90 + 180 = 639
5.1 * 91 + 182 = 646.1
5.1 * 92 + 184 = 653.2
5.1 * 93 + 186 = 660.3
5.1 * 94 + 188 = 667.4
5.1 * 95 + 190 = 674.5
5.1 * 96 + 192 = 681.6
5.1 * 97 + 194 = 688.7
5.1 * 98 + 196 = 695.8
5.1 * 99 + 198 = 702.9
5.1 * 100 + 200 = 710
5.1 * 101 + 202 = 717.1
5.1 * 102 + 204 = 724.2
5.1 * 103 + 206 = 731.3
5.1 * 104 + 208 = 738.4
5.1 * 105 + 210 = 745.5
5.1 * 106 + 212 = 752.6
5.1 * 107 + 214 = 759.7
5.1 * 108 + 216 = 766.8
5.1 * 109 + 218 = 773.9
5.1 * 110 + 220 = 781
5.1 * 111 + 222 = 788.1
5.1 * 112 + 224 = 795.2
5.1 * 113 + 226 = 802.3
5.1 * 114 + 228 = 809.4
5.1 * 115 + 230 = 816.5
5.1 * 116 + 232 = 823.6
5.1 * 117 + 234 = 830.7
5.1 * 118 + 236 = 837.8
5.1 * 119 + 238 = 844.9
5.1 * 120 + 240 = 852
5.1 * 121 + 242 = 859.1
5.1 * 122 + 244 = 866.2
5.1 * 123 + 246 = 873.3
5.1 * 124 + 248 = 880.4
5.1 * 125 + 250 = 887.5
5.1 * 126 + 252 = 894.6
5.1 * 127 + 254 = 901.7
5.1 * 128 + 256 = 908.8
5.1 * 129 + 258 = 915.9
5.1 * 130 + 260 = 923
5.1 * 131 + 262 = 930.1
5.1 * 132 + 264 = 937.2
5.1 * 133 + 266 = 944.3
5.1 * 134 + 268 = 951.4
5.1 * 135 + 270 = 958.5
5.1 * 136 + 272 = 965.6
5.1 * 137 + 274 = 972.7
5.1 * 138 + 276 = 979.8
5.1 * 139 + 278 = 986.9
5.1 * 140 + 280 = 994
5.1 * 141 + 282 = 1001.1
5.1 * 142 + 284 = 1008.2
5.1 * 143 + 286 = 1015.3
5.1 * 144 + 288 = 1022.4
5.1 * 145 + 290 = 1029.5
5.1 * 146 + 292 = 1036.6
5.1 * 147 + 294 = 1043.7
5.1 * 148 + 296 = 1050.8
5.1 * 149 + 298 = 1057.9
5.1 * 150 + 300 = 1065
5.1 * 151 + 302 = 1072.1
5.1 * 152 + 304 = 1079.2
5.1 * 153 + 306 = 1086.3
5.1 * 154 + 308 = 1093.4
5.1 * 155 + 310 = 1100.5
5.1 * 156 + 312 = 1107.6
5.1 * 157 + 314 = 1114.7
5.1 * 158 + 316 = 1121.8
5.1 * 159 + 318 = 1128.9
5.1 * 160 + 320 = 1136
5.1 * 161 + 322 = 1143.1
5.1 * 162 + 324 = 1150.2
5.1 * 163 + 326 = 1157.3
5.1 * 164 + 328 = 1164.4
5.1 * 165 + 330 = 1171.5
5.1 * 166 + 332 = 1178.6
5.1 * 167 + 334 = 1185.7
5.1 * 168 + 336 = 1192.8
5.1 * 169 + 338 = 1199.9
5.1 * 170 + 340 = 1207
5.1 * 171 + 342 = 1214.1
5.1 * 172 + 344 = 1221.2
5.1 * 173 + 346 = 1228.3
5.1 * 174 + 348 = 1235.4
5.1 * 175 + 350 = 1242.5
5.1 * 176 + 352 = 1249.6
5.1 * 177 + 354 = 1256.7
5.1 * 178 + 356 = 1263.8
5.1 * 179 + 358 = 1270.9
5.1 * 180 + 360 = 1278
5.1 * 181 + 362 = 1285.1
5.1 * 182 + 364 = 1292.2
5.1 * 183 + 366 = 1299.3
5.1 * 184 + 368 = 1306.4
5.1 * 185 + 370 = 1313.5
5.1 * 186 + 372 = 1320.6
5.1 * 187 + 374 = 1327.7
5.1 * 188 + 376 = 1334.8
5.1 * 189 + 378 = 1341.9
5.1 * 190 + 380 = 1349
5.1 * 191 + 382 = 1356.1
5.1 * 192 + 384 = 1363.2
5.1 * 193 + 386 = 1370.3
5.1 * 194 + 388 = 1377.4
5.1 * 195 + 390 = 1384.5
5.1 * 196 + 392 = 1391.6
5.1 * 197 + 394 = 1398.7
5.1 * 198 + 396 = 1405.8
5.1 * 199 + 398 = 1412.9
5.1 * 200 + 400 = 1420
5.1 * 201 + 402 = 1427.1
5.1 * 202 + 404 = 1434.2
5.1 * 203 + 406 = 1441.3
5.1 * 204 + 408 = 1448.4
5.1 * 205 + 410 = 1455.5
5.1 * 206 + 412 = 1462.6
5.1 * 207 + 414 = 1469.7
5.1 * 208 + 416 = 1476.8
5.1 * 209 + 418 = 1483.9
5.1 * 210 + 420 = 1491
5.1 * 211 + 422 = 1498.1
5.1 * 212 + 424 = 1505.2
5.1 * 213 + 426 = 1512.3
5.1 * 214 + 428 = 1519.4
5.1 * 215 + 430 = 1526.5
5.1 * 216 + 432 = 1533.6
5.1 * 217 + 434 = 1540.7
5.1 * 218 + 436 = 1547.8
5.1 * 219 + 438 = 1554.9
5.1 * 220 + 440 = 1562
5.1 * 221 + 442 = 1569.1
5.1 * 222 + 444 = 1576.2
5.1 * 223 + 446 = 1583.3
5.1 * 224 + 448 = 1590.4
5.1 * 225 + 450 = 1597.5
5.1 * 226 + 452 = 1604.6
5.1 * 227 + 454 = 1611.7
5.1 * 228 + 456 = 1618.8
5.1 * 229 + 458 = 1625.9
5.1 * 230 + 460 = 1633
5.1 * 231 + 462 = 1640.1
5.1 * 232 + 464 = 1647.2
5.1 * 233 + 466 = 1654.3
5.1 * 234 + 468 = 1661.4
5.1 * 235 + 470 = 1668.5
5.1 * 236 + 472 = 1675.6
5.1 * 237 + 474 = 1682.7
5.1 * 238 + 476 = 1689.8
5.1 * 239 + 478 = 1696.9
5.1 * 240 + 480 = 1704
5.1 * 241 + 482 = 1711.1
5.1 * 242 + 484 = 1718.2
5.1 * 243 + 486 = 1725.3
5.1 * 244 + 488 = 1732.4
5.1 * 245 + 490 = 1739.5
5.1 * 246 + 492 = 1746.6
5.1 * 247 + 494 = 1753.7
5.1 * 248 + 496 = 1760.8
5.1 * 249 + 498 = 1767.9
5.1 * 250 + 500 = 1775
5.1 * 251 + 502 = 1782.1
5.1 * 252 + 504 = 1789.2
5.1 * 253 + 506 = 1796.3
5.1 * 254 + 508 = 1803.4
5.1 * 255 + 510 = 1810.5
5.1 * 256 + 512 = 1817.6
5.1 * 257 + 514 = 1824.7
5.1 * 258 + 516 = 1831.8
5.1 * 259 + 518 = 1838.9
5.1 * 260 + 520 = 1846
5.1 * 261 + 522 = 1853.1
5.1 * 262 + 524 = 1860.2
5.1 * 263 + 526 = 1867.3
5.1 * 264 + 528 = 1874.4
5.1 * 265 + 530 = 1881.5
5.1 * 266 + 532 = 1888.6
5.1 * 267 + 534 = 1895.7
5.1 * 268 + 536 = 1902.8
5.1 * 269 + 538 = 1909.9
5.1 * 270 + 540 = 1917
5.1 * 271 + 542 = 1924.1
5.1 * 272 + 544 = 1931.2
5.1 * 273 + 546 = 1938.3
5.1 * 274 + 548 = 1945.4
5.1 * 275 + 550 = 1952.5
5.1 * 276 + 552 = 1959.6
5.1 * 277 + 554 = 1966.7
5.1 * 278 + 556 = 1973.8
5.1 * 279 + 558 = 1980.9
5.1 * 280 + 560 = 1988
5.1 * 281 + 562 = 1995.1
5.1 * 282 + 564 = 2002.2
5.1 * 283 + 566 = 2009.3
5.1 * 284 + 568 = 2016.4
5.1 * 285 + 570 = 2023.5
5.1 * 286 + 572 = 2030.6
5.1 * 287 + 574 = 2037.7
5.1 * 288 + 576 = 2044.8
5.1 * 289 + 578 = 2051.9
5.1 * 290 + 580 = 2059
5.1 * 291 + 582 = 2066.1
5.1 * 292 + 584 = 2073.2
5.1 * 293 + 586 = 2080.3
5.1 * 294 + 588 = 2087.4
5.1 * 295 + 590 = 2094.5
5.1 * 296 + 592 = 2101.6
5.1 * 297 + 594 = 2108.7
5.1 * 298 + 596 = 2115.8
5.1 * 299 + 598 = 2122.9
5.1 * 300 + 600 = 2130
5.1 * 301 + 602 = 2137.1
5.1 * 302 + 604 = 2144.2
5.1 * 303 + 606 = 2151.3
5.1 * 304 + 608 = 2158.4
5.1 * 305 + 610 = 2165.5
5.1 * 306 + 612 = 2172.6
5.1 * 307 + 614 = 2179.7
5.1 * 308 + 616 = 2186.8
5.1 * 309 + 618 = 2193.9
5.1 * 310 + 620 = 2201
5.1 * 311 + 622 = 2208.1
5.1 * 312 + 624 = 2215.2
5.1 * 313 + 626 = 2222.3
5.1 * 314 + 628 = 2229.4
5.1 * 315 + 630 = 2236.5
5.1 * 316 + 632 = 2243.6
5.1 * 317 + 634 = 2250.7
5.1 * 318 + 636 = 2257.8
5.1 * 319 + 638 = 2264.9
5.1 * 320 + 640 = 2272
5.1 * 321 + 642 = 2279.1
5.1 * 322 + 644 = 2286.2
5.1 * 323 + 646 = 2293.3
5.1 * 324 + 648 = 2300.4
5.1 * 325 + 650 = 2307.5
5.1 * 326 + 652 = 2314.6
5.1 * 327 + 654 = 2321.7
5.1 * 328 + 656 = 2328.8
5.1 * 329 + 658 = 2335.9
5.1 * 330 + 660 = 2343
5.1 * 331 + 662 = 2350.1
5.1 * 332 + 664 = 2357.2
5.1 * 333 + 666 = 2364.3
5.1 * 334 + 668 = 2371.4
5.1 * 335 + 670 = 2378.5
5.1 * 336 + 672 = 2385.6
5.1 * 337 + 674 = 2392.7
5.1 * 338 + 676 = 2399.8
5.1 * 339 + 678 = 2406.9
5.1 * 340 + 680 = 2414
5.1 * 341 + 682 = 2421.1
5.1 * 342 + 684 = 2428.2
5.1 * 343 + 686 = 2435.3
5.1 * 344 + 688 = 2442.4
5.1 * 345 + 690 = 2449.5
5.1 * 346 + 692 = 2456.6
5.1 * 347 + 694 = 2463.7
5.1 * 348 + 696 = 2470.8
5.1 * 349 + 698 = 2477.9
5.1 * 350 + 700 = 2485
5.1 * 351 + 702 = 2492.1
5.1 * 352 + 704 = 2499.2
5.1 * 353 + 706 = 2506.3
5.1 * 354 + 708 = 2513.4
5.1 * 355 + 710 = 2520.5
5.1 * 356 + 712 = 2527.6
5.1 * 357 + 714 = 2534.7
5.1 * 358 + 716 = 2541.8
5.1 * 359 + 718 = 2548.9
5.1 * 360 + 720 = 2556
5.1 * 361 + 722 = 2563.1
5.1 * 362 + 724 = 2570.2
5.1 * 363 + 726 = 2577.3
5.1 * 364 + 728 = 2584.4
5.1 * 365 + 730 = 2591.5
5.1 * 366 + 732 = 2598.6
5.1 * 367 + 734 = 2605.7
5.1 * 368 + 736 = 2612.8
5.1 * 369 + 738 = 2619.9
5.1 * 370 + 740 = 2627
5.1 * 371 + 742 = 2634.1
5.1 * 372 + 744 = 2641.2
5.1 * 373 + 746 = 2648.3
5.1 * 374 + 748 = 2655.4
5.1 * 375 + 750 = 2662.5
5.1 * 376 + 752 = 2669.6
5.1 * 377 + 754 = 2676.7
5.1 * 378 + 756 = 2683.8
5.1 * 379 + 758 = 2690.9
5.1 * 380 + 760 = 2698
5.1 * 381 + 762 = 2705.1
5.1 * 382 + 764 = 2712.2
5.1 * 383 + 766 = 2719.3
5.1 * 384 + 768 = 2726.4
5.1 * 385 + 770 = 2733.5
5.1 * 386 + 772 = 2740.6
5.1 * 387 + 774 = 2747.7
5.1 * 388 + 776 = 2754.8
5.1 * 389 + 778 = 2761.9
5.1 * 390 + 780 = 2769
5.1 * 391 + 782 = 2776.1
5.1 * 392 + 784 = 2783.2
5.1 * 393 + 786 = 2790.3
5.1 * 394 + 788 = 2797.4
5.1 * 395 + 790 = 2804.5
5.1 * 396 + 792 = 2811.6
5.1 * 397 + 794 = 2818.7
5.1 * 398 + 796 = 2825.8
5.1 * 399 + 798 = 2832.9
5.1 * 400 + 800 = 2840
5.1 * 401 + 802 = 2847.1
5.1 * 402 + 804 = 2854.2
5.1 * 403 + 806 = 2861.3
5.1 * 404 + 808 = 2868.4
5.1 * 405 + 810 = 2875.5
5.1 * 406 + 812 = 2882.6
5.1 * 407 + 814 = 2889.7
5.1 * 408 + 816 = 2896.8
5.1 * 409 + 818 = 2903.9
5.1 * 410 + 820 = 2911
5.1 * 411 + 822 = 2918.1
5.1 * 412 + 824 = 2925.2
5.1 * 413 + 826 = 2932.3
5.1 * 414 + 828 = 2939.4
5.1 * 415 + 830 = 2946.5
5.1 * 416 + 832 = 2953.6
5.1 * 417 + 834 = 2960.7
5.1 * 418 + 836 = 2967.8
5.1 * 419 + 838 = 2974.9
5.1 * 420 + 840 = 2982
5.1 * 421 + 842 = 2989.1
5.1 * 422 + 844 = 2996.2
5.1 * 423 + 846 = 3003.3
5.1 * 424 + 848 = 3010.4
5.1 * 425 + 850 = 3017.5
5.1 * 426 + 852 = 3024.6
5.1 * 427 + 854 = 3031.7
5.1 * 428 + 856 = 3038.8
5.1 * 429 + 858 = 3045.9
5.1 * 430 + 860 = 3053
5.1 * 431 + 862 = 3060.1
5.1 * 432 + 864 = 3067.2
5.1 * 433 + 866 = 3074.3
5.1 * 434 + 868 = 3081.4
5.1 * 435 + 870 = 3088.5
5.1 * 436 + 872 = 3095.6
5.1 * 437 + 874 = 3102.7
5.1 * 438 + 876 = 3109.8
5.1 * 439 + 878 = 3116.9
5.1 * 440 + 880 = 3124
5.1 * 441 + 882 = 3131.1
5.1 * 442 + 884 = 3138.2
5.1 * 443 + 886 = 3145.3
5.1 * 444 + 888 = 3152.4
5.1 * 445 + 890 = 3159.5
5.1 * 446 + 892 = 3166.6
5.1 * 447 + 894 = 3173.7
5.1 * 448 + 896 = 3180.8
5.1 * 449 + 898 = 3187.9
5.1 * 450 + 900 = 3195
5.1 * 451 + 902 = 3202.1
5.1 * 452 + 904 = 3209.2
5.1 * 453 + 906 = 3216.3
5.1 * 454 + 908 = 3223.4
5.1 * 455 + 910 = 3230.5
5.1 * 456 + 912 = 3237.6
5.1 * 457 + 914 = 3244.7
5.1 * 458 + 916 = 3251.8
5.1 * 459 + 918 = 3258.9
5.1 * 460 + 920 = 3266
5.1 * 461 + 922 = 3273.1
5.1 * 462 + 924 = 3280.2
5.1 * 463 + 926 = 3287.3
5.1 * 464 + 928 = 3294.4
5.1 * 465 + 930 = 3301.5
5.1 * 466 + 932 = 3308.6
5.1 * 467 + 934 = 3315.7
5.1 * 468 + 936 = 3322.8
5.1 * 469 + 938 = 3329.9
5.1 * 470 + 940 = 3337
5.1 * 471 + 942 = 3344.1
5.1 * 472 + 944 = 3351.2
5.1 * 473 + 946 = 3358.3
5.1 * 474 + 948 = 3365.4
5.1 * 475 + 950 = 3372.5
5.1 * 476 + 952 = 3379.6
5.1 * 477 + 954 = 3386.7
5.1 * 478 + 956 = 3393.8
5.1 * 479 + 958 = 3400.9
5.1 * 480 + 960 = 3408
5.1 * 481 + 962 = 3415.1
5.1 * 482 + 964 = 3422.2
5.1 * 483 + 966 = 3429.3
5.1 * 484 + 968 = 3436.4
5.1 * 485 + 970 = 3443.5
5.1 * 486 + 972 = 3450.6
5.1 * 487 + 974 = 3457.7
5.1 * 488 + 976 = 3464.8
5.1 * 489 + 978 = 3471.9
5.1 * 490 + 980 = 3479
5.1 * 491 + 982 = 3486.1
5.1 * 492 + 984 = 3493.2
5.1 * 493 + 986 = 3500.3
5.1 * 494 + 988 = 3507.4
5.1 * 495 + 990 = 3514.5
5.1 * 496 + 992 = 3521.6
5.1 * 497 + 994 = 3528.7
5.1 * 498 + 996 = 3535.8
5.1 * 499 + 998 = 3542.9
5.1 * 500 + 1000 = 3550
5.1 * 501 + 1002 = 3557.1
5.1 * 502 + 1004 = 3564.2
5.1 * 503 + 1006 = 3571.3
5.1 * 504 + 1008 = 3578.4
5.1 * 505 + 1010 = 3585.5
5.1 * 506 + 1012 = 3592.6
5.1 * 507 + 1014 = 3599.7
5.1 * 508 + 1016 = 3606.8
5.1 * 509 + 1018 = 3613.9
5.1 * 510 + 1020 = 3621
5.1 * 511 + 1022 = 3628.1
5.1 * 512 + 1024 = 3635.2
5.1 * 513 + 1026 = 3642.3
5.1 * 514 + 1028 = 3649.4
5.1 * 515 + 1030 = 3656.5
5.1 * 516 + 1032 = 3663.6
5.1 * 517 + 1034 = 3670.7
5.1 * 518 + 1036 = 3677.8
5.1 * 519 + 1038 = 3684.9
5.1 * 520 + 1040 = 3692
5.1 * 521 + 1042 = 3699.1
5.1 * 522 + 1044 = 3706.2
5.1 * 523 + 1046 = 3713.3
5.1 * 524 + 1048 = 3720.4
5.1 * 525 + 1050 = 3727.5
5.1 * 526 + 1052 = 3734.6
5.1 * 527 + 1054 = 3741.7
5.1 * 528 + 1056 = 3748.8
5.1 * 529 + 1058 = 3755.9
5.1 * 530 + 1060 = 3763
5.1 * 531 + 1062 = 3770.1
5.1 * 532 + 1064 = 3777.2
5.1 * 533 + 1066 = 3784.3
5.1 * 534 + 1068 = 3791.4
5.1 * 535 + 1070 = 3798.5
5.1 * 536 + 1072 = 3805.6
5.1 * 537 + 1074 = 3812.7
5.1 * 538 + 1076 = 3819.8
5.1 * 539 + 1078 = 3826.9
5.1 * 540 + 1080 = 3834
5.1 * 541 + 1082 = 3841.1
5.1 * 542 + 1084 = 3848.2
5.1 * 543 + 1086 = 3855.3
5.1 * 544 + 1088 = 3862.4
5.1 * 545 + 1090 = 3869.5
5.1 * 546 + 1092 = 3876.6
5.1 * 547 + 1094 = 3883.7
5.1 * 548 + 1096 = 3890.8
5.1 * 549 + 1098 = 3897.9
5.1 * 550 + 1100 = 3905
5.1 * 551 + 1102 = 3912.1
5.1 * 552 + 1104 = 3919.2
5.1 * 553 + 1106 = 3926.3
5.1 * 554 + 1108 = 3933.4
5.1 * 555 + 1110 = 3940.5
5.1 * 556 + 1112 = 3947.6
5.1 * 557 + 1114 = 3954.7
5.1 * 558 + 1116 = 3961.8
5.1 * 559 + 1118 = 3968.9
5.1 * 560 + 1120 = 3976
5.1 * 561 + 1122 = 3983.1
5.1 * 562 + 1124 = 3990.2
5.1 * 563 + 1126 = 3997.3
5.1 * 564 + 1128 = 4004.4
5.1 * 565 + 1130 = 4011.5
5.1 * 566 + 1132 = 4018.6
5.1 * 567 + 1134 = 4025.7
5.1 * 568 + 1136 = 4032.8
5.1 * 569 + 1138 = 4039.9
5.1 * 570 + 1140 = 4047
5.1 * 571 + 1142 = 4054.1
5.1 * 572 + 1144 = 4061.2
5.1 * 573 + 1146 = 4068.3
5.1 * 574 + 1148 = 4075.4
5.1 * 575 + 1150 = 4082.5
5.1 * 576 + 1152 = 4089.6
5.1 * 577 + 1154 = 4096.7
5.1 * 578 + 1156 = 4103.8
5.1 * 579 + 1158 = 4110.9
5.1 * 580 + 1160 = 4118
5.1 * 581 + 1162 = 4125.1
5.1 * 582 + 1164 = 4132.2
5.1 * 583 + 1166 = 4139.3
5.1 * 584 + 1168 = 4146.4
5.1 * 585 + 1170 = 4153.5
5.1 * 586 + 1172 = 4160.6
5.1 * 587 + 1174 = 4167.7
5.1 * 588 + 1176 = 4174.8
5.1 * 589 + 1178 = 4181.9
5.1 * 590 + 1180 = 4189
5.1 * 591 + 1182 = 4196.1
5.1 * 592 + 1184 = 4203.2
5.1 * 593 + 1186 = 4210.3
5.1 * 594 + 1188 = 4217.4
5.1 * 595 + 1190 = 4224.5
5.1 * 596 + 1192 = 4231.6
5.1 * 597 + 1194 = 4238.7
5.1 * 598 + 1196 = 4245.8
5.1 * 599 + 1198 = 4252.9
5.1 * 600 + 1200 = 4260
5.1 * 601 + 1202 = 4267.1
5.1 * 602 + 1204 = 4274.2
5.1 * 603 + 1206 = 4281.3
5.1 * 604 + 1208 = 4288.4
5.1 * 605 + 1210 = 4295.5
5.1 * 606 + 1212 = 4302.6
5.1 * 607 + 1214 = 4309.7
5.1 * 608 + 1216 = 4316.8
5.1 * 609 + 1218 = 4323.9
5.1 * 610 + 1220 = 4331
5.1 * 611 + 1222 = 4338.1
5.1 * 612 + 1224 = 4345.2
5.1 * 613 + 1226 = 4352.3
5.1 * 614 + 1228 = 4359.4
5.1 * 615 + 1230 = 4366.5
5.1 * 616 + 1232 = 4373.6
5.1 * 617 + 1234 = 4380.7
5.1 * 618 + 1236 = 4387.8
5.1 * 619 + 1238 = 4394.9
5.1 * 620 + 1240 = 4402
5.1 * 621 + 1242 = 4409.1
5.1 * 622 + 1244 = 4416.2
5.1 * 623 + 1246 = 4423.3
5.1 * 624 + 1248 = 4430.4
5.1 * 625 + 1250 = 4437.5
5.1 * 626 + 1252 = 4444.6
5.1 * 627 + 1254 = 4451.7
5.1 * 628 + 1256 = 4458.8
5.1 * 629 + 1258 = 4465.9
5.1 * 630 + 1260 = 4473
5.1 * 631 + 1262 = 4480.1
5.1 * 632 + 1264 = 4487.2
5.1 * 633 + 1266 = 4494.3
5.1 * 634 + 1268 = 4501.4
5.1 * 635 + 1270 = 4508.5
5.1 * 636 + 1272 = 4515.6
5.1 * 637 + 1274 = 4522.7
5.1 * 638 + 1276 = 4529.8
5.1 * 639 + 1278 = 4536.9
5.1 * 640 + 1280 = 4544
5.1 * 641 + 1282 = 4551.1
5.1 * 642 + 1284 = 4558.2
5.1 * 643 + 1286 = 4565.3
5.1 * 644 + 1288 = 4572.4
5.1 * 645 + 1290 = 4579.5
5.1 * 646 + 1292 = 4586.6
5.1 * 647 + 1294 = 4593.7
5.1 * 648 + 1296 = 4600.8
5.1 * 649 + 1298 = 4607.9
5.1 * 650 + 1300 = 4615
5.1 * 651 + 1302 = 4622.1
5.1 * 652 + 1304 = 4629.2
5.1 * 653 + 1306 = 4636.3
5.1 * 654 + 1308 = 4643.4
5.1 * 655 + 1310 = 4650.5
5.1 * 656 + 1312 = 4657.6
5.1 * 657 + 1314 = 4664.7
5.1 * 658 + 1316 = 4671.8
5.1 * 659 + 1318 = 4678.9
5.1 * 660 + 1320 = 4686
5.1 * 661 + 1322 = 4693.1
5.1 * 662 + 1324 = 4700.2
5.1 * 663 + 1326 = 4707.3
5.1 * 664 + 1328 = 4714.4
5.1 * 665 + 1330 = 4721.5
5.1 * 666 + 1332 = 4728.6
5.1 * 667 + 1334 = 4735.7
5.1 * 668 + 1336 = 4742.8
5.1 * 669 + 1338 = 4749.9
5.1 * 670 + 1340 = 4757
5.1 * 671 + 1342 = 4764.1
5.1 * 672 + 1344 = 4771.2
5.1 * 673 + 1346 = 4778.3
5.1 * 674 + 1348 = 4785.4
5.1 * 675 + 1350 = 4792.5
5.1 * 676 + 1352 = 4799.6
5.1 * 677 + 1354 = 4806.7
5.1 * 678 + 1356 = 4813.8
5.1 * 679 + 1358 = 4820.9
5.1 * 680 + 1360 = 4828
5.1 * 681 + 1362 = 4835.1
5.1 * 682 + 1364 = 4842.2
5.1 * 683 + 1366 = 4849.3
5.1 * 684 + 1368 = 4856.4
5.1 * 685 + 1370 = 4863.5
5.1 * 686 + 1372 = 4870.6
5.1 * 687 + 1374 = 4877.7
5.1 * 688 + 1376 = 4884.8
5.1 * 689 + 1378 = 4891.9
5.1 * 690 + 1380 = 4899
5.1 * 691 + 1382 = 4906.1
5.1 * 692 + 1384 = 4913.2
5.1 * 693 + 1386 = 4920.3
5.1 * 694 + 1388 = 4927.4
5.1 * 695 + 1390 = 4934.5
5.1 * 696 + 1392 = 4941.6
5.1 * 697 + 1394 = 4948.7
5.1 * 698 + 1396 = 4955.8
5.1 * 699 + 1398 = 4962.9
5.1 * 700 + 1400 = 4970
5.1 * 701 + 1402 = 4977.1
5.1 * 702 + 1404 = 4984.2
5.1 * 703 + 1406 = 4991.3
5.1 * 704 + 1408 = 4998.4
5.1 * 705 + 1410 = 5005.5
5.1 * 706 + 1412 = 5012.6
5.1 * 707 + 1414 = 5019.7
5.1 * 708 + 1416 = 5026.8
5.1 * 709 + 1418 = 5033.9
5.1 * 710 + 1420 = 5041
5.1 * 711 + 1422 = 5048.1
5.1 * 712 + 1424 = 5055.2
5.1 * 713 + 1426 = 5062.3
5.1 * 714 + 1428 = 5069.4
5.1 * 715 + 1430 = 5076.5
5.1 * 716 + 1432 = 5083.6
5.1 * 717 + 1434 = 5090.7
5.1 * 718 + 1436 = 5097.8
5.1 * 719 + 1438 = 5104.9
5.1 * 720 + 1440 = 5112
5.1 * 721 + 1442 = 5119.1
5.1 * 722 + 1444 = 5126.2
5.1 * 723 + 1446 = 5133.3
5.1 * 724 + 1448 = 5140.4
5.1 * 725 + 1450 = 5147.5
5.1 * 726 + 1452 = 5154.6
5.1 * 727 + 1454 = 5161.7
5.1 * 728 + 1456 = 5168.8
5.1 * 729 + 1458 = 5175.9
5.1 * 730 + 1460 = 5183
5.1 * 731 + 1462 = 5190.1
5.1 * 732 + 1464 = 5197.2
5.1 * 733 + 1466 = 5204.3
5.1 * 734 + 1468 = 5211.4
5.1 * 735 + 1470 = 5218.5
5.1 * 736 + 1472 = 5225.6
5.1 * 737 + 1474 = 5232.7
5.1 * 738 + 1476 = 5239.8
5.1 * 739 + 1478 = 5246.9
5.1 * 740 + 1480 = 5254
5.1 * 741 + 1482 = 5261.1
5.1 * 742 + 1484 = 5268.2
5.1 * 743 + 1486 = 5275.3
5.1 * 744 + 1488 = 5282.4
5.1 * 745 + 1490 = 5289.5
5.1 * 746 + 1492 = 5296.6
5.1 * 747 + 1494 = 5303.7
5.1 * 748 + 1496 = 5310.8
5.1 * 749 + 1498 = 5317.9
5.1 * 750 + 1500 = 5325
5.1 * 751 + 1502 = 5332.1
5.1 * 752 + 1504 = 5339.2
5.1 * 753 + 1506 = 5346.3
5.1 * 754 + 1508 = 5353.4
5.1 * 755 + 1510 = 5360.5
5.1 * 756 + 1512 = 5367.6
5.1 * 757 + 1514 = 5374.7
5.1 * 758 + 1516 = 5381.8
5.1 * 759 + 1518 = 5388.9
5.1 * 760 + 1520 = 5396
5.1 * 761 + 1522 = 5403.1
5.1 * 762 + 1524 = 5410.2
5.1 * 763 + 1526 = 5417.3
5.1 * 764 + 1528 = 5424.4
5.1 * 765 + 1530 = 5431.5
5.1 * 766 + 1532 = 5438.6
5.1 * 767 + 1534 = 5445.7
5.1 * 768 + 1536 = 5452.8
5.1 * 769 + 1538 = 5459.9
5.1 * 770 + 1540 = 5467
5.1 * 771 + 1542 = 5474.1
5.1 * 772 + 1544 = 5481.2
5.1 * 773 + 1546 = 5488.3
5.1 * 774 + 1548 = 5495.4
5.1 * 775 + 1550 = 5502.5
5.1 * 776 + 1552 = 5509.6
5.1 * 777 + 1554 = 5516.7
5.1 * 778 + 1556 = 5523.8
5.1 * 779 + 1558 = 5530.9
5.1 * 780 + 1560 = 5538
5.1 * 781 + 1562 = 5545.1
5.1 * 782 + 1564 = 5552.2
5.1 * 783 + 1566 = 5559.3
5.1 * 784 + 1568 = 5566.4
5.1 * 785 + 1570 = 5573.5
5.1 * 786 + 1572 = 5580.6
5.1 * 787 + 1574 = 5587.7
5.1 * 788 + 1576 = 5594.8
5.1 * 789 + 1578 = 5601.9
5.1 * 790 + 1580 = 5609
5.1 * 791 + 1582 = 5616.1
5.1 * 792 + 1584 = 5623.2
5.1 * 793 + 1586 = 5630.3
5.1 * 794 + 1588 = 5637.4
5.1 * 795 + 1590 = 5644.5
5.1 * 796 + 1592 = 5651.6
5.1 * 797 + 1594 = 5658.7
5.1 * 798 + 1596 = 5665.8
5.1 * 799 + 1598 = 5672.9
5.1 * 800 + 1600 = 5680
5.1 * 801 + 1602 = 5687.1
5.1 * 802 + 1604 = 5694.2
5.1 * 803 + 1606 = 5701.3
5.1 * 804 + 1608 = 5708.4
5.1 * 805 + 1610 = 5715.5
5.1 * 806 + 1612 = 5722.6
5.1 * 807 + 1614 = 5729.7
5.1 * 808 + 1616 = 5736.8
5.1 * 809 + 1618 = 5743.9
5.1 * 810 + 1620 = 5751
5.1 * 811 + 1622 = 5758.1
5.1 * 812 + 1624 = 5765.2
5.1 * 813 + 1626 = 5772.3
5.1 * 814 + 1628 = 5779.4
5.1 * 815 + 1630 = 5786.5
5.1 * 816 + 1632 = 5793.6
5.1 * 817 + 1634 = 5800.7
5.1 * 818 + 1636 = 5807.8
5.1 * 819 + 1638 = 5814.9
5.1 * 820 + 1640 = 5822
5.1 * 821 + 1642 = 5829.1
5.1 * 822 + 1644 = 5836.2
5.1 * 823 + 1646 = 5843.3
5.1 * 824 + 1648 = 5850.4
5.1 * 825 + 1650 = 5857.5
5.1 * 826 + 1652 = 5864.6
5.1 * 827 + 1654 = 5871.7
5.1 * 828 + 1656 = 5878.8
5.1 * 829 + 1658 = 5885.9
5.1 * 830 + 1660 = 5893
5.1 * 831 + 1662 = 5900.1
5.1 * 832 + 1664 = 5907.2
5.1 * 833 + 1666 = 5914.3
5.1 * 834 + 1668 = 5921.4
5.1 * 835 + 1670 = 5928.5
5.1 * 836 + 1672 = 5935.6
5.1 * 837 + 1674 = 5942.7
5.1 * 838 + 1676 = 5949.8
5.1 * 839 + 1678 = 5956.9
5.1 * 840 + 1680 = 5964
5.1 * 841 + 1682 = 5971.1
5.1 * 842 + 1684 = 5978.2
5.1 * 843 + 1686 = 5985.3
5.1 * 844 + 1688 = 5992.4
5.1 * 845 + 1690 = 5999.5
5.1 * 846 + 1692 = 6006.6
5.1 * 847 + 1694 = 6013.7
5.1 * 848 + 1696 = 6020.8
5.1 * 849 + 1698 = 6027.9
5.1 * 850 + 1700 = 6035
5.1 * 851 + 1702 = 6042.1
5.1 * 852 + 1704 = 6049.2
5.1 * 853 + 1706 = 6056.3
5.1 * 854 + 1708 = 6063.4
5.1 * 855 + 1710 = 6070.5
5.1 * 856 + 1712 = 6077.6
5.1 * 857 + 1714 = 6084.7
5.1 * 858 + 1716 = 6091.8
5.1 * 859 + 1718 = 6098.9
5.1 * 860 + 1720 = 6106
5.1 * 861 + 1722 = 6113.1
5.1 * 862 + 1724 = 6120.2
5.1 * 863 + 1726 = 6127.3
5.1 * 864 + 1728 = 6134.4
5.1 * 865 + 1730 = 6141.5
5.1 * 866 + 1732 = 6148.6
5.1 * 867 + 1734 = 6155.7
5.1 * 868 + 1736 = 6162.8
5.1 * 869 + 1738 = 6169.9
5.1 * 870 + 1740 = 6177
5.1 * 871 + 1742 = 6184.1
5.1 * 872 + 1744 = 6191.2
5.1 * 873 + 1746 = 6198.3
5.1 * 874 + 1748 = 6205.4
5.1 * 875 + 1750 = 6212.5
5.1 * 876 + 1752 = 6219.6
5.1 * 877 + 1754 = 6226.7
5.1 * 878 + 1756 = 6233.8
5.1 * 879 + 1758 = 6240.9
5.1 * 880 + 1760 = 6248
5.1 * 881 + 1762 = 6255.1
5.1 * 882 + 1764 = 6262.2
5.1 * 883 + 1766 = 6269.3
5.1 * 884 + 1768 = 6276.4
5.1 * 885 + 1770 = 6283.5
5.1 * 886 + 1772 = 6290.6
5.1 * 887 + 1774 = 6297.7
5.1 * 888 + 1776 = 6304.8
5.1 * 889 + 1778 = 6311.9
5.1 * 890 + 1780 = 6319
5.1 * 891 + 1782 = 6326.1
5.1 * 892 + 1784 = 6333.2
5.1 * 893 + 1786 = 6340.3
5.1 * 894 + 1788 = 6347.4
5.1 * 895 + 1790 = 6354.5
5.1 * 896 + 1792 = 6361.6
5.1 * 897 + 1794 = 6368.7
5.1 * 898 + 1796 = 6375.8
5.1 * 899 + 1798 = 6382.9
5.1 * 900 + 1800 = 6390
5.1 * 901 + 1802 = 6397.1
5.1 * 902 + 1804 = 6404.2
5.1 * 903 + 1806 = 6411.3
5.1 * 904 + 1808 = 6418.4
5.1 * 905 + 1810 = 6425.5
5.1 * 906 + 1812 = 6432.6
5.1 * 907 + 1814 = 6439.7
5.1 * 908 + 1816 = 6446.8
5.1 * 909 + 1818 = 6453.9
5.1 * 910 + 1820 = 6461
5.1 * 911 + 1822 = 6468.1
5.1 * 912 + 1824 = 6475.2
5.1 * 913 + 1826 = 6482.3
5.1 * 914 + 1828 = 6489.4
5.1 * 915 + 1830 = 6496.5
5.1 * 916 + 1832 = 6503.6
5.1 * 917 + 1834 = 6510.7
5.1 * 918 + 1836 = 6517.8
5.1 * 919 + 1838 = 6524.9
5.1 * 920 + 1840 = 6532
5.1 * 921 + 1842 = 6539.1
5.1 * 922 + 1844 = 6546.2
5.1 * 923 + 1846 = 6553.3
5.1 * 924 + 1848 = 6560.4
5.1 * 925 + 1850 = 6567.5
5.1 * 926 + 1852 = 6574.6
5.1 * 927 + 1854 = 6581.7
5.1 * 928 + 1856 = 6588.8
5.1 * 929 + 1858 = 6595.9
5.1 * 930 + 1860 = 6603
5.1 * 931 + 1862 = 6610.1
5.1 * 932 + 1864 = 6617.2
5.1 * 933 + 1866 = 6624.3
5.1 * 934 + 1868 = 6631.4
5.1 * 935 + 1870 = 6638.5
5.1 * 936 + 1872 = 6645.6
5.1 * 937 + 1874 = 6652.7
5.1 * 938 + 1876 = 6659.8
5.1 * 939 + 1878 = 6666.9
5.1 * 940 + 1880 = 6674
5.1 * 941 + 1882 = 6681.1
5.1 * 942 + 1884 = 6688.2
5.1 * 943 + 1886 = 6695.3
5.1 * 944 + 1888 = 6702.4
5.1 * 945 + 1890 = 6709.5
5.1 * 946 + 1892 = 6716.6
5.1 * 947 + 1894 = 6723.7
5.1 * 948 + 1896 = 6730.8
5.1 * 949 + 1898 = 6737.9
5.1 * 950 + 1900 = 6745
5.1 * 951 + 1902 = 6752.1
5.1 * 952 + 1904 = 6759.2
5.1 * 953 + 1906 = 6766.3
5.1 * 954 + 1908 = 6773.4
5.1 * 955 + 1910 = 6780.5
5.1 * 956 + 1912 = 6787.6
5.1 * 957 + 1914 = 6794.7
5.1 * 958 + 1916 = 6801.8
5.1 * 959 + 1918 = 6808.9
5.1 * 960 + 1920 = 6816
5.1 * 961 + 1922 = 6823.1
5.1 * 962 + 1924 = 6830.2
5.1 * 963 + 1926 = 6837.3
5.1 * 964 + 1928 = 6844.4
5.1 * 965 + 1930 = 6851.5
5.1 * 966 + 1932 = 6858.6
5.1 * 967 + 1934 = 6865.7
5.1 * 968 + 1936 = 6872.8
5.1 * 969 + 1938 = 6879.9
5.1 * 970 + 1940 = 6887
5.1 * 971 + 1942 = 6894.1
5.1 * 972 + 1944 = 6901.2
5.1 * 973 + 1946 = 6908.3
5.1 * 974 + 1948 = 6915.4
5.1 * 975 + 1950 = 6922.5
5.1 * 976 + 1952 = 6929.6
5.1 * 977 + 1954 = 6936.7
5.1 * 978 + 1956 = 6943.8
5.1 * 979 + 1958 = 6950.9
5.1 * 980 + 1960 = 6958
5.1 * 981 + 1962 = 6965.1
5.1 * 982 + 1964 = 6972.2
5.1 * 983 + 1966 = 6979.3
5.1 * 984 + 1968 = 6986.4
5.1 * 985 + 1970 = 6993.5
5.1 * 986 + 1972 = 7000.6
5.1 * 987 + 1974 = 7007.7
5.1 * 988 + 1976 = 7014.8
5.1 * 989 + 1978 = 7021.9
5.1 * 990 + 1980 = 7029
5.1 * 991 + 1982 = 7036.1
5.1 * 992 + 1984 = 7043.2
5.1 * 993 + 1986 = 7050.3
5.1 * 994 + 1988 = 7057.4
5.1 * 995 + 1990 = 7064.5
5.1 * 996 + 1992 = 7071.6
5.1 * 997 + 1994 = 7078.7
5.1 * 998 + 1996 = 7085.8
5.1 * 999 + 1998 = 7092.9
5.1 * 1000 + 2000 = 7100
5.1 * 1001 + 2002 = 7107.1
5.1 * 1002 + 2004 = 7114.2
5.1 * 1003 + 2006 = 7121.3
5.1 * 1004 + 2008 = 7128.4
5.1 * 1005 + 2010 = 7135.5
5.1 * 1006 + 2012 = 7142.6
5.1 * 1007 + 2014 = 7149.7
5.1 * 1008 + 2016 = 7156.8
5.1 * 1009 + 2018 = 7163.9
5.1 * 1010 + 2020 = 7171
5.1 * 1011 + 2022 = 7178.1
5.1 * 1012 + 2024 = 7185.2
5.1 * 1013 + 2026 = 7192.3
5.1 * 1014 + 2028 = 7199.4
5.1 * 1015 + 2030 = 7206.5
5.1 * 1016 + 2032 = 7213.6
5.1 * 1017 + 2034 = 7220.7
5.1 * 1018 + 2036 = 7227.8
5.1 * 1019 + 2038 = 7234.9
5.1 * 1020 + 2040 = 7242
5.1 * 1021 + 2042 = 7249.1
5.1 * 1022 + 2044 = 7256.2
5.1 * 1023 + 2046 = 7263.3
5.1 * 1024 + 2048 = 7270.4
5.1 * 1025 + 2050 = 7277.5
5.1 * 1026 + 2052 = 7284.6
5.1 * 1027 + 2054 = 7291.7
5.1 * 1028 + 2056 = 7298.8
5.1 * 1029 + 2058 = 7305.9
5.1 * 1030 + 2060 = 7313
5.1 * 1031 + 2062 = 7320.1
5.1 * 1032 + 2064 = 7327.2
5.1 * 1033 + 2066 = 7334.3
5.1 * 1034 + 2068 = 7341.4
5.1 * 1035 + 2070 = 7348.5
5.1 * 1036 + 2072 = 7355.6
5.1 * 1037 + 2074 = 7362.7
5.1 * 1038 + 2076 = 7369.8
5.1 * 1039 + 2078 = 7376.9
5.1 * 1040 + 2080 = 7384
5.1 * 1041 + 2082 = 7391.1
5.1 * 1042 + 2084 = 7398.2
5.1 * 1043 + 2086 = 7405.3
5.1 * 1044 + 2088 = 7412.4
5.1 * 1045 + 2090 = 7419.5
5.1 * 1046 + 2092 = 7426.6
5.1 * 1047 + 2094 = 7433.7
5.1 * 1048 + 2096 = 7440.8
5.1 * 1049 + 2098 = 7447.9
5.1 * 1050 + 2100 = 7455
5.1 * 1051 + 2102 = 7462.1
5.1 * 1052 + 2104 = 7469.2
5.1 * 1053 + 2106 = 7476.3
5.1 * 1054 + 2108 = 7483.4
5.1 * 1055 + 2110 = 7490.5
5.1 * 1056 + 2112 = 7497.6
5.1 * 1057 + 2114 = 7504.7
5.1 * 1058 + 2116 = 7511.8
5.1 * 1059 + 2118 = 7518.9
5.1 * 1060 + 2120 = 7526
5.1 * 1061 + 2122 = 7533.1
5.1 * 1062 + 2124 = 7540.2
5.1 * 1063 + 2126 = 7547.3
5.1 * 1064 + 2128 = 7554.4
5.1 * 1065 + 2130 = 7561.5
5.1 * 1066 + 2132 = 7568.6
5.1 * 1067 + 2134 = 7575.7
5.1 * 1068 + 2136 = 7582.8
5.1 * 1069 + 2138 = 7589.9
5.1 * 1070 + 2140 = 7597
5.1 * 1071 + 2142 = 7604.1
5.1 * 1072 + 2144 = 7611.2
5.1 * 1073 + 2146 = 7618.3
5.1 * 1074 + 2148 = 7625.4
5.1 * 1075 + 2150 = 7632.5
5.1 * 1076 + 2152 = 7639.6
5.1 * 1077 + 2154 = 7646.7
5.1 * 1078 + 2156 = 7653.8
5.1 * 1079 + 2158 = 7660.9
5.1 * 1080 + 2160 = 7668
5.1 * 1081 + 2162 = 7675.1
5.1 * 1082 + 2164 = 7682.2
5.1 * 1083 + 2166 = 7689.3
5.1 * 1084 + 2168 = 7696.4
5.1 * 1085 + 2170 = 7703.5
5.1 * 1086 + 2172 = 7710.6
5.1 * 1087 + 2174 = 7717.7
5.1 * 1088 + 2176 = 7724.8
5.1 * 1089 + 2178 = 7731.9
5.1 * 1090 + 2180 = 7739
5.1 * 1091 + 2182 = 7746.1
5.1 * 1092 + 2184 = 7753.2
5.1 * 1093 + 2186 = 7760.3
5.1 * 1094 + 2188 = 7767.4
5.1 * 1095 + 2190 = 7774.5
5.1 * 1096 + 2192 = 7781.6
5.1 * 1097 + 2194 = 7788.7
5.1 * 1098 + 2196 = 7795.8
5.1 * 1099 + 2198 = 7802.9
5.1 * 1100 + 2200 = 7810
5.1 * 1101 + 2202 = 7817.1
5.1 * 1102 + 2204 = 7824.2
5.1 * 1103 + 2206 = 7831.3
5.1 * 1104 + 2208 = 7838.4
5.1 * 1105 + 2210 = 7845.5
5.1 * 1106 + 2212 = 7852.6
5.1 * 1107 + 2214 = 7859.7
5.1 * 1108 + 2216 = 7866.8
5.1 * 1109 + 2218 = 7873.9
5.1 * 1110 + 2220 = 7881
5.1 * 1111 + 2222 = 7888.1
5.1 * 1112 + 2224 = 7895.2
5.1 * 1113 + 2226 = 7902.3
5.1 * 1114 + 2228 = 7909.4
5.1 * 1115 + 2230 = 7916.5
5.1 * 1116 + 2232 = 7923.6
5.1 * 1117 + 2234 = 7930.7
5.1 * 1118 + 2236 = 7937.8
5.1 * 1119 + 2238 = 7944.9
5.1 * 1120 + 2240 = 7952
5.1 * 1121 + 2242 = 7959.1
5.1 * 1122 + 2244 = 7966.2
5.1 * 1123 + 2246 = 7973.3
5.1 * 1124 + 2248 = 7980.4
5.1 * 1125 + 2250 = 7987.5
5.1 * 1126 + 2252 = 7994.6
5.1 * 1127 + 2254 = 8001.7
5.1 * 1128 + 2256 = 8008.8
5.1 * 1129 + 2258 = 8015.9
5.1 * 1130 + 2260 = 8023
5.1 * 1131 + 2262 = 8030.1
5.1 * 1132 + 2264 = 8037.2
5.1 * 1133 + 2266 = 8044.3
5.1 * 1134 + 2268 = 8051.4
5.1 * 1135 + 2270 = 8058.5
5.1 * 1136 + 2272 = 8065.6
5.1 * 1137 + 2274 = 8072.7
5.1 * 1138 + 2276 = 8079.8
5.1 * 1139 + 2278 = 8086.9
5.1 * 1140 + 2280 = 8094
5.1 * 1141 + 2282 = 8101.1
5.1 * 1142 + 2284 = 8108.2
5.1 * 1143 + 2286 = 8115.3
5.1 * 1144 + 2288 = 8122.4
5.1 * 1145 + 2290 = 8129.5
5.1 * 1146 + 2292 = 8136.6
5.1 * 1147 + 2294 = 8143.7
5.1 * 1148 + 2296 = 8150.8
5.1 * 1149 + 2298 = 8157.9
5.1 * 1150 + 2300 = 8165
5.1 * 1151 + 2302 = 8172.1
5.1 * 1152 + 2304 = 8179.2
5.1 * 1153 + 2306 = 8186.3
5.1 * 1154 + 2308 = 8193.4
5.1 * 1155 + 2310 = 8200.5
5.1 * 1156 + 2312 = 8207.6
5.1 * 1157 + 2314 = 8214.7
5.1 * 1158 + 2316 = 8221.8
5.1 * 1159 + 2318 = 8228.9
5.1 * 1160 + 2320 = 8236
5.1 * 1161 + 2322 = 8243.1
5.1 * 1162 + 2324 = 8250.2
5.1 * 1163 + 2326 = 8257.3
5.1 * 1164 + 2328 = 8264.4
5.1 * 1165 + 2330 = 8271.5
5.1 * 1166 + 2332 = 8278.6
5.1 * 1167 + 2334 = 8285.7
5.1 * 1168 + 2336 = 8292.8
5.1 * 1169 + 2338 = 8299.9
5.1 * 1170 + 2340 = 8307
5.1 * 1171 + 2342 = 8314.1
5.1 * 1172 + 2344 = 8321.2
5.1 * 1173 + 2346 = 8328.3
5.1 * 1174 + 2348 = 8335.4
5.1 * 1175 + 2350 = 8342.5
5.1 * 1176 + 2352 = 8349.6
5.1 * 1177 + 2354 = 8356.7
5.1 * 1178 + 2356 = 8363.8
5.1 * 1179 + 2358 = 8370.9
5.1 * 1180 + 2360 = 8378
5.1 * 1181 + 2362 = 8385.1
5.1 * 1182 + 2364 = 8392.2
5.1 * 1183 + 2366 = 8399.3
5.1 * 1184 + 2368 = 8406.4
5.1 * 1185 + 2370 = 8413.5
5.1 * 1186 + 2372 = 8420.6
5.1 * 1187 + 2374 = 8427.7
5.1 * 1188 + 2376 = 8434.8
5.1 * 1189 + 2378 = 8441.9
5.1 * 1190 + 2380 = 8449
5.1 * 1191 + 2382 = 8456.1
5.1 * 1192 + 2384 = 8463.2
5.1 * 1193 + 2386 = 8470.3
5.1 * 1194 + 2388 = 8477.4
5.1 * 1195 + 2390 = 8484.5
5.1 * 1196 + 2392 = 8491.6
5.1 * 1197 + 2394 = 8498.7
5.1 * 1198 + 2396 = 8505.8
5.1 * 1199 + 2398 = 8512.9
5.1 * 1200 + 2400 = 8520
5.1 * 1201 + 2402 = 8527.1
5.1 * 1202 + 2404 = 8534.2
5.1 * 1203 + 2406 = 8541.3
5.1 * 1204 + 2408 = 8548.4
5.1 * 1205 + 2410 = 8555.5
5.1 * 1206 + 2412 = 8562.6
5.1 * 1207 + 2414 = 8569.7
5.1 * 1208 + 2416 = 8576.8
5.1 * 1209 + 2418 = 8583.9
5.1 * 1210 + 2420 = 8591
5.1 * 1211 + 2422 = 8598.1
5.1 * 1212 + 2424 = 8605.2
5.1 * 1213 + 2426 = 8612.3
5.1 * 1214 + 2428 = 8619.4
5.1 * 1215 + 2430 = 8626.5
5.1 * 1216 + 2432 = 8633.6
5.1 * 1217 + 2434 = 8640.7
5.1 * 1218 + 2436 = 8647.8
5.1 * 1219 + 2438 = 8654.9
5.1 * 1220 + 2440 = 8662
5.1 * 1221 + 2442 = 8669.1
5.1 * 1222 + 2444 = 8676.2
5.1 * 1223 + 2446 = 8683.3
5.1 * 1224 + 2448 = 8690.4
5.1 * 1225 + 2450 = 8697.5
5.1 * 1226 + 2452 = 8704.6
5.1 * 1227 + 2454 = 8711.7
5.1 * 1228 + 2456 = 8718.8
5.1 * 1229 + 2458 = 8725.9
5.1 * 1230 + 2460 = 8733
5.1 * 1231 + 2462 = 8740.1
5.1 * 1232 + 2464 = 8747.2
5.1 * 1233 + 2466 = 8754.3
5.1 * 1234 + 2468 = 8761.4
5.1 * 1235 + 2470 = 8768.5
5.1 * 1236 + 2472 = 8775.6
5.1 * 1237 + 2474 = 8782.7
5.1 * 1238 + 2476 = 8789.8
5.1 * 1239 + 2478 = 8796.9
5.1 * 1240 + 2480 = 8804
5.1 * 1241 + 2482 = 8811.1
5.1 * 1242 + 2484 = 8818.2
5.1 * 1243 + 2486 = 8825.3
5.1 * 1244 + 2488 = 8832.4
5.1 * 1245 + 2490 = 8839.5
5.1 * 1246 + 2492 = 8846.6
5.1 * 1247 + 2494 = 8853.7
5.1 * 1248 + 2496 = 8860.8
5.1 * 1249 + 2498 = 8867.9
5.1 * 1250 + 2500 = 8875
5.1 * 1251 + 2502 = 8882.1
5.1 * 1252 + 2504 = 8889.2
5.1 * 1253 + 2506 = 8896.3
5.1 * 1254 + 2508 = 8903.4
5.1 * 1255 + 2510 = 8910.5
5.1 * 1256 + 2512 = 8917.6
5.1 * 1257 + 2514 = 8924.7
5.1 * 1258 + 2516 = 8931.8
5.1 * 1259 + 2518 = 8938.9
5.1 * 1260 + 2520 = 8946
5.1 * 1261 + 2522 = 8953.1
5.1 * 1262 + 2524 = 8960.2
5.1 * 1263 + 2526 = 8967.3
5.1 * 1264 + 2528 = 8974.4
5.1 * 1265 + 2530 = 8981.5
5.1 * 1266 + 2532 = 8988.6
5.1 * 1267 + 2534 = 8995.7
5.1 * 1268 + 2536 = 9002.8
5.1 * 1269 + 2538 = 9009.9
5.1 * 1270 + 2540 = 9017
5.1 * 1271 + 2542 = 9024.1
5.1 * 1272 + 2544 = 9031.2
5.1 * 1273 + 2546 = 9038.3
5.1 * 1274 + 2548 = 9045.4
5.1 * 1275 + 2550 = 9052.5
5.1 * 1276 + 2552 = 9059.6
5.1 * 1277 + 2554 = 9066.7
5.1 * 1278 + 2556 = 9073.8
5.1 * 1279 + 2558 = 9080.9
5.1 * 1280 + 2560 = 9088
5.1 * 1281 + 2562 = 9095.1
5.1 * 1282 + 2564 = 9102.2
5.1 * 1283 + 2566 = 9109.3
5.1 * 1284 + 2568 = 9116.4
5.1 * 1285 + 2570 = 9123.5
5.1 * 1286 + 2572 = 9130.6
5.1 * 1287 + 2574 = 9137.7
5.1 * 1288 + 2576 = 9144.8
5.1 * 1289 + 2578 = 9151.9
5.1 * 1290 + 2580 = 9159
5.1 * 1291 + 2582 = 9166.1
5.1 * 1292 + 2584 = 9173.2
5.1 * 1293 + 2586 = 9180.3
5.1 * 1294 + 2588 = 9187.4
5.1 * 1295 + 2590 = 9194.5
5.1 * 1296 + 2592 = 9201.6
5.1 * 1297 + 2594 = 9208.7
5.1 * 1298 + 2596 = 9215.8
5.1 * 1299 + 2598 = 9222.9
5.1 * 1300 + 2600 = 9230
5.1 * 1301 + 2602 = 9237.1
5.1 * 1302 + 2604 = 9244.2
5.1 * 1303 + 2606 = 9251.3
5.1 * 1304 + 2608 = 9258.4
5.1 * 1305 + 2610 = 9265.5
5.1 * 1306 + 2612 = 9272.6
5.1 * 1307 + 2614 = 9279.7
5.1 * 1308 + 2616 = 9286.8
5.1 * 1309 + 2618 = 9293.9
5.1 * 1310 + 2620 = 9301
5.1 * 1311 + 2622 = 9308.1
5.1 * 1312 + 2624 = 9315.2
5.1 * 1313 + 2626 = 9322.3
5.1 * 1314 + 2628 = 9329.4
5.1 * 1315 + 2630 = 9336.5
5.1 * 1316 + 2632 = 9343.6
5.1 * 1317 + 2634 = 9350.7
5.1 * 1318 + 2636 = 9357.8
5.1 * 1319 + 2638 = 9364.9
5.1 * 1320 + 2640 = 9372
5.1 * 1321 + 2642 = 9379.1
5.1 * 1322 + 2644 = 9386.2
5.1 * 1323 + 2646 = 9393.3
5.1 * 1324 + 2648 = 9400.4
5.1 * 1325 + 2650 = 9407.5
5.1 * 1326 + 2652 = 9414.6
5.1 * 1327 + 2654 = 9421.7
5.1 * 1328 + 2656 = 9428.8
5.1 * 1329 + 2658 = 9435.9
5.1 * 1330 + 2660 = 9443
5.1 * 1331 + 2662 = 9450.1
5.1 * 1332 + 2664 = 9457.2
5.1 * 1333 + 2666 = 9464.3
5.1 * 1334 + 2668 = 9471.4
5.1 * 1335 + 2670 = 9478.5
5.1 * 1336 + 2672 = 9485.6
5.1 * 1337 + 2674 = 9492.7
5.1 * 1338 + 2676 = 9499.8
5.1 * 1339 + 2678 = 9506.9
5.1 * 1340 + 2680 = 9514
5.1 * 1341 + 2682 = 9521.1
5.1 * 1342 + 2684 = 9528.2
5.1 * 1343 + 2686 = 9535.3
5.1 * 1344 + 2688 = 9542.4
5.1 * 1345 + 2690 = 9549.5
5.1 * 1346 + 2692 = 9556.6
5.1 * 1347 + 2694 = 9563.7
5.1 * 1348 + 2696 = 9570.8
5.1 * 1349 + 2698 = 9577.9
5.1 * 1350 + 2700 = 9585
5.1 * 1351 + 2702 = 9592.1
5.1 * 1352 + 2704 = 9599.2
5.1 * 1353 + 2706 = 9606.3
5.1 * 1354 + 2708 = 9613.4
5.1 * 1355 + 2710 = 9620.5
5.1 * 1356 + 2712 = 9627.6
5.1 * 1357 + 2714 = 9634.7
5.1 * 1358 + 2716 = 9641.8
5.1 * 1359 + 2718 = 9648.9
5.1 * 1360 + 2720 = 9656
5.1 * 1361 + 2722 = 9663.1
5.1 * 1362 + 2724 = 9670.2
5.1 * 1363 + 2726 = 9677.3
5.1 * 1364 + 2728 = 9684.4
5.1 * 1365 + 2730 = 9691.5
5.1 * 1366 + 2732 = 9698.6
5.1 * 1367 + 2734 = 9705.7
5.1 * 1368 + 2736 = 9712.8
5.1 * 1369 + 2738 = 9719.9
5.1 * 1370 + 2740 = 9727
5.1 * 1371 + 2742 = 9734.1
5.1 * 1372 + 2744 = 9741.2
5.1 * 1373 + 2746 = 9748.3
5.1 * 1374 + 2748 = 9755.4
5.1 * 1375 + 2750 = 9762.5
5.1 * 1376 + 2752 = 9769.6
5.1 * 1377 + 2754 = 9776.7
5.1 * 1378 + 2756 = 9783.8
5.1 * 1379 + 2758 = 9790.9
5.1 * 1380 + 2760 = 9798
5.1 * 1381 + 2762 = 9805.1
5.1 * 1382 + 2764 = 9812.2
5.1 * 1383 + 2766 = 9819.3
5.1 * 1384 + 2768 = 9826.4
5.1 * 1385 + 2770 = 9833.5
5.1 * 1386 + 2772 = 9840.6
5.1 * 1387 + 2774 = 9847.7
5.1 * 1388 + 2776 = 9854.8
5.1 * 1389 + 2778 = 9861.9
5.1 * 1390 + 2780 = 9869
5.1 * 1391 + 2782 = 9876.1
5.1 * 1392 + 2784 = 9883.2
5.1 * 1393 + 2786 = 9890.3
5.1 * 1394 + 2788 = 9897.4
5.1 * 1395 + 2790 = 9904.5
5.1 * 1396 + 2792 = 9911.6
5.1 * 1397 + 2794 = 9918.7
5.1 * 1398 + 2796 = 9925.8
5.1 * 1399 + 2798 = 9932.9
5.1 * 1400 + 2800 = 9940
5.1 * 1401 + 2802 = 9947.1
5.1 * 1402 + 2804 = 9954.2
5.1 * 1403 + 2806 = 9961.3
5.1 * 1404 + 2808 = 9968.4
5.1 * 1405 + 2810 = 9975.5
5.1 * 1406 + 2812 = 9982.6
5.1 * 1407 + 2814 = 9989.7
5.1 * 1408 + 2816 = 9996.8
5.1 * 1409 + 2818 = 10003.9
5.1 * 1410 + 2820 = 10011
5.1 * 1411 + 2822 = 10018.1
5.1 * 1412 + 2824 = 10025.2
5.1 * 1413 + 2826 = 10032.3
5.1 * 1414 + 2828 = 10039.4
5.1 * 1415 + 2830 = 10046.5
5.1 * 1416 + 2832 = 10053.6
5.1 * 1417 + 2834 = 10060.7
5.1 * 1418 + 2836 = 10067.8
5.1 * 1419 + 2838 = 10074.9
5.1 * 1420 + 2840 = 10082
5.1 * 1421 + 2842 = 10089.1
5.1 * 1422 + 2844 = 10096.2
5.1 * 1423 + 2846 = 10103.3
5.1 * 1424 + 2848 = 10110.4
5.1 * 1425 + 2850 = 10117.5
5.1 * 1426 + 2852 = 10124.6
5.1 * 1427 + 2854 = 10131.7
5.1 * 1428 + 2856 = 10138.8
5.1 * 1429 + 2858 = 10145.9
5.1 * 1430 + 2860 = 10153
5.1 * 1431 + 2862 = 10160.1
5.1 * 1432 + 2864 = 10167.2
5.1 * 1433 + 2866 = 10174.3
5.1 * 1434 + 2868 = 10181.4
5.1 * 1435 + 2870 = 10188.5
5.1 * 1436 + 2872 = 10195.6
5.1 * 1437 + 2874 = 10202.7
5.1 * 1438 + 2876 = 10209.8
5.1 * 1439 + 2878 = 10216.9
5.1 * 1440 + 2880 = 10224
5.1 * 1441 + 2882 = 10231.1
5.1 * 1442 + 2884 = 10238.2
5.1 * 1443 + 2886 = 10245.3
5.1 * 1444 + 2888 = 10252.4
5.1 * 1445 + 2890 = 10259.5
5.1 * 1446 + 2892 = 10266.6
5.1 * 1447 + 2894 = 10273.7
5.1 * 1448 + 2896 = 10280.8
5.1 * 1449 + 2898 = 10287.9
5.1 * 1450 + 2900 = 10295
5.1 * 1451 + 2902 = 10302.1
5.1 * 1452 + 2904 = 10309.2
5.1 * 1453 + 2906 = 10316.3
5.1 * 1454 + 2908 = 10323.4
5.1 * 1455 + 2910 = 10330.5
5.1 * 1456 + 2912 = 10337.6
5.1 * 1457 + 2914 = 10344.7
5.1 * 1458 + 2916 = 10351.8
5.1 * 1459 + 2918 = 10358.9
5.1 * 1460 + 2920 = 10366
5.1 * 1461 + 2922 = 10373.1
5.1 * 1462 + 2924 = 10380.2
5.1 * 1463 + 2926 = 10387.3
5.1 * 1464 + 2928 = 10394.4
5.1 * 1465 + 2930 = 10401.5
5.1 * 1466 + 2932 = 10408.6
5.1 * 1467 + 2934 = 10415.7
5.1 * 1468 + 2936 = 10422.8
5.1 * 1469 + 2938 = 10429.9
5.1 * 1470 + 2940 = 10437
5.1 * 1471 + 2942 = 10444.1
5.1 * 1472 + 2944 = 10451.2
5.1 * 1473 + 2946 = 10458.3
5.1 * 1474 + 2948 = 10465.4
5.1 * 1475 + 2950 = 10472.5
5.1 * 1476 + 2952 = 10479.6
5.1 * 1477 + 2954 = 10486.7
5.1 * 1478 + 2956 = 10493.8
5.1 * 1479 + 2958 = 10500.9
5.1 * 1480 + 2960 = 10508
5.1 * 1481 + 2962 = 10515.1
5.1 * 1482 + 2964 = 10522.2
5.1 * 1483 + 2966 = 10529.3
5.1 * 1484 + 2968 = 10536.4
5.1 * 1485 + 2970 = 10543.5
5.1 * 1486 + 2972 = 10550.6
5.1 * 1487 + 2974 = 10557.7
5.1 * 1488 + 2976 = 10564.8
5.1 * 1489 + 2978 = 10571.9
5.1 * 1490 + 2980 = 10579
5.1 * 1491 + 2982 = 10586.1
5.1 * 1492 + 2984 = 10593.2
5.1 * 1493 + 2986 = 10600.3
5.1 * 1494 + 2988 = 10607.4
5.1 * 1495 + 2990 = 10614.5
5.1 * 1496 + 2992 = 10621.6
5.1 * 1497 + 2994 = 10628.7
5.1 * 1498 + 2996 = 10635.8
5.1 * 1499 + 2998 = 10642.9
5.1 * 1500 + 3000 = 10650
5.1 * 1501 + 3002 = 10657.1
5.1 * 1502 + 3004 = 10664.2
5.1 * 1503 + 3006 = 10671.3
5.1 * 1504 + 3008 = 10678.4
5.1 * 1505 + 3010 = 10685.5
5.1 * 1506 + 3012 = 10692.6
5.1 * 1507 + 3014 = 10699.7
5.1 * 1508 + 3016 = 10706.8
5.1 * 1509 + 3018 = 10713.9
5.1 * 1510 + 3020 = 10721
5.1 * 1511 + 3022 = 10728.1
5.1 * 1512 + 3024 = 10735.2
5.1 * 1513 + 3026 = 10742.3
5.1 * 1514 + 3028 = 10749.4
5.1 * 1515 + 3030 = 10756.5
5.1 * 1516 + 3032 = 10763.6
5.1 * 1517 + 3034 = 10770.7
5.1 * 1518 + 3036 = 10777.8
5.1 * 1519 + 3038 = 10784.9
5.1 * 1520 + 3040 = 10792
5.1 * 1521 + 3042 = 10799.1
5.1 * 1522 + 3044 = 10806.2
5.1 * 1523 + 3046 = 10813.3
5.1 * 1524 + 3048 = 10820.4
5.1 * 1525 + 3050 = 10827.5
5.1 * 1526 + 3052 = 10834.6
5.1 * 1527 + 3054 = 10841.7
5.1 * 1528 + 3056 = 10848.8
5.1 * 1529 + 3058 = 10855.9
5.1 * 1530 + 3060 = 10863
5.1 * 1531 + 3062 = 10870.1
5.1 * 1532 + 3064 = 10877.2
5.1 * 1533 + 3066 = 10884.3
5.1 * 1534 + 3068 = 10891.4
5.1 * 1535 + 3070 = 10898.5
5.1 * 1536 + 3072 = 10905.6
5.1 * 1537 + 3074 = 10912.7
5.1 * 1538 + 3076 = 10919.8
5.1 * 1539 + 3078 = 10926.9
5.1 * 1540 + 3080 = 10934
5.1 * 1541 + 3082 = 10941.1
5.1 * 1542 + 3084 = 10948.2
5.1 * 1543 + 3086 = 10955.3
5.1 * 1544 + 3088 = 10962.4
5.1 * 1545 + 3090 = 10969.5
5.1 * 1546 + 3092 = 10976.6
5.1 * 1547 + 3094 = 10983.7
5.1 * 1548 + 3096 = 10990.8
5.1 * 1549 + 3098 = 10997.9
5.1 * 1550 + 3100 = 11005
5.1 * 1551 + 3102 = 11012.1
5.1 * 1552 + 3104 = 11019.2
5.1 * 1553 + 3106 = 11026.3
5.1 * 1554 + 3108 = 11033.4
5.1 * 1555 + 3110 = 11040.5
5.1 * 1556 + 3112 = 11047.6
5.1 * 1557 + 3114 = 11054.7
5.1 * 1558 + 3116 = 11061.8
5.1 * 1559 + 3118 = 11068.9
5.1 * 1560 + 3120 = 11076
5.1 * 1561 + 3122 = 11083.1
5.1 * 1562 + 3124 = 11090.2
5.1 * 1563 + 3126 = 11097.3
5.1 * 1564 + 3128 = 11104.4
5.1 * 1565 + 3130 = 11111.5
5.1 * 1566 + 3132 = 11118.6
5.1 * 1567 + 3134 = 11125.7
5.1 * 1568 + 3136 = 11132.8
5.1 * 1569 + 3138 = 11139.9
5.1 * 1570 + 3140 = 11147
5.1 * 1571 + 3142 = 11154.1
5.1 * 1572 + 3144 = 11161.2
5.1 * 1573 + 3146 = 11168.3
5.1 * 1574 + 3148 = 11175.4
5.1 * 1575 + 3150 = 11182.5
5.1 * 1576 + 3152 = 11189.6
5.1 * 1577 + 3154 = 11196.7
5.1 * 1578 + 3156 = 11203.8
5.1 * 1579 + 3158 = 11210.9
5.1 * 1580 + 3160 = 11218
5.1 * 1581 + 3162 = 11225.1
5.1 * 1582 + 3164 = 11232.2
5.1 * 1583 + 3166 = 11239.3
5.1 * 1584 + 3168 = 11246.4
5.1 * 1585 + 3170 = 11253.5
5.1 * 1586 + 3172 = 11260.6
5.1 * 1587 + 3174 = 11267.7
5.1 * 1588 + 3176 = 11274.8
5.1 * 1589 + 3178 = 11281.9
5.1 * 1590 + 3180 = 11289
5.1 * 1591 + 3182 = 11296.1
5.1 * 1592 + 3184 = 11303.2
5.1 * 1593 + 3186 = 11310.3
5.1 * 1594 + 3188 = 11317.4
5.1 * 1595 + 3190 = 11324.5
5.1 * 1596 + 3192 = 11331.6
5.1 * 1597 + 3194 = 11338.7
5.1 * 1598 + 3196 = 11345.8
5.1 * 1599 + 3198 = 11352.9
5.1 * 1600 + 3200 = 11360
5.1 * 1601 + 3202 = 11367.1
5.1 * 1602 + 3204 = 11374.2
5.1 * 1603 + 3206 = 11381.3
5.1 * 1604 + 3208 = 11388.4
5.1 * 1605 + 3210 = 11395.5
5.1 * 1606 + 3212 = 11402.6
5.1 * 1607 + 3214 = 11409.7
5.1 * 1608 + 3216 = 11416.8
5.1 * 1609 + 3218 = 11423.9
5.1 * 1610 + 3220 = 11431
5.1 * 1611 + 3222 = 11438.1
5.1 * 1612 + 3224 = 11445.2
5.1 * 1613 + 3226 = 11452.3
5.1 * 1614 + 3228 = 11459.4
5.1 * 1615 + 3230 = 11466.5
5.1 * 1616 + 3232 = 11473.6
5.1 * 1617 + 3234 = 11480.7
5.1 * 1618 + 3236 = 11487.8
5.1 * 1619 + 3238 = 11494.9
5.1 * 1620 + 3240 = 11502
5.1 * 1621 + 3242 = 11509.1
5.1 * 1622 + 3244 = 11516.2
5.1 * 1623 + 3246 = 11523.3
5.1 * 1624 + 3248 = 11530.4
5.1 * 1625 + 3250 = 11537.5
5.1 * 1626 + 3252 = 11544.6
5.1 * 1627 + 3254 = 11551.7
5.1 * 1628 + 3256 = 11558.8
5.1 * 1629 + 3258 = 11565.9
5.1 * 1630 + 3260 = 11573
5.1 * 1631 + 3262 = 11580.1
5.1 * 1632 + 3264 = 11587.2
5.1 * 1633 + 3266 = 11594.3
5.1 * 1634 + 3268 = 11601.4
5.1 * 1635 + 3270 = 11608.5
5.1 * 1636 + 3272 = 11615.6
5.1 * 1637 + 3274 = 11622.7
5.1 * 1638 + 3276 = 11629.8
5.1 * 1639 + 3278 = 11636.9
5.1 * 1640 + 3280 = 11644
5.1 * 1641 + 3282 = 11651.1
5.1 * 1642 + 3284 = 11658.2
5.1 * 1643 + 3286 = 11665.3
5.1 * 1644 + 3288 = 11672.4
5.1 * 1645 + 3290 = 11679.5
5.1 * 1646 + 3292 = 11686.6
5.1 * 1647 + 3294 = 11693.7
5.1 * 1648 + 3296 = 11700.8
5.1 * 1649 + 3298 = 11707.9
5.1 * 1650 + 3300 = 11715
5.1 * 1651 + 3302 = 11722.1
5.1 * 1652 + 3304 = 11729.2
5.1 * 1653 + 3306 = 11736.3
5.1 * 1654 + 3308 = 11743.4
5.1 * 1655 + 3310 = 11750.5
5.1 * 1656 + 3312 = 11757.6
5.1 * 1657 + 3314 = 11764.7
5.1 * 1658 + 3316 = 11771.8
5.1 * 1659 + 3318 = 11778.9
5.1 * 1660 + 3320 = 11786
5.1 * 1661 + 3322 = 11793.1
5.1 * 1662 + 3324 = 11800.2
5.1 * 1663 + 3326 = 11807.3
5.1 * 1664 + 3328 = 11814.4
5.1 * 1665 + 3330 = 11821.5
5.1 * 1666 + 3332 = 11828.6
5.1 * 1667 + 3334 = 11835.7
5.1 * 1668 + 3336 = 11842.8
5.1 * 1669 + 3338 = 11849.9
5.1 * 1670 + 3340 = 11857
5.1 * 1671 + 3342 = 11864.1
5.1 * 1672 + 3344 = 11871.2
5.1 * 1673 + 3346 = 11878.3
5.1 * 1674 + 3348 = 11885.4
5.1 * 1675 + 3350 = 11892.5
5.1 * 1676 + 3352 = 11899.6
5.1 * 1677 + 3354 = 11906.7
5.1 * 1678 + 3356 = 11913.8
5.1 * 1679 + 3358 = 11920.9
5.1 * 1680 + 3360 = 11928
5.1 * 1681 + 3362 = 11935.1
5.1 * 1682 + 3364 = 11942.2
5.1 * 1683 + 3366 = 11949.3
5.1 * 1684 + 3368 = 11956.4
5.1 * 1685 + 3370 = 11963.5
5.1 * 1686 + 3372 = 11970.6
5.1 * 1687 + 3374 = 11977.7
5.1 * 1688 + 3376 = 11984.8
5.1 * 1689 + 3378 = 11991.9
5.1 * 1690 + 3380 = 11999
5.1 * 1691 + 3382 = 12006.1
5.1 * 1692 + 3384 = 12013.2
5.1 * 1693 + 3386 = 12020.3
5.1 * 1694 + 3388 = 12027.4
5.1 * 1695 + 3390 = 12034.5
5.1 * 1696 + 3392 = 12041.6
5.1 * 1697 + 3394 = 12048.7
5.1 * 1698 + 3396 = 12055.8
5.1 * 1699 + 3398 = 12062.9
5.1 * 1700 + 3400 = 12070
5.1 * 1701 + 3402 = 12077.1
5.1 * 1702 + 3404 = 12084.2
5.1 * 1703 + 3406 = 12091.3
5.1 * 1704 + 3408 = 12098.4
5.1 * 1705 + 3410 = 12105.5
5.1 * 1706 + 3412 = 12112.6
5.1 * 1707 + 3414 = 12119.7
5.1 * 1708 + 3416 = 12126.8
5.1 * 1709 + 3418 = 12133.9
5.1 * 1710 + 3420 = 12141
5.1 * 1711 + 3422 = 12148.1
5.1 * 1712 + 3424 = 12155.2
5.1 * 1713 + 3426 = 12162.3
5.1 * 1714 + 3428 = 12169.4
5.1 * 1715 + 3430 = 12176.5
5.1 * 1716 + 3432 = 12183.6
5.1 * 1717 + 3434 = 12190.7
5.1 * 1718 + 3436 = 12197.8
5.1 * 1719 + 3438 = 12204.9
5.1 * 1720 + 3440 = 12212
5.1 * 1721 + 3442 = 12219.1
5.1 * 1722 + 3444 = 12226.2
5.1 * 1723 + 3446 = 12233.3
5.1 * 1724 + 3448 = 12240.4
5.1 * 1725 + 3450 = 12247.5
5.1 * 1726 + 3452 = 12254.6
5.1 * 1727 + 3454 = 12261.7
5.1 * 1728 + 3456 = 12268.8
5.1 * 1729 + 3458 = 12275.9
5.1 * 1730 + 3460 = 12283
5.1 * 1731 + 3462 = 12290.1
5.1 * 1732 + 3464 = 12297.2
5.1 * 1733 + 3466 = 12304.3
5.1 * 1734 + 3468 = 12311.4
5.1 * 1735 + 3470 = 12318.5
5.1 * 1736 + 3472 = 12325.6
5.1 * 1737 + 3474 = 12332.7
5.1 * 1738 + 3476 = 12339.8
5.1 * 1739 + 3478 = 12346.9
5.1 * 1740 + 3480 = 12354
5.1 * 1741 + 3482 = 12361.1
5.1 * 1742 + 3484 = 12368.2
5.1 * 1743 + 3486 = 12375.3
5.1 * 1744 + 3488 = 12382.4
5.1 * 1745 + 3490 = 12389.5
5.1 * 1746 + 3492 = 12396.6
5.1 * 1747 + 3494 = 12403.7
5.1 * 1748 + 3496 = 12410.8
5.1 * 1749 + 3498 = 12417.9
5.1 * 1750 + 3500 = 12425
5.1 * 1751 + 3502 = 12432.1
5.1 * 1752 + 3504 = 12439.2
5.1 * 1753 + 3506 = 12446.3
5.1 * 1754 + 3508 = 12453.4
5.1 * 1755 + 3510 = 12460.5
5.1 * 1756 + 3512 = 12467.6
5.1 * 1757 + 3514 = 12474.7
5.1 * 1758 + 3516 = 12481.8
5.1 * 1759 + 3518 = 12488.9
5.1 * 1760 + 3520 = 12496
5.1 * 1761 + 3522 = 12503.1
5.1 * 1762 + 3524 = 12510.2
5.1 * 1763 + 3526 = 12517.3
5.1 * 1764 + 3528 = 12524.4
5.1 * 1765 + 3530 = 12531.5
5.1 * 1766 + 3532 = 12538.6
5.1 * 1767 + 3534 = 12545.7
5.1 * 1768 + 3536 = 12552.8
5.1 * 1769 + 3538 = 12559.9
5.1 * 1770 + 3540 = 12567
5.1 * 1771 + 3542 = 12574.1
5.1 * 1772 + 3544 = 12581.2
5.1 * 1773 + 3546 = 12588.3
5.1 * 1774 + 3548 = 12595.4
5.1 * 1775 + 3550 = 12602.5
5.1 * 1776 + 3552 = 12609.6
5.1 * 1777 + 3554 = 12616.7
5.1 * 1778 + 3556 = 12623.8
5.1 * 1779 + 3558 = 12630.9
5.1 * 1780 + 3560 = 12638
5.1 * 1781 + 3562 = 12645.1
5.1 * 1782 + 3564 = 12652.2
5.1 * 1783 + 3566 = 12659.3
5.1 * 1784 + 3568 = 12666.4
5.1 * 1785 + 3570 = 12673.5
5.1 * 1786 + 3572 = 12680.6
5.1 * 1787 + 3574 = 12687.7
5.1 * 1788 + 3576 = 12694.8
5.1 * 1789 + 3578 = 12701.9
5.1 * 1790 + 3580 = 12709
5.1 * 1791 + 3582 = 12716.1
5.1 * 1792 + 3584 = 12723.2
5.1 * 1793 + 3586 = 12730.3
5.1 * 1794 + 3588 = 12737.4
5.1 * 1795 + 3590 = 12744.5
5.1 * 1796 + 3592 = 12751.6
5.1 * 1797 + 3594 = 12758.7
5.1 * 1798 + 3596 = 12765.8
5.1 * 1799 + 3598 = 12772.9
5.1 * 1800 + 3600 = 12780
5.1 * 1801 + 3602 = 12787.1
5.1 * 1802 + 3604 = 12794.2
5.1 * 1803 + 3606 = 12801.3
5.1 * 1804 + 3608 = 12808.4
5.1 * 1805 + 3610 = 12815.5
5.1 * 1806 + 3612 = 12822.6
5.1 * 1807 + 3614 = 12829.7
5.1 * 1808 + 3616 = 12836.8
5.1 * 1809 + 3618 = 12843.9
5.1 * 1810 + 3620 = 12851
5.1 * 1811 + 3622 = 12858.1
5.1 * 1812 + 3624 = 12865.2
5.1 * 1813 + 3626 = 12872.3
5.1 * 1814 + 3628 = 12879.4
5.1 * 1815 + 3630 = 12886.5
5.1 * 1816 + 3632 = 12893.6
5.1 * 1817 + 3634 = 12900.7
5.1 * 1818 + 3636 = 12907.8
5.1 * 1819 + 3638 = 12914.9
5.1 * 1820 + 3640 = 12922
5.1 * 1821 + 3642 = 12929.1
5.1 * 1822 + 3644 = 12936.2
5.1 * 1823 + 3646 = 12943.3
5.1 * 1824 + 3648 = 12950.4
5.1 * 1825 + 3650 = 12957.5
5.1 * 1826 + 3652 = 12964.6
5.1 * 1827 + 3654 = 12971.7
5.1 * 1828 + 3656 = 12978.8
5.1 * 1829 + 3658 = 12985.9
5.1 * 1830 + 3660 = 12993
5.1 * 1831 + 3662 = 13000.1
5.1 * 1832 + 3664 = 13007.2
5.1 * 1833 + 3666 = 13014.3
5.1 * 1834 + 3668 = 13021.4
5.1 * 1835 + 3670 = 13028.5
5.1 * 1836 + 3672 = 13035.6
5.1 * 1837 + 3674 = 13042.7
5.1 * 1838 + 3676 = 13049.8
5.1 * 1839 + 3678 = 13056.9
5.1 * 1840 + 3680 = 13064
5.1 * 1841 + 3682 = 13071.1
5.1 * 1842 + 3684 = 13078.2
5.1 * 1843 + 3686 = 13085.3
5.1 * 1844 + 3688 = 13092.4
5.1 * 1845 + 3690 = 13099.5
5.1 * 1846 + 3692 = 13106.6
5.1 * 1847 + 3694 = 13113.7
5.1 * 1848 + 3696 = 13120.8
5.1 * 1849 + 3698 = 13127.9
5.1 * 1850 + 3700 = 13135
5.1 * 1851 + 3702 = 13142.1
5.1 * 1852 + 3704 = 13149.2
5.1 * 1853 + 3706 = 13156.3
5.1 * 1854 + 3708 = 13163.4
5.1 * 1855 + 3710 = 13170.5
5.1 * 1856 + 3712 = 13177.6
5.1 * 1857 + 3714 = 13184.7
5.1 * 1858 + 3716 = 13191.8
5.1 * 1859 + 3718 = 13198.9
5.1 * 1860 + 3720 = 13206
5.1 * 1861 + 3722 = 13213.1
5.1 * 1862 + 3724 = 13220.2
5.1 * 1863 + 3726 = 13227.3
5.1 * 1864 + 3728 = 13234.4
5.1 * 1865 + 3730 = 13241.5
5.1 * 1866 + 3732 = 13248.6
5.1 * 1867 + 3734 = 13255.7
5.1 * 1868 + 3736 = 13262.8
5.1 * 1869 + 3738 = 13269.9
5.1 * 1870 + 3740 = 13277
5.1 * 1871 + 3742 = 13284.1
5.1 * 1872 + 3744 = 13291.2
5.1 * 1873 + 3746 = 13298.3
5.1 * 1874 + 3748 = 13305.4
5.1 * 1875 + 3750 = 13312.5
5.1 * 1876 + 3752 = 13319.6
5.1 * 1877 + 3754 = 13326.7
5.1 * 1878 + 3756 = 13333.8
5.1 * 1879 + 3758 = 13340.9
5.1 * 1880 + 3760 = 13348
5.1 * 1881 + 3762 = 13355.1
5.1 * 1882 + 3764 = 13362.2
5.1 * 1883 + 3766 = 13369.3
5.1 * 1884 + 3768 = 13376.4
5.1 * 1885 + 3770 = 13383.5
5.1 * 1886 + 3772 = 13390.6
5.1 * 1887 + 3774 = 13397.7
5.1 * 1888 + 3776 = 13404.8
5.1 * 1889 + 3778 = 13411.9
5.1 * 1890 + 3780 = 13419
5.1 * 1891 + 3782 = 13426.1
5.1 * 1892 + 3784 = 13433.2
5.1 * 1893 + 3786 = 13440.3
5.1 * 1894 + 3788 = 13447.4
5.1 * 1895 + 3790 = 13454.5
5.1 * 1896 + 3792 = 13461.6
5.1 * 1897 + 3794 = 13468.7
5.1 * 1898 + 3796 = 13475.8
5.1 * 1899 + 3798 = 13482.9
5.1 * 1900 + 3800 = 13490
5.1 * 1901 + 3802 = 13497.1
5.1 * 1902 + 3804 = 13504.2
5.1 * 1903 + 3806 = 13511.3
5.1 * 1904 + 3808 = 13518.4
5.1 * 1905 + 3810 = 13525.5
5.1 * 1906 + 3812 = 13532.6
5.1 * 1907 + 3814 = 13539.7
5.1 * 1908 + 3816 = 13546.8
5.1 * 1909 + 3818 = 13553.9
5.1 * 1910 + 3820 = 13561
5.1 * 1911 + 3822 = 13568.1
5.1 * 1912 + 3824 = 13575.2
5.1 * 1913 + 3826 = 13582.3
5.1 * 1914 + 3828 = 13589.4
5.1 * 1915 + 3830 = 13596.5
5.1 * 1916 + 3832 = 13603.6
5.1 * 1917 + 3834 = 13610.7
5.1 * 1918 + 3836 = 13617.8
5.1 * 1919 + 3838 = 13624.9
5.1 * 1920 + 3840 = 13632
5.1 * 1921 + 3842 = 13639.1
5.1 * 1922 + 3844 = 13646.2
5.1 * 1923 + 3846 = 13653.3
5.1 * 1924 + 3848 = 13660.4
5.1 * 1925 + 3850 = 13667.5
5.1 * 1926 + 3852 = 13674.6
5.1 * 1927 + 3854 = 13681.7
5.1 * 1928 + 3856 = 13688.8
5.1 * 1929 + 3858 = 13695.9
5.1 * 1930 + 3860 = 13703
5.1 * 1931 + 3862 = 13710.1
5.1 * 1932 + 3864 = 13717.2
5.1 * 1933 + 3866 = 13724.3
5.1 * 1934 + 3868 = 13731.4
5.1 * 1935 + 3870 = 13738.5
5.1 * 1936 + 3872 = 13745.6
5.1 * 1937 + 3874 = 13752.7
5.1 * 1938 + 3876 = 13759.8
5.1 * 1939 + 3878 = 13766.9
5.1 * 1940 + 3880 = 13774
5.1 * 1941 + 3882 = 13781.1
5.1 * 1942 + 3884 = 13788.2
5.1 * 1943 + 3886 = 13795.3
5.1 * 1944 + 3888 = 13802.4
5.1 * 1945 + 3890 = 13809.5
5.1 * 1946 + 3892 = 13816.6
5.1 * 1947 + 3894 = 13823.7
5.1 * 1948 + 3896 = 13830.8
5.1 * 1949 + 3898 = 13837.9
5.1 * 1950 + 3900 = 13845
5.1 * 1951 + 3902 = 13852.1
5.1 * 1952 + 3904 = 13859.2
5.1 * 1953 + 3906 = 13866.3
5.1 * 1954 + 3908 = 13873.4
5.1 * 1955 + 3910 = 13880.5
5.1 * 1956 + 3912 = 13887.6
5.1 * 1957 + 3914 = 13894.7
5.1 * 1958 + 3916 = 13901.8
5.1 * 1959 + 3918 = 13908.9
5.1 * 1960 + 3920 = 13916
5.1 * 1961 + 3922 = 13923.1
5.1 * 1962 + 3924 = 13930.2
5.1 * 1963 + 3926 = 13937.3
5.1 * 1964 + 3928 = 13944.4
5.1 * 1965 + 3930 = 13951.5
5.1 * 1966 + 3932 = 13958.6
5.1 * 1967 + 3934 = 13965.7
5.1 * 1968 + 3936 = 13972.8
5.1 * 1969 + 3938 = 13979.9
5.1 * 1970 + 3940 = 13987
5.1 * 1971 + 3942 = 13994.1
5.1 * 1972 + 3944 = 14001.2
5.1 * 1973 + 3946 = 14008.3
5.1 * 1974 + 3948 = 14015.4
5.1 * 1975 + 3950 = 14022.5
5.1 * 1976 + 3952 = 14029.6
5.1 * 1977 + 3954 = 14036.7
5.1 * 1978 + 3956 = 14043.8
5.1 * 1979 + 3958 = 14050.9
5.1 * 1980 + 3960 = 14058
5.1 * 1981 + 3962 = 14065.1
5.1 * 1982 + 3964 = 14072.2
5.1 * 1983 + 3966 = 14079.3
5.1 * 1984 + 3968 = 14086.4
5.1 * 1985 + 3970 = 14093.5
5.1 * 1986 + 3972 = 14100.6
5.1 * 1987 + 3974 = 14107.7
5.1 * 1988 + 3976 = 14114.8
5.1 * 1989 + 3978 = 14121.9
5.1 * 1990 + 3980 = 14129
5.1 * 1991 + 3982 = 14136.1
5.1 * 1992 + 3984 = 14143.2
5.1 * 1993 + 3986 = 14150.3
5.1 * 1994 + 3988 = 14157.4
5.1 * 1995 + 3990 = 14164.5
5.1 * 1996 + 3992 = 14171.6
5.1 * 1997 + 3994 = 14178.7
5.1 * 1998 + 3996 = 14185.8
5.1 * 1999 + 3998 = 14192.9
5.1 * 2000 + 4000 = 14200
5.1 * 2001 + 4002 = 14207.1
5.1 * 2002 + 4004 = 14214.2
5.1 * 2003 + 4006 = 14221.3
5.1 * 2004 + 4008 = 14228.4
5.1 * 2005 + 4010 = 14235.5
5.1 * 2006 + 4012 = 14242.6
5.1 * 2007 + 4014 = 14249.7
5.1 * 2008 + 4016 = 14256.8
5.1 * 2009 + 4018 = 14263.9
5.1 * 2010 + 4020 = 14271
5.1 * 2011 + 4022 = 14278.1
5.1 * 2012 + 4024 = 14285.2
5.1 * 2013 + 4026 = 14292.3
5.1 * 2014 + 4028 = 14299.4
5.1 * 2015 + 4030 = 14306.5
5.1 * 2016 + 4032 = 14313.6
5.1 * 2017 + 4034 = 14320.7
5.1 * 2018 + 4036 = 14327.8
5.1 * 2019 + 4038 = 14334.9
5.1 * 2020 + 4040 = 14342
5.1 * 2021 + 4042 = 14349.1
5.1 * 2022 + 4044 = 14356.2
5.1 * 2023 + 4046 = 14363.3
5.1 * 2024 + 4048 = 14370.4
5.1 * 2025 + 4050 = 14377.5
5.1 * 2026 + 4052 = 14384.6
5.1 * 2027 + 4054 = 14391.7
5.1 * 2028 + 4056 = 14398.8
5.1 * 2029 + 4058 = 14405.9
5.1 * 2030 + 4060 = 14413
5.1 * 2031 + 4062 = 14420.1
5.1 * 2032 + 4064 = 14427.2
5.1 * 2033 + 4066 = 14434.3
5.1 * 2034 + 4068 = 14441.4
5.1 * 2035 + 4070 = 14448.5
5.1 * 2036 + 4072 = 14455.6
5.1 * 2037 + 4074 = 14462.7
5.1 * 2038 + 4076 = 14469.8
5.1 * 2039 + 4078 = 14476.9
5.1 * 2040 + 4080 = 14484
5.1 * 2041 + 4082 = 14491.1
5.1 * 2042 + 4084 = 14498.2
5.1 * 2043 + 4086 = 14505.3
5.1 * 2044 + 4088 = 14512.4
5.1 * 2045 + 4090 = 14519.5
5.1 * 2046 + 4092 = 14526.6
5.1 * 2047 + 4094 = 14533.7
5.1 * 2048 + 4096 = 14540.8
5.1 * 2049 + 4098 = 14547.9
5.1 * 2050 + 4100 = 14555
5.1 * 2051 + 4102 = 14562.1
5.1 * 2052 + 4104 = 14569.2
5.1 * 2053 + 4106 = 14576.3
5.1 * 2054 + 4108 = 14583.4
5.1 * 2055 + 4110 = 14590.5
5.1 * 2056 + 4112 = 14597.6
5.1 * 2057 + 4114 = 14604.7
5.1 * 2058 + 4116 = 14611.8
5.1 * 2059 + 4118 = 14618.9
5.1 * 2060 + 4120 = 14626
5.1 * 2061 + 4122 = 14633.1
5.1 * 2062 + 4124 = 14640.2
5.1 * 2063 + 4126 = 14647.3
5.1 * 2064 + 4128 = 14654.4
5.1 * 2065 + 4130 = 14661.5
5.1 * 2066 + 4132 = 14668.6
5.1 * 2067 + 4134 = 14675.7
5.1 * 2068 + 4136 = 14682.8
5.1 * 2069 + 4138 = 14689.9
5.1 * 2070 + 4140 = 14697
5.1 * 2071 + 4142 = 14704.1
5.1 * 2072 + 4144 = 14711.2
5.1 * 2073 + 4146 = 14718.3
5.1 * 2074 + 4148 = 14725.4
5.1 * 2075 + 4150 = 14732.5
5.1 * 2076 + 4152 = 14739.6
5.1 * 2077 + 4154 = 14746.7
5.1 * 2078 + 4156 = 14753.8
5.1 * 2079 + 4158 = 14760.9
5.1 * 2080 + 4160 = 14768
5.1 * 2081 + 4162 = 14775.1
5.1 * 2082 + 4164 = 14782.2
5.1 * 2083 + 4166 = 14789.3
5.1 * 2084 + 4168 = 14796.4
5.1 * 2085 + 4170 = 14803.5
5.1 * 2086 + 4172 = 14810.6
5.1 * 2087 + 4174 = 14817.7
5.1 * 2088 + 4176 = 14824.8
5.1 * 2089 + 4178 = 14831.9
5.1 * 2090 + 4180 = 14839
5.1 * 2091 + 4182 = 14846.1
5.1 * 2092 + 4184 = 14853.2
5.1 * 2093 + 4186 = 14860.3
5.1 * 2094 + 4188 = 14867.4
5.1 * 2095 + 4190 = 14874.5
5.1 * 2096 + 4192 = 14881.6
5.1 * 2097 + 4194 = 14888.7
5.1 * 2098 + 4196 = 14895.8
5.1 * 2099 + 4198 = 14902.9
5.1 * 2100 + 4200 = 14910
5.1 * 2101 + 4202 = 14917.1
5.1 * 2102 + 4204 = 14924.2
5.1 * 2103 + 4206 = 14931.3
5.1 * 2104 + 4208 = 14938.4
5.1 * 2105 + 4210 = 14945.5
5.1 * 2106 + 4212 = 14952.6
5.1 * 2107 + 4214 = 14959.7
5.1 * 2108 + 4216 = 14966.8
5.1 * 2109 + 4218 = 14973.9
5.1 * 2110 + 4220 = 14981
5.1 * 2111 + 4222 = 14988.1
5.1 * 2112 + 4224 = 14995.2
5.1 * 2113 + 4226 = 15002.3
5.1 * 2114 + 4228 = 15009.4
5.1 * 2115 + 4230 = 15016.5
5.1 * 2116 + 4232 = 15023.6
5.1 * 2117 + 4234 = 15030.7
5.1 * 2118 + 4236 = 15037.8
5.1 * 2119 + 4238 = 15044.9
5.1 * 2120 + 4240 = 15052
5.1 * 2121 + 4242 = 15059.1
5.1 * 2122 + 4244 = 15066.2
5.1 * 2123 + 4246 = 15073.3
5.1 * 2124 + 4248 = 15080.4
5.1 * 2125 + 4250 = 15087.5
5.1 * 2126 + 4252 = 15094.6
5.1 * 2127 + 4254 = 15101.7
5.1 * 2128 + 4256 = 15108.8
5.1 * 2129 + 4258 = 15115.9
5.1 * 2130 + 4260 = 15123
5.1 * 2131 + 4262 = 15130.1
5.1 * 2132 + 4264 = 15137.2
5.1 * 2133 + 4266 = 15144.3
5.1 * 2134 + 4268 = 15151.4
5.1 * 2135 + 4270 = 15158.5
5.1 * 2136 + 4272 = 15165.6
5.1 * 2137 + 4274 = 15172.7
5.1 * 2138 + 4276 = 15179.8
5.1 * 2139 + 4278 = 15186.9
5.1 * 2140 + 4280 = 15194
5.1 * 2141 + 4282 = 15201.1
5.1 * 2142 + 4284 = 15208.2
5.1 * 2143 + 4286 = 15215.3
5.1 * 2144 + 4288 = 15222.4
5.1 * 2145 + 4290 = 15229.5
5.1 * 2146 + 4292 = 15236.6
5.1 * 2147 + 4294 = 15243.7
5.1 * 2148 + 4296 = 15250.8
5.1 * 2149 + 4298 = 15257.9
5.1 * 2150 + 4300 = 15265
5.1 * 2151 + 4302 = 15272.1
5.1 * 2152 + 4304 = 15279.2
5.1 * 2153 + 4306 = 15286.3
5.1 * 2154 + 4308 = 15293.4
5.1 * 2155 + 4310 = 15300.5
5.1 * 2156 + 4312 = 15307.6
5.1 * 2157 + 4314 = 15314.7
5.1 * 2158 + 4316 = 15321.8
5.1 * 2159 + 4318 = 15328.9
5.1 * 2160 + 4320 = 15336
5.1 * 2161 + 4322 = 15343.1
5.1 * 2162 + 4324 = 15350.2
5.1 * 2163 + 4326 = 15357.3
5.1 * 2164 + 4328 = 15364.4
5.1 * 2165 + 4330 = 15371.5
5.1 * 2166 + 4332 = 15378.6
5.1 * 2167 + 4334 = 15385.7
5.1 * 2168 + 4336 = 15392.8
5.1 * 2169 + 4338 = 15399.9
5.1 * 2170 + 4340 = 15407
5.1 * 2171 + 4342 = 15414.1
5.1 * 2172 + 4344 = 15421.2
5.1 * 2173 + 4346 = 15428.3
5.1 * 2174 + 4348 = 15435.4
5.1 * 2175 + 4350 = 15442.5
5.1 * 2176 + 4352 = 15449.6
5.1 * 2177 + 4354 = 15456.7
5.1 * 2178 + 4356 = 15463.8
5.1 * 2179 + 4358 = 15470.9
5.1 * 2180 + 4360 = 15478
5.1 * 2181 + 4362 = 15485.1
5.1 * 2182 + 4364 = 15492.2
5.1 * 2183 + 4366 = 15499.3
5.1 * 2184 + 4368 = 15506.4
5.1 * 2185 + 4370 = 15513.5
5.1 * 2186 + 4372 = 15520.6
5.1 * 2187 + 4374 = 15527.7
5.1 * 2188 + 4376 = 15534.8
5.1 * 2189 + 4378 = 15541.9
5.1 * 2190 + 4380 = 15549
5.1 * 2191 + 4382 = 15556.1
5.1 * 2192 + 4384 = 15563.2
5.1 * 2193 + 4386 = 15570.3
5.1 * 2194 + 4388 = 15577.4
5.1 * 2195 + 4390 = 15584.5
5.1 * 2196 + 4392 = 15591.6
5.1 * 2197 + 4394 = 15598.7
5.1 * 2198 + 4396 = 15605.8
5.1 * 2199 + 4398 = 15612.9
5.1 * 2200 + 4400 = 15620
5.1 * 2201 + 4402 = 15627.1
5.1 * 2202 + 4404 = 15634.2
5.1 * 2203 + 4406 = 15641.3
5.1 * 2204 + 4408 = 15648.4
5.1 * 2205 + 4410 = 15655.5
5.1 * 2206 + 4412 = 15662.6
5.1 * 2207 + 4414 = 15669.7
5.1 * 2208 + 4416 = 15676.8
5.1 * 2209 + 4418 = 15683.9
5.1 * 2210 + 4420 = 15691
5.1 * 2211 + 4422 = 15698.1
5.1 * 2212 + 4424 = 15705.2
5.1 * 2213 + 4426 = 15712.3
5.1 * 2214 + 4428 = 15719.4
5.1 * 2215 + 4430 = 15726.5
5.1 * 2216 + 4432 = 15733.6
5.1 * 2217 + 4434 = 15740.7
5.1 * 2218 + 4436 = 15747.8
5.1 * 2219 + 4438 = 15754.9
5.1 * 2220 + 4440 = 15762
5.1 * 2221 + 4442 = 15769.1
5.1 * 2222 + 4444 = 15776.2
5.1 * 2223 + 4446 = 15783.3
5.1 * 2224 + 4448 = 15790.4
5.1 * 2225 + 4450 = 15797.5
5.1 * 2226 + 4452 = 15804.6
5.1 * 2227 + 4454 = 15811.7
5.1 * 2228 + 4456 = 15818.8
5.1 * 2229 + 4458 = 15825.9
5.1 * 2230 + 4460 = 15833
5.1 * 2231 + 4462 = 15840.1
5.1 * 2232 + 4464 = 15847.2
5.1 * 2233 + 4466 = 15854.3
5.1 * 2234 + 4468 = 15861.4
5.1 * 2235 + 4470 = 15868.5
5.1 * 2236 + 4472 = 15875.6
5.1 * 2237 + 4474 = 15882.7
5.1 * 2238 + 4476 = 15889.8
5.1 * 2239 + 4478 = 15896.9
5.1 * 2240 + 4480 = 15904
5.1 * 2241 + 4482 = 15911.1
5.1 * 2242 + 4484 = 15918.2
5.1 * 2243 + 4486 = 15925.3
5.1 * 2244 + 4488 = 15932.4
5.1 * 2245 + 4490 = 15939.5
5.1 * 2246 + 4492 = 15946.6
5.1 * 2247 + 4494 = 15953.7
5.1 * 2248 + 4496 = 15960.8
5.1 * 2249 + 4498 = 15967.9
5.1 * 2250 + 4500 = 15975
5.1 * 2251 + 4502 = 15982.1
5.1 * 2252 + 4504 = 15989.2
5.1 * 2253 + 4506 = 15996.3
5.1 * 2254 + 4508 = 16003.4
5.1 * 2255 + 4510 = 16010.5
5.1 * 2256 + 4512 = 16017.6
5.1 * 2257 + 4514 = 16024.7
5.1 * 2258 + 4516 = 16031.8
5.1 * 2259 + 4518 = 16038.9
5.1 * 2260 + 4520 = 16046
5.1 * 2261 + 4522 = 16053.1
5.1 * 2262 + 4524 = 16060.2
5.1 * 2263 + 4526 = 16067.3
5.1 * 2264 + 4528 = 16074.4
5.1 * 2265 + 4530 = 16081.5
5.1 * 2266 + 4532 = 16088.6
5.1 * 2267 + 4534 = 16095.7
5.1 * 2268 + 4536 = 16102.8
5.1 * 2269 + 4538 = 16109.9
5.1 * 2270 + 4540 = 16117
5.1 * 2271 + 4542 = 16124.1
5.1 * 2272 + 4544 = 16131.2
5.1 * 2273 + 4546 = 16138.3
5.1 * 2274 + 4548 = 16145.4
5.1 * 2275 + 4550 = 16152.5
5.1 * 2276 + 4552 = 16159.6
5.1 * 2277 + 4554 = 16166.7
5.1 * 2278 + 4556 = 16173.8
5.1 * 2279 + 4558 = 16180.9
5.1 * 2280 + 4560 = 16188
5.1 * 2281 + 4562 = 16195.1
5.1 * 2282 + 4564 = 16202.2
5.1 * 2283 + 4566 = 16209.3
5.1 * 2284 + 4568 = 16216.4
5.1 * 2285 + 4570 = 16223.5
5.1 * 2286 + 4572 = 16230.6
5.1 * 2287 + 4574 = 16237.7
5.1 * 2288 + 4576 = 16244.8
5.1 * 2289 + 4578 = 16251.9
5.1 * 2290 + 4580 = 16259
5.1 * 2291 + 4582 = 16266.1
5.1 * 2292 + 4584 = 16273.2
5.1 * 2293 + 4586 = 16280.3
5.1 * 2294 + 4588 = 16287.4
5.1 * 2295 + 4590 = 16294.5
5.1 * 2296 + 4592 = 16301.6
5.1 * 2297 + 4594 = 16308.7
5.1 * 2298 + 4596 = 16315.8
5.1 * 2299 + 4598 = 16322.9
5.1 * 2300 + 4600 = 16330
5.1 * 2301 + 4602 = 16337.1
5.1 * 2302 + 4604 = 16344.2
5.1 * 2303 + 4606 = 16351.3
5.1 * 2304 + 4608 = 16358.4
5.1 * 2305 + 4610 = 16365.5
5.1 * 2306 + 4612 = 16372.6
5.1 * 2307 + 4614 = 16379.7
5.1 * 2308 + 4616 = 16386.8
5.1 * 2309 + 4618 = 16393.9
5.1 * 2310 + 4620 = 16401
5.1 * 2311 + 4622 = 16408.1
5.1 * 2312 + 4624 = 16415.2
5.1 * 2313 + 4626 = 16422.3
5.1 * 2314 + 4628 = 16429.4
5.1 * 2315 + 4630 = 16436.5
5.1 * 2316 + 4632 = 16443.6
5.1 * 2317 + 4634 = 16450.7
5.1 * 2318 + 4636 = 16457.8
5.1 * 2319 + 4638 = 16464.9
5.1 * 2320 + 4640 = 16472
5.1 * 2321 + 4642 = 16479.1
5.1 * 2322 + 4644 = 16486.2
5.1 * 2323 + 4646 = 16493.3
5.1 * 2324 + 4648 = 16500.4
5.1 * 2325 + 4650 = 16507.5
5.1 * 2326 + 4652 = 16514.6
5.1 * 2327 + 4654 = 16521.7
5.1 * 2328 + 4656 = 16528.8
5.1 * 2329 + 4658 = 16535.9
5.1 * 2330 + 4660 = 16543
5.1 * 2331 + 4662 = 16550.1
5.1 * 2332 + 4664 = 16557.2
5.1 * 2333 + 4666 = 16564.3
5.1 * 2334 + 4668 = 16571.4
5.1 * 2335 + 4670 = 16578.5
5.1 * 2336 + 4672 = 16585.6
5.1 * 2337 + 4674 = 16592.7
5.1 * 2338 + 4676 = 16599.8
5.1 * 2339 + 4678 = 16606.9
5.1 * 2340 + 4680 = 16614
5.1 * 2341 + 4682 = 16621.1
5.1 * 2342 + 4684 = 16628.2
5.1 * 2343 + 4686 = 16635.3
5.1 * 2344 + 4688 = 16642.4
5.1 * 2345 + 4690 = 16649.5
5.1 * 2346 + 4692 = 16656.6
5.1 * 2347 + 4694 = 16663.7
5.1 * 2348 + 4696 = 16670.8
5.1 * 2349 + 4698 = 16677.9
5.1 * 2350 + 4700 = 16685
5.1 * 2351 + 4702 = 16692.1
5.1 * 2352 + 4704 = 16699.2
5.1 * 2353 + 4706 = 16706.3
5.1 * 2354 + 4708 = 16713.4
5.1 * 2355 + 4710 = 16720.5
5.1 * 2356 + 4712 = 16727.6
5.1 * 2357 + 4714 = 16734.7
5.1 * 2358 + 4716 = 16741.8
5.1 * 2359 + 4718 = 16748.9
5.1 * 2360 + 4720 = 16756
5.1 * 2361 + 4722 = 16763.1
5.1 * 2362 + 4724 = 16770.2
5.1 * 2363 + 4726 = 16777.3
5.1 * 2364 + 4728 = 16784.4
5.1 * 2365 + 4730 = 16791.5
5.1 * 2366 + 4732 = 16798.6
5.1 * 2367 + 4734 = 16805.7
5.1 * 2368 + 4736 = 16812.8
5.1 * 2369 + 4738 = 16819.9
5.1 * 2370 + 4740 = 16827
5.1 * 2371 + 4742 = 16834.1
5.1 * 2372 + 4744 = 16841.2
5.1 * 2373 + 4746 = 16848.3
5.1 * 2374 + 4748 = 16855.4
5.1 * 2375 + 4750 = 16862.5
5.1 * 2376 + 4752 = 16869.6
5.1 * 2377 + 4754 = 16876.7
5.1 * 2378 + 4756 = 16883.8
5.1 * 2379 + 4758 = 16890.9
5.1 * 2380 + 4760 = 16898
5.1 * 2381 + 4762 = 16905.1
5.1 * 2382 + 4764 = 16912.2
5.1 * 2383 + 4766 = 16919.3
5.1 * 2384 + 4768 = 16926.4
5.1 * 2385 + 4770 = 16933.5
5.1 * 2386 + 4772 = 16940.6
5.1 * 2387 + 4774 = 16947.7
5.1 * 2388 + 4776 = 16954.8
5.1 * 2389 + 4778 = 16961.9
5.1 * 2390 + 4780 = 16969
5.1 * 2391 + 4782 = 16976.1
5.1 * 2392 + 4784 = 16983.2
5.1 * 2393 + 4786 = 16990.3
5.1 * 2394 + 4788 = 16997.4
5.1 * 2395 + 4790 = 17004.5
5.1 * 2396 + 4792 = 17011.6
5.1 * 2397 + 4794 = 17018.7
5.1 * 2398 + 4796 = 17025.8
5.1 * 2399 + 4798 = 17032.9
5.1 * 2400 + 4800 = 17040
5.1 * 2401 + 4802 = 17047.1
5.1 * 2402 + 4804 = 17054.2
5.1 * 2403 + 4806 = 17061.3
5.1 * 2404 + 4808 = 17068.4
5.1 * 2405 + 4810 = 17075.5
5.1 * 2406 + 4812 = 17082.6
5.1 * 2407 + 4814 = 17089.7
5.1 * 2408 + 4816 = 17096.8
5.1 * 2409 + 4818 = 17103.9
5.1 * 2410 + 4820 = 17111
5.1 * 2411 + 4822 = 17118.1
5.1 * 2412 + 4824 = 17125.2
5.1 * 2413 + 4826 = 17132.3
5.1 * 2414 + 4828 = 17139.4
5.1 * 2415 + 4830 = 17146.5
5.1 * 2416 + 4832 = 17153.6
5.1 * 2417 + 4834 = 17160.7
5.1 * 2418 + 4836 = 17167.8
5.1 * 2419 + 4838 = 17174.9
5.1 * 2420 + 4840 = 17182
5.1 * 2421 + 4842 = 17189.1
5.1 * 2422 + 4844 = 17196.2
5.1 * 2423 + 4846 = 17203.3
5.1 * 2424 + 4848 = 17210.4
5.1 * 2425 + 4850 = 17217.5
5.1 * 2426 + 4852 = 17224.6
5.1 * 2427 + 4854 = 17231.7
5.1 * 2428 + 4856 = 17238.8
5.1 * 2429 + 4858 = 17245.9
5.1 * 2430 + 4860 = 17253
5.1 * 2431 + 4862 = 17260.1
5.1 * 2432 + 4864 = 17267.2
5.1 * 2433 + 4866 = 17274.3
5.1 * 2434 + 4868 = 17281.4
5.1 * 2435 + 4870 = 17288.5
5.1 * 2436 + 4872 = 17295.6
5.1 * 2437 + 4874 = 17302.7
5.1 * 2438 + 4876 = 17309.8
5.1 * 2439 + 4878 = 17316.9
5.1 * 2440 + 4880 = 17324
5.1 * 2441 + 4882 = 17331.1
5.1 * 2442 + 4884 = 17338.2
5.1 * 2443 + 4886 = 17345.3
5.1 * 2444 + 4888 = 17352.4
5.1 * 2445 + 4890 = 17359.5
5.1 * 2446 + 4892 = 17366.6
5.1 * 2447 + 4894 = 17373.7
5.1 * 2448 + 4896 = 17380.8
5.1 * 2449 + 4898 = 17387.9
5.1 * 2450 + 4900 = 17395
5.1 * 2451 + 4902 = 17402.1
5.1 * 2452 + 4904 = 17409.2
5.1 * 2453 + 4906 = 17416.3
5.1 * 2454 + 4908 = 17423.4
5.1 * 2455 + 4910 = 17430.5
5.1 * 2456 + 4912 = 17437.6
5.1 * 2457 + 4914 = 17444.7
5.1 * 2458 + 4916 = 17451.8
5.1 * 2459 + 4918 = 17458.9
5.1 * 2460 + 4920 = 17466
5.1 * 2461 + 4922 = 17473.1
5.1 * 2462 + 4924 = 17480.2
5.1 * 2463 + 4926 = 17487.3
5.1 * 2464 + 4928 = 17494.4
5.1 * 2465 + 4930 = 17501.5
5.1 * 2466 + 4932 = 17508.6
5.1 * 2467 + 4934 = 17515.7
5.1 * 2468 + 4936 = 17522.8
5.1 * 2469 + 4938 = 17529.9
5.1 * 2470 + 4940 = 17537
5.1 * 2471 + 4942 = 17544.1
5.1 * 2472 + 4944 = 17551.2
5.1 * 2473 + 4946 = 17558.3
5.1 * 2474 + 4948 = 17565.4
5.1 * 2475 + 4950 = 17572.5
5.1 * 2476 + 4952 = 17579.6
5.1 * 2477 + 4954 = 17586.7
5.1 * 2478 + 4956 = 17593.8
5.1 * 2479 + 4958 = 17600.9
5.1 * 2480 + 4960 = 17608
5.1 * 2481 + 4962 = 17615.1
5.1 * 2482 + 4964 = 17622.2
5.1 * 2483 + 4966 = 17629.3
5.1 * 2484 + 4968 = 17636.4
5.1 * 2485 + 4970 = 17643.5
5.1 * 2486 + 4972 = 17650.6
5.1 * 2487 + 4974 = 17657.7
5.1 * 2488 + 4976 = 17664.8
5.1 * 2489 + 4978 = 17671.9
5.1 * 2490 + 4980 = 17679
5.1 * 2491 + 4982 = 17686.1
5.1 * 2492 + 4984 = 17693.2
5.1 * 2493 + 4986 = 17700.3
5.1 * 2494 + 4988 = 17707.4
5.1 * 2495 + 4990 = 17714.5
5.1 * 2496 + 4992 = 17721.6
5.1 * 2497 + 4994 = 17728.7
5.1 * 2498 + 4996 = 17735.8
5.1 * 2499 + 4998 = 17742.9
5.1 * 2500 + 5000 = 17750
5.1 * 2501 + 5002 = 17757.1
5.1 * 2502 + 5004 = 17764.2
5.1 * 2503 + 5006 = 17771.3
5.1 * 2504 + 5008 = 17778.4
5.1 * 2505 + 5010 = 17785.5
5.1 * 2506 + 5012 = 17792.6
5.1 * 2507 + 5014 = 17799.7
5.1 * 2508 + 5016 = 17806.8
5.1 * 2509 + 5018 = 17813.9
5.1 * 2510 + 5020 = 17821
5.1 * 2511 + 5022 = 17828.1
5.1 * 2512 + 5024 = 17835.2
5.1 * 2513 + 5026 = 17842.3
5.1 * 2514 + 5028 = 17849.4
5.1 * 2515 + 5030 = 17856.5
5.1 * 2516 + 5032 = 17863.6
5.1 * 2517 + 5034 = 17870.7
5.1 * 2518 + 5036 = 17877.8
5.1 * 2519 + 5038 = 17884.9
5.1 * 2520 + 5040 = 17892
5.1 * 2521 + 5042 = 17899.1
5.1 * 2522 + 5044 = 17906.2
5.1 * 2523 + 5046 = 17913.3
5.1 * 2524 + 5048 = 17920.4
5.1 * 2525 + 5050 = 17927.5
5.1 * 2526 + 5052 = 17934.6
5.1 * 2527 + 5054 = 17941.7
5.1 * 2528 + 5056 = 17948.8
5.1 * 2529 + 5058 = 17955.9
5.1 * 2530 + 5060 = 17963
5.1 * 2531 + 5062 = 17970.1
5.1 * 2532 + 5064 = 17977.2
5.1 * 2533 + 5066 = 17984.3
5.1 * 2534 + 5068 = 17991.4
5.1 * 2535 + 5070 = 17998.5
5.1 * 2536 + 5072 = 18005.6
5.1 * 2537 + 5074 = 18012.7
5.1 * 2538 + 5076 = 18019.8
5.1 * 2539 + 5078 = 18026.9
5.1 * 2540 + 5080 = 18034
5.1 * 2541 + 5082 = 18041.1
5.1 * 2542 + 5084 = 18048.2
5.1 * 2543 + 5086 = 18055.3
5.1 * 2544 + 5088 = 18062.4
5.1 * 2545 + 5090 = 18069.5
5.1 * 2546 + 5092 = 18076.6
5.1 * 2547 + 5094 = 18083.7
5.1 * 2548 + 5096 = 18090.8
5.1 * 2549 + 5098 = 18097.9
5.1 * 2550 + 5100 = 18105
5.1 * 2551 + 5102 = 18112.1
5.1 * 2552 + 5104 = 18119.2
5.1 * 2553 + 5106 = 18126.3
5.1 * 2554 + 5108 = 18133.4
5.1 * 2555 + 5110 = 18140.5
5.1 * 2556 + 5112 = 18147.6
5.1 * 2557 + 5114 = 18154.7
5.1 * 2558 + 5116 = 18161.8
5.1 * 2559 + 5118 = 18168.9
5.1 * 2560 + 5120 = 18176
5.1 * 2561 + 5122 = 18183.1
5.1 * 2562 + 5124 = 18190.2
5.1 * 2563 + 5126 = 18197.3
5.1 * 2564 + 5128 = 18204.4
5.1 * 2565 + 5130 = 18211.5
5.1 * 2566 + 5132 = 18218.6
5.1 * 2567 + 5134 = 18225.7
5.1 * 2568 + 5136 = 18232.8
5.1 * 2569 + 5138 = 18239.9
5.1 * 2570 + 5140 = 18247
5.1 * 2571 + 5142 = 18254.1
5.1 * 2572 + 5144 = 18261.2
5.1 * 2573 + 5146 = 18268.3
5.1 * 2574 + 5148 = 18275.4
5.1 * 2575 + 5150 = 18282.5
5.1 * 2576 + 5152 = 18289.6
5.1 * 2577 + 5154 = 18296.7
5.1 * 2578 + 5156 = 18303.8
5.1 * 2579 + 5158 = 18310.9
5.1 * 2580 + 5160 = 18318
5.1 * 2581 + 5162 = 18325.1
5.1 * 2582 + 5164 = 18332.2
5.1 * 2583 + 5166 = 18339.3
5.1 * 2584 + 5168 = 18346.4
5.1 * 2585 + 5170 = 18353.5
5.1 * 2586 + 5172 = 18360.6
5.1 * 2587 + 5174 = 18367.7
5.1 * 2588 + 5176 = 18374.8
5.1 * 2589 + 5178 = 18381.9
5.1 * 2590 + 5180 = 18389
5.1 * 2591 + 5182 = 18396.1
5.1 * 2592 + 5184 = 18403.2
5.1 * 2593 + 5186 = 18410.3
5.1 * 2594 + 5188 = 18417.4
5.1 * 2595 + 5190 = 18424.5
5.1 * 2596 + 5192 = 18431.6
5.1 * 2597 + 5194 = 18438.7
5.1 * 2598 + 5196 = 18445.8
5.1 * 2599 + 5198 = 18452.9
5.1 * 2600 + 5200 = 18460
5.1 * 2601 + 5202 = 18467.1
5.1 * 2602 + 5204 = 18474.2
5.1 * 2603 + 5206 = 18481.3
5.1 * 2604 + 5208 = 18488.4
5.1 * 2605 + 5210 = 18495.5
5.1 * 2606 + 5212 = 18502.6
5.1 * 2607 + 5214 = 18509.7
5.1 * 2608 + 5216 = 18516.8
5.1 * 2609 + 5218 = 18523.9
5.1 * 2610 + 5220 = 18531
5.1 * 2611 + 5222 = 18538.1
5.1 * 2612 + 5224 = 18545.2
5.1 * 2613 + 5226 = 18552.3
5.1 * 2614 + 5228 = 18559.4
5.1 * 2615 + 5230 = 18566.5
5.1 * 2616 + 5232 = 18573.6
5.1 * 2617 + 5234 = 18580.7
5.1 * 2618 + 5236 = 18587.8
5.1 * 2619 + 5238 = 18594.9
5.1 * 2620 + 5240 = 18602
5.1 * 2621 + 5242 = 18609.1
5.1 * 2622 + 5244 = 18616.2
5.1 * 2623 + 5246 = 18623.3
5.1 * 2624 + 5248 = 18630.4
5.1 * 2625 + 5250 = 18637.5
5.1 * 2626 + 5252 = 18644.6
5.1 * 2627 + 5254 = 18651.7
5.1 * 2628 + 5256 = 18658.8
5.1 * 2629 + 5258 = 18665.9
5.1 * 2630 + 5260 = 18673
5.1 * 2631 + 5262 = 18680.1
5.1 * 2632 + 5264 = 18687.2
5.1 * 2633 + 5266 = 18694.3
5.1 * 2634 + 5268 = 18701.4
5.1 * 2635 + 5270 = 18708.5
5.1 * 2636 + 5272 = 18715.6
5.1 * 2637 + 5274 = 18722.7
5.1 * 2638 + 5276 = 18729.8
5.1 * 2639 + 5278 = 18736.9
5.1 * 2640 + 5280 = 18744
5.1 * 2641 + 5282 = 18751.1
5.1 * 2642 + 5284 = 18758.2
5.1 * 2643 + 5286 = 18765.3
5.1 * 2644 + 5288 = 18772.4
5.1 * 2645 + 5290 = 18779.5
5.1 * 2646 + 5292 = 18786.6
5.1 * 2647 + 5294 = 18793.7
5.1 * 2648 + 5296 = 18800.8
5.1 * 2649 + 5298 = 18807.9
5.1 * 2650 + 5300 = 18815
5.1 * 2651 + 5302 = 18822.1
5.1 * 2652 + 5304 = 18829.2
5.1 * 2653 + 5306 = 18836.3
5.1 * 2654 + 5308 = 18843.4
5.1 * 2655 + 5310 = 18850.5
5.1 * 2656 + 5312 = 18857.6
5.1 * 2657 + 5314 = 18864.7
5.1 * 2658 + 5316 = 18871.8
5.1 * 2659 + 5318 = 18878.9
5.1 * 2660 + 5320 = 18886
5.1 * 2661 + 5322 = 18893.1
5.1 * 2662 + 5324 = 18900.2
5.1 * 2663 + 5326 = 18907.3
5.1 * 2664 + 5328 = 18914.4
5.1 * 2665 + 5330 = 18921.5
5.1 * 2666 + 5332 = 18928.6
5.1 * 2667 + 5334 = 18935.7
5.1 * 2668 + 5336 = 18942.8
5.1 * 2669 + 5338 = 18949.9
5.1 * 2670 + 5340 = 18957
5.1 * 2671 + 5342 = 18964.1
5.1 * 2672 + 5344 = 18971.2
5.1 * 2673 + 5346 = 18978.3
5.1 * 2674 + 5348 = 18985.4
5.1 * 2675 + 5350 = 18992.5
5.1 * 2676 + 5352 = 18999.6
5.1 * 2677 + 5354 = 19006.7
5.1 * 2678 + 5356 = 19013.8
5.1 * 2679 + 5358 = 19020.9
5.1 * 2680 + 5360 = 19028
5.1 * 2681 + 5362 = 19035.1
5.1 * 2682 + 5364 = 19042.2
5.1 * 2683 + 5366 = 19049.3
5.1 * 2684 + 5368 = 19056.4
5.1 * 2685 + 5370 = 19063.5
5.1 * 2686 + 5372 = 19070.6
5.1 * 2687 + 5374 = 19077.7
5.1 * 2688 + 5376 = 19084.8
5.1 * 2689 + 5378 = 19091.9
5.1 * 2690 + 5380 = 19099
5.1 * 2691 + 5382 = 19106.1
5.1 * 2692 + 5384 = 19113.2
5.1 * 2693 + 5386 = 19120.3
5.1 * 2694 + 5388 = 19127.4
5.1 * 2695 + 5390 = 19134.5
5.1 * 2696 + 5392 = 19141.6
5.1 * 2697 + 5394 = 19148.7
5.1 * 2698 + 5396 = 19155.8
5.1 * 2699 + 5398 = 19162.9
5.1 * 2700 + 5400 = 19170
5.1 * 2701 + 5402 = 19177.1
5.1 * 2702 + 5404 = 19184.2
5.1 * 2703 + 5406 = 19191.3
5.1 * 2704 + 5408 = 19198.4
5.1 * 2705 + 5410 = 19205.5
5.1 * 2706 + 5412 = 19212.6
5.1 * 2707 + 5414 = 19219.7
5.1 * 2708 + 5416 = 19226.8
5.1 * 2709 + 5418 = 19233.9
5.1 * 2710 + 5420 = 19241
5.1 * 2711 + 5422 = 19248.1
5.1 * 2712 + 5424 = 19255.2
5.1 * 2713 + 5426 = 19262.3
5.1 * 2714 + 5428 = 19269.4
5.1 * 2715 + 5430 = 19276.5
5.1 * 2716 + 5432 = 19283.6
5.1 * 2717 + 5434 = 19290.7
5.1 * 2718 + 5436 = 19297.8
5.1 * 2719 + 5438 = 19304.9
5.1 * 2720 + 5440 = 19312
5.1 * 2721 + 5442 = 19319.1
5.1 * 2722 + 5444 = 19326.2
5.1 * 2723 + 5446 = 19333.3
5.1 * 2724 + 5448 = 19340.4
5.1 * 2725 + 5450 = 19347.5
5.1 * 2726 + 5452 = 19354.6
5.1 * 2727 + 5454 = 19361.7
5.1 * 2728 + 5456 = 19368.8
5.1 * 2729 + 5458 = 19375.9
5.1 * 2730 + 5460 = 19383
5.1 * 2731 + 5462 = 19390.1
5.1 * 2732 + 5464 = 19397.2
5.1 * 2733 + 5466 = 19404.3
5.1 * 2734 + 5468 = 19411.4
5.1 * 2735 + 5470 = 19418.5
5.1 * 2736 + 5472 = 19425.6
5.1 * 2737 + 5474 = 19432.7
5.1 * 2738 + 5476 = 19439.8
5.1 * 2739 + 5478 = 19446.9
5.1 * 2740 + 5480 = 19454
5.1 * 2741 + 5482 = 19461.1
5.1 * 2742 + 5484 = 19468.2
5.1 * 2743 + 5486 = 19475.3
5.1 * 2744 + 5488 = 19482.4
5.1 * 2745 + 5490 = 19489.5
5.1 * 2746 + 5492 = 19496.6
5.1 * 2747 + 5494 = 19503.7
5.1 * 2748 + 5496 = 19510.8
5.1 * 2749 + 5498 = 19517.9
5.1 * 2750 + 5500 = 19525
5.1 * 2751 + 5502 = 19532.1
5.1 * 2752 + 5504 = 19539.2
5.1 * 2753 + 5506 = 19546.3
5.1 * 2754 + 5508 = 19553.4
5.1 * 2755 + 5510 = 19560.5
5.1 * 2756 + 5512 = 19567.6
5.1 * 2757 + 5514 = 19574.7
5.1 * 2758 + 5516 = 19581.8
5.1 * 2759 + 5518 = 19588.9
5.1 * 2760 + 5520 = 19596
5.1 * 2761 + 5522 = 19603.1
5.1 * 2762 + 5524 = 19610.2
5.1 * 2763 + 5526 = 19617.3
5.1 * 2764 + 5528 = 19624.4
5.1 * 2765 + 5530 = 19631.5
5.1 * 2766 + 5532 = 19638.6
5.1 * 2767 + 5534 = 19645.7
5.1 * 2768 + 5536 = 19652.8
5.1 * 2769 + 5538 = 19659.9
5.1 * 2770 + 5540 = 19667
5.1 * 2771 + 5542 = 19674.1
5.1 * 2772 + 5544 = 19681.2
5.1 * 2773 + 5546 = 19688.3
5.1 * 2774 + 5548 = 19695.4
5.1 * 2775 + 5550 = 19702.5
5.1 * 2776 + 5552 = 19709.6
5.1 * 2777 + 5554 = 19716.7
5.1 * 2778 + 5556 = 19723.8
5.1 * 2779 + 5558 = 19730.9
5.1 * 2780 + 5560 = 19738
5.1 * 2781 + 5562 = 19745.1
5.1 * 2782 + 5564 = 19752.2
5.1 * 2783 + 5566 = 19759.3
5.1 * 2784 + 5568 = 19766.4
5.1 * 2785 + 5570 = 19773.5
5.1 * 2786 + 5572 = 19780.6
5.1 * 2787 + 5574 = 19787.7
5.1 * 2788 + 5576 = 19794.8
5.1 * 2789 + 5578 = 19801.9
5.1 * 2790 + 5580 = 19809
5.1 * 2791 + 5582 = 19816.1
5.1 * 2792 + 5584 = 19823.2
5.1 * 2793 + 5586 = 19830.3
5.1 * 2794 + 5588 = 19837.4
5.1 * 2795 + 5590 = 19844.5
5.1 * 2796 + 5592 = 19851.6
5.1 * 2797 + 5594 = 19858.7
5.1 * 2798 + 5596 = 19865.8
5.1 * 2799 + 5598 = 19872.9
5.1 * 2800 + 5600 = 19880
5.1 * 2801 + 5602 = 19887.1
5.1 * 2802 + 5604 = 19894.2
5.1 * 2803 + 5606 = 19901.3
5.1 * 2804 + 5608 = 19908.4
5.1 * 2805 + 5610 = 19915.5
5.1 * 2806 + 5612 = 19922.6
5.1 * 2807 + 5614 = 19929.7
5.1 * 2808 + 5616 = 19936.8
5.1 * 2809 + 5618 = 19943.9
5.1 * 2810 + 5620 = 19951
5.1 * 2811 + 5622 = 19958.1
5.1 * 2812 + 5624 = 19965.2
5.1 * 2813 + 5626 = 19972.3
5.1 * 2814 + 5628 = 19979.4
5.1 * 2815 + 5630 = 19986.5
5.1 * 2816 + 5632 = 19993.6
5.1 * 2817 + 5634 = 20000.7
5.1 * 2818 + 5636 = 20007.8
5.1 * 2819 + 5638 = 20014.9
5.1 * 2820 + 5640 = 20022
5.1 * 2821 + 5642 = 20029.1
5.1 * 2822 + 5644 = 20036.2
5.1 * 2823 + 5646 = 20043.3
5.1 * 2824 + 5648 = 20050.4
5.1 * 2825 + 5650 = 20057.5
5.1 * 2826 + 5652 = 20064.6
5.1 * 2827 + 5654 = 20071.7
5.1 * 2828 + 5656 = 20078.8
5.1 * 2829 + 5658 = 20085.9
5.1 * 2830 + 5660 = 20093
5.1 * 2831 + 5662 = 20100.1
5.1 * 2832 + 5664 = 20107.2
5.1 * 2833 + 5666 = 20114.3
5.1 * 2834 + 5668 = 20121.4
5.1 * 2835 + 5670 = 20128.5
5.1 * 2836 + 5672 = 20135.6
5.1 * 2837 + 5674 = 20142.7
5.1 * 2838 + 5676 = 20149.8
5.1 * 2839 + 5678 = 20156.9
5.1 * 2840 + 5680 = 20164
5.1 * 2841 + 5682 = 20171.1
5.1 * 2842 + 5684 = 20178.2
5.1 * 2843 + 5686 = 20185.3
5.1 * 2844 + 5688 = 20192.4
5.1 * 2845 + 5690 = 20199.5
5.1 * 2846 + 5692 = 20206.6
5.1 * 2847 + 5694 = 20213.7
5.1 * 2848 + 5696 = 20220.8
5.1 * 2849 + 5698 = 20227.9
5.1 * 2850 + 5700 = 20235
5.1 * 2851 + 5702 = 20242.1
5.1 * 2852 + 5704 = 20249.2
5.1 * 2853 + 5706 = 20256.3
5.1 * 2854 + 5708 = 20263.4
5.1 * 2855 + 5710 = 20270.5
5.1 * 2856 + 5712 = 20277.6
5.1 * 2857 + 5714 = 20284.7
5.1 * 2858 + 5716 = 20291.8
5.1 * 2859 + 5718 = 20298.9
5.1 * 2860 + 5720 = 20306
5.1 * 2861 + 5722 = 20313.1
5.1 * 2862 + 5724 = 20320.2
5.1 * 2863 + 5726 = 20327.3
5.1 * 2864 + 5728 = 20334.4
5.1 * 2865 + 5730 = 20341.5
5.1 * 2866 + 5732 = 20348.6
5.1 * 2867 + 5734 = 20355.7
5.1 * 2868 + 5736 = 20362.8
5.1 * 2869 + 5738 = 20369.9
5.1 * 2870 + 5740 = 20377
5.1 * 2871 + 5742 = 20384.1
5.1 * 2872 + 5744 = 20391.2
5.1 * 2873 + 5746 = 20398.3
5.1 * 2874 + 5748 = 20405.4
5.1 * 2875 + 5750 = 20412.5
5.1 * 2876 + 5752 = 20419.6
5.1 * 2877 + 5754 = 20426.7
5.1 * 2878 + 5756 = 20433.8
5.1 * 2879 + 5758 = 20440.9
5.1 * 2880 + 5760 = 20448
5.1 * 2881 + 5762 = 20455.1
5.1 * 2882 + 5764 = 20462.2
5.1 * 2883 + 5766 = 20469.3
5.1 * 2884 + 5768 = 20476.4
5.1 * 2885 + 5770 = 20483.5
5.1 * 2886 + 5772 = 20490.6
5.1 * 2887 + 5774 = 20497.7
5.1 * 2888 + 5776 = 20504.8
5.1 * 2889 + 5778 = 20511.9
5.1 * 2890 + 5780 = 20519
5.1 * 2891 + 5782 = 20526.1
5.1 * 2892 + 5784 = 20533.2
5.1 * 2893 + 5786 = 20540.3
5.1 * 2894 + 5788 = 20547.4
5.1 * 2895 + 5790 = 20554.5
5.1 * 2896 + 5792 = 20561.6
5.1 * 2897 + 5794 = 20568.7
5.1 * 2898 + 5796 = 20575.8
5.1 * 2899 + 5798 = 20582.9
5.1 * 2900 + 5800 = 20590
5.1 * 2901 + 5802 = 20597.1
5.1 * 2902 + 5804 = 20604.2
5.1 * 2903 + 5806 = 20611.3
5.1 * 2904 + 5808 = 20618.4
5.1 * 2905 + 5810 = 20625.5
5.1 * 2906 + 5812 = 20632.6
5.1 * 2907 + 5814 = 20639.7
5.1 * 2908 + 5816 = 20646.8
5.1 * 2909 + 5818 = 20653.9
5.1 * 2910 + 5820 = 20661
5.1 * 2911 + 5822 = 20668.1
5.1 * 2912 + 5824 = 20675.2
5.1 * 2913 + 5826 = 20682.3
5.1 * 2914 + 5828 = 20689.4
5.1 * 2915 + 5830 = 20696.5
5.1 * 2916 + 5832 = 20703.6
5.1 * 2917 + 5834 = 20710.7
5.1 * 2918 + 5836 = 20717.8
5.1 * 2919 + 5838 = 20724.9
5.1 * 2920 + 5840 = 20732
5.1 * 2921 + 5842 = 20739.1
5.1 * 2922 + 5844 = 20746.2
5.1 * 2923 + 5846 = 20753.3
5.1 * 2924 + 5848 = 20760.4
5.1 * 2925 + 5850 = 20767.5
5.1 * 2926 + 5852 = 20774.6
5.1 * 2927 + 5854 = 20781.7
5.1 * 2928 + 5856 = 20788.8
5.1 * 2929 + 5858 = 20795.9
5.1 * 2930 + 5860 = 20803
5.1 * 2931 + 5862 = 20810.1
5.1 * 2932 + 5864 = 20817.2
5.1 * 2933 + 5866 = 20824.3
5.1 * 2934 + 5868 = 20831.4
5.1 * 2935 + 5870 = 20838.5
5.1 * 2936 + 5872 = 20845.6
5.1 * 2937 + 5874 = 20852.7
5.1 * 2938 + 5876 = 20859.8
5.1 * 2939 + 5878 = 20866.9
5.1 * 2940 + 5880 = 20874
5.1 * 2941 + 5882 = 20881.1
5.1 * 2942 + 5884 = 20888.2
5.1 * 2943 + 5886 = 20895.3
5.1 * 2944 + 5888 = 20902.4
5.1 * 2945 + 5890 = 20909.5
5.1 * 2946 + 5892 = 20916.6
5.1 * 2947 + 5894 = 20923.7
5.1 * 2948 + 5896 = 20930.8
5.1 * 2949 + 5898 = 20937.9
5.1 * 2950 + 5900 = 20945
5.1 * 2951 + 5902 = 20952.1
5.1 * 2952 + 5904 = 20959.2
5.1 * 2953 + 5906 = 20966.3
5.1 * 2954 + 5908 = 20973.4
5.1 * 2955 + 5910 = 20980.5
5.1 * 2956 + 5912 = 20987.6
5.1 * 2957 + 5914 = 20994.7
5.1 * 2958 + 5916 = 21001.8
5.1 * 2959 + 5918 = 21008.9
5.1 * 2960 + 5920 = 21016
5.1 * 2961 + 5922 = 21023.1
5.1 * 2962 + 5924 = 21030.2
5.1 * 2963 + 5926 = 21037.3
5.1 * 2964 + 5928 = 21044.4
5.1 * 2965 + 5930 = 21051.5
5.1 * 2966 + 5932 = 21058.6
5.1 * 2967 + 5934 = 21065.7
5.1 * 2968 + 5936 = 21072.8
5.1 * 2969 + 5938 = 21079.9
5.1 * 2970 + 5940 = 21087
5.1 * 2971 + 5942 = 21094.1
5.1 * 2972 + 5944 = 21101.2
5.1 * 2973 + 5946 = 21108.3
5.1 * 2974 + 5948 = 21115.4
5.1 * 2975 + 5950 = 21122.5
5.1 * 2976 + 5952 = 21129.6
5.1 * 2977 + 5954 = 21136.7
5.1 * 2978 + 5956 = 21143.8
5.1 * 2979 + 5958 = 21150.9
5.1 * 2980 + 5960 = 21158
5.1 * 2981 + 5962 = 21165.1
5.1 * 2982 + 5964 = 21172.2
5.1 * 2983 + 5966 = 21179.3
5.1 * 2984 + 5968 = 21186.4
5.1 * 2985 + 5970 = 21193.5
5.1 * 2986 + 5972 = 21200.6
5.1 * 2987 + 5974 = 21207.7
5.1 * 2988 + 5976 = 21214.8
5.1 * 2989 + 5978 = 21221.9
5.1 * 2990 + 5980 = 21229
5.1 * 2991 + 5982 = 21236.1
5.1 * 2992 + 5984 = 21243.2
5.1 * 2993 + 5986 = 21250.3
5.1 * 2994 + 5988 = 21257.4
5.1 * 2995 + 5990 = 21264.5
5.1 * 2996 + 5992 = 21271.6
5.1 * 2997 + 5994 = 21278.7
5.1 * 2998 + 5996 = 21285.8
5.1 * 2999 + 5998 = 21292.9
5.1 * 3000 + 6000 = 21300
5.1 * 3001 + 6002 = 21307.1
5.1 * 3002 + 6004 = 21314.2
5.1 * 3003 + 6006 = 21321.3
5.1 * 3004 + 6008 = 21328.4
5.1 * 3005 + 6010 = 21335.5
5.1 * 3006 + 6012 = 21342.6
5.1 * 3007 + 6014 = 21349.7
5.1 * 3008 + 6016 = 21356.8
5.1 * 3009 + 6018 = 21363.9
5.1 * 3010 + 6020 = 21371
5.1 * 3011 + 6022 = 21378.1
5.1 * 3012 + 6024 = 21385.2
5.1 * 3013 + 6026 = 21392.3
5.1 * 3014 + 6028 = 21399.4
5.1 * 3015 + 6030 = 21406.5
5.1 * 3016 + 6032 = 21413.6
5.1 * 3017 + 6034 = 21420.7
5.1 * 3018 + 6036 = 21427.8
5.1 * 3019 + 6038 = 21434.9
5.1 * 3020 + 6040 = 21442
5.1 * 3021 + 6042 = 21449.1
5.1 * 3022 + 6044 = 21456.2
5.1 * 3023 + 6046 = 21463.3
5.1 * 3024 + 6048 = 21470.4
5.1 * 3025 + 6050 = 21477.5
5.1 * 3026 + 6052 = 21484.6
5.1 * 3027 + 6054 = 21491.7
5.1 * 3028 + 6056 = 21498.8
5.1 * 3029 + 6058 = 21505.9
5.1 * 3030 + 6060 = 21513
5.1 * 3031 + 6062 = 21520.1
5.1 * 3032 + 6064 = 21527.2
5.1 * 3033 + 6066 = 21534.3
5.1 * 3034 + 6068 = 21541.4
5.1 * 3035 + 6070 = 21548.5
5.1 * 3036 + 6072 = 21555.6
5.1 * 3037 + 6074 = 21562.7
5.1 * 3038 + 6076 = 21569.8
5.1 * 3039 + 6078 = 21576.9
5.1 * 3040 + 6080 = 21584
5.1 * 3041 + 6082 = 21591.1
5.1 * 3042 + 6084 = 21598.2
5.1 * 3043 + 6086 = 21605.3
5.1 * 3044 + 6088 = 21612.4
5.1 * 3045 + 6090 = 21619.5
5.1 * 3046 + 6092 = 21626.6
5.1 * 3047 + 6094 = 21633.7
5.1 * 3048 + 6096 = 21640.8
5.1 * 3049 + 6098 = 21647.9
5.1 * 3050 + 6100 = 21655
5.1 * 3051 + 6102 = 21662.1
5.1 * 3052 + 6104 = 21669.2
5.1 * 3053 + 6106 = 21676.3
5.1 * 3054 + 6108 = 21683.4
5.1 * 3055 + 6110 = 21690.5
5.1 * 3056 + 6112 = 21697.6
5.1 * 3057 + 6114 = 21704.7
5.1 * 3058 + 6116 = 21711.8
5.1 * 3059 + 6118 = 21718.9
5.1 * 3060 + 6120 = 21726
5.1 * 3061 + 6122 = 21733.1
5.1 * 3062 + 6124 = 21740.2
5.1 * 3063 + 6126 = 21747.3
5.1 * 3064 + 6128 = 21754.4
5.1 * 3065 + 6130 = 21761.5
5.1 * 3066 + 6132 = 21768.6
5.1 * 3067 + 6134 = 21775.7
5.1 * 3068 + 6136 = 21782.8
5.1 * 3069 + 6138 = 21789.9
5.1 * 3070 + 6140 = 21797
5.1 * 3071 + 6142 = 21804.1
5.1 * 3072 + 6144 = 21811.2
5.1 * 3073 + 6146 = 21818.3
5.1 * 3074 + 6148 = 21825.4
5.1 * 3075 + 6150 = 21832.5
5.1 * 3076 + 6152 = 21839.6
5.1 * 3077 + 6154 = 21846.7
5.1 * 3078 + 6156 = 21853.8
5.1 * 3079 + 6158 = 21860.9
5.1 * 3080 + 6160 = 21868
5.1 * 3081 + 6162 = 21875.1
5.1 * 3082 + 6164 = 21882.2
5.1 * 3083 + 6166 = 21889.3
5.1 * 3084 + 6168 = 21896.4
5.1 * 3085 + 6170 = 21903.5
5.1 * 3086 + 6172 = 21910.6
5.1 * 3087 + 6174 = 21917.7
5.1 * 3088 + 6176 = 21924.8
5.1 * 3089 + 6178 = 21931.9
5.1 * 3090 + 6180 = 21939
5.1 * 3091 + 6182 = 21946.1
5.1 * 3092 + 6184 = 21953.2
5.1 * 3093 + 6186 = 21960.3
5.1 * 3094 + 6188 = 21967.4
5.1 * 3095 + 6190 = 21974.5
5.1 * 3096 + 6192 = 21981.6
5.1 * 3097 + 6194 = 21988.7
5.1 * 3098 + 6196 = 21995.8
5.1 * 3099 + 6198 = 22002.9
5.1 * 3100 + 6200 = 22010
5.1 * 3101 + 6202 = 22017.1
5.1 * 3102 + 6204 = 22024.2
5.1 * 3103 + 6206 = 22031.3
5.1 * 3104 + 6208 = 22038.4
5.1 * 3105 + 6210 = 22045.5
5.1 * 3106 + 6212 = 22052.6
5.1 * 3107 + 6214 = 22059.7
5.1 * 3108 + 6216 = 22066.8
5.1 * 3109 + 6218 = 22073.9
5.1 * 3110 + 6220 = 22081
5.1 * 3111 + 6222 = 22088.1
5.1 * 3112 + 6224 = 22095.2
5.1 * 3113 + 6226 = 22102.3
5.1 * 3114 + 6228 = 22109.4
5.1 * 3115 + 6230 = 22116.5
5.1 * 3116 + 6232 = 22123.6
5.1 * 3117 + 6234 = 22130.7
5.1 * 3118 + 6236 = 22137.8
5.1 * 3119 + 6238 = 22144.9
5.1 * 3120 + 6240 = 22152
5.1 * 3121 + 6242 = 22159.1
5.1 * 3122 + 6244 = 22166.2
5.1 * 3123 + 6246 = 22173.3
5.1 * 3124 + 6248 = 22180.4
5.1 * 3125 + 6250 = 22187.5
5.1 * 3126 + 6252 = 22194.6
5.1 * 3127 + 6254 = 22201.7
5.1 * 3128 + 6256 = 22208.8
5.1 * 3129 + 6258 = 22215.9
5.1 * 3130 + 6260 = 22223
5.1 * 3131 + 6262 = 22230.1
5.1 * 3132 + 6264 = 22237.2
5.1 * 3133 + 6266 = 22244.3
5.1 * 3134 + 6268 = 22251.4
5.1 * 3135 + 6270 = 22258.5
5.1 * 3136 + 6272 = 22265.6
5.1 * 3137 + 6274 = 22272.7
5.1 * 3138 + 6276 = 22279.8
5.1 * 3139 + 6278 = 22286.9
5.1 * 3140 + 6280 = 22294
5.1 * 3141 + 6282 = 22301.1
5.1 * 3142 + 6284 = 22308.2
5.1 * 3143 + 6286 = 22315.3
5.1 * 3144 + 6288 = 22322.4
5.1 * 3145 + 6290 = 22329.5
5.1 * 3146 + 6292 = 22336.6
5.1 * 3147 + 6294 = 22343.7
5.1 * 3148 + 6296 = 22350.8
5.1 * 3149 + 6298 = 22357.9
5.1 * 3150 + 6300 = 22365
5.1 * 3151 + 6302 = 22372.1
5.1 * 3152 + 6304 = 22379.2
5.1 * 3153 + 6306 = 22386.3
5.1 * 3154 + 6308 = 22393.4
5.1 * 3155 + 6310 = 22400.5
5.1 * 3156 + 6312 = 22407.6
5.1 * 3157 + 6314 = 22414.7
5.1 * 3158 + 6316 = 22421.8
5.1 * 3159 + 6318 = 22428.9
5.1 * 3160 + 6320 = 22436
5.1 * 3161 + 6322 = 22443.1
5.1 * 3162 + 6324 = 22450.2
5.1 * 3163 + 6326 = 22457.3
5.1 * 3164 + 6328 = 22464.4
5.1 * 3165 + 6330 = 22471.5
5.1 * 3166 + 6332 = 22478.6
5.1 * 3167 + 6334 = 22485.7
5.1 * 3168 + 6336 = 22492.8
5.1 * 3169 + 6338 = 22499.9
5.1 * 3170 + 6340 = 22507
5.1 * 3171 + 6342 = 22514.1
5.1 * 3172 + 6344 = 22521.2
5.1 * 3173 + 6346 = 22528.3
5.1 * 3174 + 6348 = 22535.4
5.1 * 3175 + 6350 = 22542.5
5.1 * 3176 + 6352 = 22549.6
5.1 * 3177 + 6354 = 22556.7
5.1 * 3178 + 6356 = 22563.8
5.1 * 3179 + 6358 = 22570.9
5.1 * 3180 + 6360 = 22578
5.1 * 3181 + 6362 = 22585.1
5.1 * 3182 + 6364 = 22592.2
5.1 * 3183 + 6366 = 22599.3
5.1 * 3184 + 6368 = 22606.4
5.1 * 3185 + 6370 = 22613.5
5.1 * 3186 + 6372 = 22620.6
5.1 * 3187 + 6374 = 22627.7
5.1 * 3188 + 6376 = 22634.8
5.1 * 3189 + 6378 = 22641.9
5.1 * 3190 + 6380 = 22649
5.1 * 3191 + 6382 = 22656.1
5.1 * 3192 + 6384 = 22663.2
5.1 * 3193 + 6386 = 22670.3
5.1 * 3194 + 6388 = 22677.4
5.1 * 3195 + 6390 = 22684.5
5.1 * 3196 + 6392 = 22691.6
5.1 * 3197 + 6394 = 22698.7
5.1 * 3198 + 6396 = 22705.8
5.1 * 3199 + 6398 = 22712.9
5.1 * 3200 + 6400 = 22720
5.1 * 3201 + 6402 = 22727.1
5.1 * 3202 + 6404 = 22734.2
5.1 * 3203 + 6406 = 22741.3
5.1 * 3204 + 6408 = 22748.4
5.1 * 3205 + 6410 = 22755.5
5.1 * 3206 + 6412 = 22762.6
5.1 * 3207 + 6414 = 22769.7
5.1 * 3208 + 6416 = 22776.8
5.1 * 3209 + 6418 = 22783.9
5.1 * 3210 + 6420 = 22791
5.1 * 3211 + 6422 = 22798.1
5.1 * 3212 + 6424 = 22805.2
5.1 * 3213 + 6426 = 22812.3
5.1 * 3214 + 6428 = 22819.4
5.1 * 3215 + 6430 = 22826.5
5.1 * 3216 + 6432 = 22833.6
5.1 * 3217 + 6434 = 22840.7
5.1 * 3218 + 6436 = 22847.8
5.1 * 3219 + 6438 = 22854.9
5.1 * 3220 + 6440 = 22862
5.1 * 3221 + 6442 = 22869.1
5.1 * 3222 + 6444 = 22876.2
5.1 * 3223 + 6446 = 22883.3
5.1 * 3224 + 6448 = 22890.4
5.1 * 3225 + 6450 = 22897.5
5.1 * 3226 + 6452 = 22904.6
5.1 * 3227 + 6454 = 22911.7
5.1 * 3228 + 6456 = 22918.8
5.1 * 3229 + 6458 = 22925.9
5.1 * 3230 + 6460 = 22933
5.1 * 3231 + 6462 = 22940.1
5.1 * 3232 + 6464 = 22947.2
5.1 * 3233 + 6466 = 22954.3
5.1 * 3234 + 6468 = 22961.4
5.1 * 3235 + 6470 = 22968.5
5.1 * 3236 + 6472 = 22975.6
5.1 * 3237 + 6474 = 22982.7
5.1 * 3238 + 6476 = 22989.8
5.1 * 3239 + 6478 = 22996.9
5.1 * 3240 + 6480 = 23004
5.1 * 3241 + 6482 = 23011.1
5.1 * 3242 + 6484 = 23018.2
5.1 * 3243 + 6486 = 23025.3
5.1 * 3244 + 6488 = 23032.4
5.1 * 3245 + 6490 = 23039.5
5.1 * 3246 + 6492 = 23046.6
5.1 * 3247 + 6494 = 23053.7
5.1 * 3248 + 6496 = 23060.8
5.1 * 3249 + 6498 = 23067.9
5.1 * 3250 + 6500 = 23075
5.1 * 3251 + 6502 = 23082.1
5.1 * 3252 + 6504 = 23089.2
5.1 * 3253 + 6506 = 23096.3
5.1 * 3254 + 6508 = 23103.4
5.1 * 3255 + 6510 = 23110.5
5.1 * 3256 + 6512 = 23117.6
5.1 * 3257 + 6514 = 23124.7
5.1 * 3258 + 6516 = 23131.8
5.1 * 3259 + 6518 = 23138.9
5.1 * 3260 + 6520 = 23146
5.1 * 3261 + 6522 = 23153.1
5.1 * 3262 + 6524 = 23160.2
5.1 * 3263 + 6526 = 23167.3
5.1 * 3264 + 6528 = 23174.4
5.1 * 3265 + 6530 = 23181.5
5.1 * 3266 + 6532 = 23188.6
5.1 * 3267 + 6534 = 23195.7
5.1 * 3268 + 6536 = 23202.8
5.1 * 3269 + 6538 = 23209.9
5.1 * 3270 + 6540 = 23217
5.1 * 3271 + 6542 = 23224.1
5.1 * 3272 + 6544 = 23231.2
5.1 * 3273 + 6546 = 23238.3
5.1 * 3274 + 6548 = 23245.4
5.1 * 3275 + 6550 = 23252.5
5.1 * 3276 + 6552 = 23259.6
5.1 * 3277 + 6554 = 23266.7
5.1 * 3278 + 6556 = 23273.8
5.1 * 3279 + 6558 = 23280.9
5.1 * 3280 + 6560 = 23288
5.1 * 3281 + 6562 = 23295.1
5.1 * 3282 + 6564 = 23302.2
5.1 * 3283 + 6566 = 23309.3
5.1 * 3284 + 6568 = 23316.4
5.1 * 3285 + 6570 = 23323.5
5.1 * 3286 + 6572 = 23330.6
5.1 * 3287 + 6574 = 23337.7
5.1 * 3288 + 6576 = 23344.8
5.1 * 3289 + 6578 = 23351.9
5.1 * 3290 + 6580 = 23359
5.1 * 3291 + 6582 = 23366.1
5.1 * 3292 + 6584 = 23373.2
5.1 * 3293 + 6586 = 23380.3
5.1 * 3294 + 6588 = 23387.4
5.1 * 3295 + 6590 = 23394.5
5.1 * 3296 + 6592 = 23401.6
5.1 * 3297 + 6594 = 23408.7
5.1 * 3298 + 6596 = 23415.8
5.1 * 3299 + 6598 = 23422.9
5.1 * 3300 + 6600 = 23430
5.1 * 3301 + 6602 = 23437.1
5.1 * 3302 + 6604 = 23444.2
5.1 * 3303 + 6606 = 23451.3
5.1 * 3304 + 6608 = 23458.4
5.1 * 3305 + 6610 = 23465.5
5.1 * 3306 + 6612 = 23472.6
5.1 * 3307 + 6614 = 23479.7
5.1 * 3308 + 6616 = 23486.8
5.1 * 3309 + 6618 = 23493.9
5.1 * 3310 + 6620 = 23501
5.1 * 3311 + 6622 = 23508.1
5.1 * 3312 + 6624 = 23515.2
5.1 * 3313 + 6626 = 23522.3
5.1 * 3314 + 6628 = 23529.4
5.1 * 3315 + 6630 = 23536.5
5.1 * 3316 + 6632 = 23543.6
5.1 * 3317 + 6634 = 23550.7
5.1 * 3318 + 6636 = 23557.8
5.1 * 3319 + 6638 = 23564.9
5.1 * 3320 + 6640 = 23572
5.1 * 3321 + 6642 = 23579.1
5.1 * 3322 + 6644 = 23586.2
5.1 * 3323 + 6646 = 23593.3
5.1 * 3324 + 6648 = 23600.4
5.1 * 3325 + 6650 = 23607.5
5.1 * 3326 + 6652 = 23614.6
5.1 * 3327 + 6654 = 23621.7
5.1 * 3328 + 6656 = 23628.8
5.1 * 3329 + 6658 = 23635.9
5.1 * 3330 + 6660 = 23643
5.1 * 3331 + 6662 = 23650.1
5.1 * 3332 + 6664 = 23657.2
5.1 * 3333 + 6666 = 23664.3
5.1 * 3334 + 6668 = 23671.4
5.1 * 3335 + 6670 = 23678.5
5.1 * 3336 + 6672 = 23685.6
5.1 * 3337 + 6674 = 23692.7
5.1 * 3338 + 6676 = 23699.8
5.1 * 3339 + 6678 = 23706.9
5.1 * 3340 + 6680 = 23714
5.1 * 3341 + 6682 = 23721.1
5.1 * 3342 + 6684 = 23728.2
5.1 * 3343 + 6686 = 23735.3
5.1 * 3344 + 6688 = 23742.4
5.1 * 3345 + 6690 = 23749.5
5.1 * 3346 + 6692 = 23756.6
5.1 * 3347 + 6694 = 23763.7
5.1 * 3348 + 6696 = 23770.8
5.1 * 3349 + 6698 = 23777.9
5.1 * 3350 + 6700 = 23785
5.1 * 3351 + 6702 = 23792.1
5.1 * 3352 + 6704 = 23799.2
5.1 * 3353 + 6706 = 23806.3
5.1 * 3354 + 6708 = 23813.4
5.1 * 3355 + 6710 = 23820.5
5.1 * 3356 + 6712 = 23827.6
5.1 * 3357 + 6714 = 23834.7
5.1 * 3358 + 6716 = 23841.8
5.1 * 3359 + 6718 = 23848.9
5.1 * 3360 + 6720 = 23856
5.1 * 3361 + 6722 = 23863.1
5.1 * 3362 + 6724 = 23870.2
5.1 * 3363 + 6726 = 23877.3
5.1 * 3364 + 6728 = 23884.4
5.1 * 3365 + 6730 = 23891.5
5.1 * 3366 + 6732 = 23898.6
5.1 * 3367 + 6734 = 23905.7
5.1 * 3368 + 6736 = 23912.8
5.1 * 3369 + 6738 = 23919.9
5.1 * 3370 + 6740 = 23927
5.1 * 3371 + 6742 = 23934.1
5.1 * 3372 + 6744 = 23941.2
5.1 * 3373 + 6746 = 23948.3
5.1 * 3374 + 6748 = 23955.4
5.1 * 3375 + 6750 = 23962.5
5.1 * 3376 + 6752 = 23969.6
5.1 * 3377 + 6754 = 23976.7
5.1 * 3378 + 6756 = 23983.8
5.1 * 3379 + 6758 = 23990.9
5.1 * 3380 + 6760 = 23998
5.1 * 3381 + 6762 = 24005.1
5.1 * 3382 + 6764 = 24012.2
5.1 * 3383 + 6766 = 24019.3
5.1 * 3384 + 6768 = 24026.4
5.1 * 3385 + 6770 = 24033.5
5.1 * 3386 + 6772 = 24040.6
5.1 * 3387 + 6774 = 24047.7
5.1 * 3388 + 6776 = 24054.8
5.1 * 3389 + 6778 = 24061.9
5.1 * 3390 + 6780 = 24069
5.1 * 3391 + 6782 = 24076.1
5.1 * 3392 + 6784 = 24083.2
5.1 * 3393 + 6786 = 24090.3
5.1 * 3394 + 6788 = 24097.4
5.1 * 3395 + 6790 = 24104.5
5.1 * 3396 + 6792 = 24111.6
5.1 * 3397 + 6794 = 24118.7
5.1 * 3398 + 6796 = 24125.8
5.1 * 3399 + 6798 = 24132.9
5.1 * 3400 + 6800 = 24140
5.1 * 3401 + 6802 = 24147.1
5.1 * 3402 + 6804 = 24154.2
5.1 * 3403 + 6806 = 24161.3
5.1 * 3404 + 6808 = 24168.4
5.1 * 3405 + 6810 = 24175.5
5.1 * 3406 + 6812 = 24182.6
5.1 * 3407 + 6814 = 24189.7
5.1 * 3408 + 6816 = 24196.8
5.1 * 3409 + 6818 = 24203.9
5.1 * 3410 + 6820 = 24211
5.1 * 3411 + 6822 = 24218.1
5.1 * 3412 + 6824 = 24225.2
5.1 * 3413 + 6826 = 24232.3
5.1 * 3414 + 6828 = 24239.4
5.1 * 3415 + 6830 = 24246.5
5.1 * 3416 + 6832 = 24253.6
5.1 * 3417 + 6834 = 24260.7
5.1 * 3418 + 6836 = 24267.8
5.1 * 3419 + 6838 = 24274.9
5.1 * 3420 + 6840 = 24282
5.1 * 3421 + 6842 = 24289.1
5.1 * 3422 + 6844 = 24296.2
5.1 * 3423 + 6846 = 24303.3
5.1 * 3424 + 6848 = 24310.4
5.1 * 3425 + 6850 = 24317.5
5.1 * 3426 + 6852 = 24324.6
5.1 * 3427 + 6854 = 24331.7
5.1 * 3428 + 6856 = 24338.8
5.1 * 3429 + 6858 = 24345.9
5.1 * 3430 + 6860 = 24353
5.1 * 3431 + 6862 = 24360.1
5.1 * 3432 + 6864 = 24367.2
5.1 * 3433 + 6866 = 24374.3
5.1 * 3434 + 6868 = 24381.4
5.1 * 3435 + 6870 = 24388.5
5.1 * 3436 + 6872 = 24395.6
5.1 * 3437 + 6874 = 24402.7
5.1 * 3438 + 6876 = 24409.8
5.1 * 3439 + 6878 = 24416.9
5.1 * 3440 + 6880 = 24424
5.1 * 3441 + 6882 = 24431.1
5.1 * 3442 + 6884 = 24438.2
5.1 * 3443 + 6886 = 24445.3
5.1 * 3444 + 6888 = 24452.4
5.1 * 3445 + 6890 = 24459.5
5.1 * 3446 + 6892 = 24466.6
5.1 * 3447 + 6894 = 24473.7
5.1 * 3448 + 6896 = 24480.8
5.1 * 3449 + 6898 = 24487.9
5.1 * 3450 + 6900 = 24495
5.1 * 3451 + 6902 = 24502.1
5.1 * 3452 + 6904 = 24509.2
5.1 * 3453 + 6906 = 24516.3
5.1 * 3454 + 6908 = 24523.4
5.1 * 3455 + 6910 = 24530.5
5.1 * 3456 + 6912 = 24537.6
5.1 * 3457 + 6914 = 24544.7
5.1 * 3458 + 6916 = 24551.8
5.1 * 3459 + 6918 = 24558.9
5.1 * 3460 + 6920 = 24566
5.1 * 3461 + 6922 = 24573.1
5.1 * 3462 + 6924 = 24580.2
5.1 * 3463 + 6926 = 24587.3
5.1 * 3464 + 6928 = 24594.4
5.1 * 3465 + 6930 = 24601.5
5.1 * 3466 + 6932 = 24608.6
5.1 * 3467 + 6934 = 24615.7
5.1 * 3468 + 6936 = 24622.8
5.1 * 3469 + 6938 = 24629.9
5.1 * 3470 + 6940 = 24637
5.1 * 3471 + 6942 = 24644.1
5.1 * 3472 + 6944 = 24651.2
5.1 * 3473 + 6946 = 24658.3
5.1 * 3474 + 6948 = 24665.4
5.1 * 3475 + 6950 = 24672.5
5.1 * 3476 + 6952 = 24679.6
5.1 * 3477 + 6954 = 24686.7
5.1 * 3478 + 6956 = 24693.8
5.1 * 3479 + 6958 = 24700.9
5.1 * 3480 + 6960 = 24708
5.1 * 3481 + 6962 = 24715.1
5.1 * 3482 + 6964 = 24722.2
5.1 * 3483 + 6966 = 24729.3
5.1 * 3484 + 6968 = 24736.4
5.1 * 3485 + 6970 = 24743.5
5.1 * 3486 + 6972 = 24750.6
5.1 * 3487 + 6974 = 24757.7
5.1 * 3488 + 6976 = 24764.8
5.1 * 3489 + 6978 = 24771.9
5.1 * 3490 + 6980 = 24779
5.1 * 3491 + 6982 = 24786.1
5.1 * 3492 + 6984 = 24793.2
5.1 * 3493 + 6986 = 24800.3
5.1 * 3494 + 6988 = 24807.4
5.1 * 3495 + 6990 = 24814.5
5.1 * 3496 + 6992 = 24821.6
5.1 * 3497 + 6994 = 24828.7
5.1 * 3498 + 6996 = 24835.8
5.1 * 3499 + 6998 = 24842.9
5.1 * 3500 + 7000 = 24850
5.1 * 3501 + 7002 = 24857.1
5.1 * 3502 + 7004 = 24864.2
5.1 * 3503 + 7006 = 24871.3
5.1 * 3504 + 7008 = 24878.4
5.1 * 3505 + 7010 = 24885.5
5.1 * 3506 + 7012 = 24892.6
5.1 * 3507 + 7014 = 24899.7
5.1 * 3508 + 7016 = 24906.8
5.1 * 3509 + 7018 = 24913.9
5.1 * 3510 + 7020 = 24921
5.1 * 3511 + 7022 = 24928.1
5.1 * 3512 + 7024 = 24935.2
5.1 * 3513 + 7026 = 24942.3
5.1 * 3514 + 7028 = 24949.4
5.1 * 3515 + 7030 = 24956.5
5.1 * 3516 + 7032 = 24963.6
5.1 * 3517 + 7034 = 24970.7
5.1 * 3518 + 7036 = 24977.8
5.1 * 3519 + 7038 = 24984.9
5.1 * 3520 + 7040 = 24992
5.1 * 3521 + 7042 = 24999.1
5.1 * 3522 + 7044 = 25006.2
5.1 * 3523 + 7046 = 25013.3
5.1 * 3524 + 7048 = 25020.4
5.1 * 3525 + 7050 = 25027.5
5.1 * 3526 + 7052 = 25034.6
5.1 * 3527 + 7054 = 25041.7
5.1 * 3528 + 7056 = 25048.8
5.1 * 3529 + 7058 = 25055.9
5.1 * 3530 + 7060 = 25063
5.1 * 3531 + 7062 = 25070.1
5.1 * 3532 + 7064 = 25077.2
5.1 * 3533 + 7066 = 25084.3
5.1 * 3534 + 7068 = 25091.4
5.1 * 3535 + 7070 = 25098.5
5.1 * 3536 + 7072 = 25105.6
5.1 * 3537 + 7074 = 25112.7
5.1 * 3538 + 7076 = 25119.8
5.1 * 3539 + 7078 = 25126.9
5.1 * 3540 + 7080 = 25134
5.1 * 3541 + 7082 = 25141.1
5.1 * 3542 + 7084 = 25148.2
5.1 * 3543 + 7086 = 25155.3
5.1 * 3544 + 7088 = 25162.4
5.1 * 3545 + 7090 = 25169.5
5.1 * 3546 + 7092 = 25176.6
5.1 * 3547 + 7094 = 25183.7
5.1 * 3548 + 7096 = 25190.8
5.1 * 3549 + 7098 = 25197.9
5.1 * 3550 + 7100 = 25205
5.1 * 3551 + 7102 = 25212.1
5.1 * 3552 + 7104 = 25219.2
5.1 * 3553 + 7106 = 25226.3
5.1 * 3554 + 7108 = 25233.4
5.1 * 3555 + 7110 = 25240.5
5.1 * 3556 + 7112 = 25247.6
5.1 * 3557 + 7114 = 25254.7
5.1 * 3558 + 7116 = 25261.8
5.1 * 3559 + 7118 = 25268.9
5.1 * 3560 + 7120 = 25276
5.1 * 3561 + 7122 = 25283.1
5.1 * 3562 + 7124 = 25290.2
5.1 * 3563 + 7126 = 25297.3
5.1 * 3564 + 7128 = 25304.4
5.1 * 3565 + 7130 = 25311.5
5.1 * 3566 + 7132 = 25318.6
5.1 * 3567 + 7134 = 25325.7
5.1 * 3568 + 7136 = 25332.8
5.1 * 3569 + 7138 = 25339.9
5.1 * 3570 + 7140 = 25347
5.1 * 3571 + 7142 = 25354.1
5.1 * 3572 + 7144 = 25361.2
5.1 * 3573 + 7146 = 25368.3
5.1 * 3574 + 7148 = 25375.4
5.1 * 3575 + 7150 = 25382.5
5.1 * 3576 + 7152 = 25389.6
5.1 * 3577 + 7154 = 25396.7
5.1 * 3578 + 7156 = 25403.8
5.1 * 3579 + 7158 = 25410.9
5.1 * 3580 + 7160 = 25418
5.1 * 3581 + 7162 = 25425.1
5.1 * 3582 + 7164 = 25432.2
5.1 * 3583 + 7166 = 25439.3
5.1 * 3584 + 7168 = 25446.4
5.1 * 3585 + 7170 = 25453.5
5.1 * 3586 + 7172 = 25460.6
5.1 * 3587 + 7174 = 25467.7
5.1 * 3588 + 7176 = 25474.8
5.1 * 3589 + 7178 = 25481.9
5.1 * 3590 + 7180 = 25489
5.1 * 3591 + 7182 = 25496.1
5.1 * 3592 + 7184 = 25503.2
5.1 * 3593 + 7186 = 25510.3
5.1 * 3594 + 7188 = 25517.4
5.1 * 3595 + 7190 = 25524.5
5.1 * 3596 + 7192 = 25531.6
5.1 * 3597 + 7194 = 25538.7
5.1 * 3598 + 7196 = 25545.8
5.1 * 3599 + 7198 = 25552.9
5.1 * 3600 + 7200 = 25560
5.1 * 3601 + 7202 = 25567.1
5.1 * 3602 + 7204 = 25574.2
5.1 * 3603 + 7206 = 25581.3
5.1 * 3604 + 7208 = 25588.4
5.1 * 3605 + 7210 = 25595.5
5.1 * 3606 + 7212 = 25602.6
5.1 * 3607 + 7214 = 25609.7
5.1 * 3608 + 7216 = 25616.8
5.1 * 3609 + 7218 = 25623.9
5.1 * 3610 + 7220 = 25631
5.1 * 3611 + 7222 = 25638.1
5.1 * 3612 + 7224 = 25645.2
5.1 * 3613 + 7226 = 25652.3
5.1 * 3614 + 7228 = 25659.4
5.1 * 3615 + 7230 = 25666.5
5.1 * 3616 + 7232 = 25673.6
5.1 * 3617 + 7234 = 25680.7
5.1 * 3618 + 7236 = 25687.8
5.1 * 3619 + 7238 = 25694.9
5.1 * 3620 + 7240 = 25702
5.1 * 3621 + 7242 = 25709.1
5.1 * 3622 + 7244 = 25716.2
5.1 * 3623 + 7246 = 25723.3
5.1 * 3624 + 7248 = 25730.4
5.1 * 3625 + 7250 = 25737.5
5.1 * 3626 + 7252 = 25744.6
5.1 * 3627 + 7254 = 25751.7
5.1 * 3628 + 7256 = 25758.8
5.1 * 3629 + 7258 = 25765.9
5.1 * 3630 + 7260 = 25773
5.1 * 3631 + 7262 = 25780.1
5.1 * 3632 + 7264 = 25787.2
5.1 * 3633 + 7266 = 25794.3
5.1 * 3634 + 7268 = 25801.4
5.1 * 3635 + 7270 = 25808.5
5.1 * 3636 + 7272 = 25815.6
5.1 * 3637 + 7274 = 25822.7
5.1 * 3638 + 7276 = 25829.8
5.1 * 3639 + 7278 = 25836.9
5.1 * 3640 + 7280 = 25844
5.1 * 3641 + 7282 = 25851.1
5.1 * 3642 + 7284 = 25858.2
5.1 * 3643 + 7286 = 25865.3
5.1 * 3644 + 7288 = 25872.4
5.1 * 3645 + 7290 = 25879.5
5.1 * 3646 + 7292 = 25886.6
5.1 * 3647 + 7294 = 25893.7
5.1 * 3648 + 7296 = 25900.8
5.1 * 3649 + 7298 = 25907.9
5.1 * 3650 + 7300 = 25915
5.1 * 3651 + 7302 = 25922.1
5.1 * 3652 + 7304 = 25929.2
5.1 * 3653 + 7306 = 25936.3
5.1 * 3654 + 7308 = 25943.4
5.1 * 3655 + 7310 = 25950.5
5.1 * 3656 + 7312 = 25957.6
5.1 * 3657 + 7314 = 25964.7
5.1 * 3658 + 7316 = 25971.8
5.1 * 3659 + 7318 = 25978.9
5.1 * 3660 + 7320 = 25986
5.1 * 3661 + 7322 = 25993.1
5.1 * 3662 + 7324 = 26000.2
5.1 * 3663 + 7326 = 26007.3
5.1 * 3664 + 7328 = 26014.4
5.1 * 3665 + 7330 = 26021.5
5.1 * 3666 + 7332 = 26028.6
5.1 * 3667 + 7334 = 26035.7
5.1 * 3668 + 7336 = 26042.8
5.1 * 3669 + 7338 = 26049.9
5.1 * 3670 + 7340 = 26057
5.1 * 3671 + 7342 = 26064.1
5.1 * 3672 + 7344 = 26071.2
5.1 * 3673 + 7346 = 26078.3
5.1 * 3674 + 7348 = 26085.4
5.1 * 3675 + 7350 = 26092.5
5.1 * 3676 + 7352 = 26099.6
5.1 * 3677 + 7354 = 26106.7
5.1 * 3678 + 7356 = 26113.8
5.1 * 3679 + 7358 = 26120.9
5.1 * 3680 + 7360 = 26128
5.1 * 3681 + 7362 = 26135.1
5.1 * 3682 + 7364 = 26142.2
5.1 * 3683 + 7366 = 26149.3
5.1 * 3684 + 7368 = 26156.4
5.1 * 3685 + 7370 = 26163.5
5.1 * 3686 + 7372 = 26170.6
5.1 * 3687 + 7374 = 26177.7
5.1 * 3688 + 7376 = 26184.8
5.1 * 3689 + 7378 = 26191.9
5.1 * 3690 + 7380 = 26199
5.1 * 3691 + 7382 = 26206.1
5.1 * 3692 + 7384 = 26213.2
5.1 * 3693 + 7386 = 26220.3
5.1 * 3694 + 7388 = 26227.4
5.1 * 3695 + 7390 = 26234.5
5.1 * 3696 + 7392 = 26241.6
5.1 * 3697 + 7394 = 26248.7
5.1 * 3698 + 7396 = 26255.8
5.1 * 3699 + 7398 = 26262.9
5.1 * 3700 + 7400 = 26270
5.1 * 3701 + 7402 = 26277.1
5.1 * 3702 + 7404 = 26284.2
5.1 * 3703 + 7406 = 26291.3
5.1 * 3704 + 7408 = 26298.4
5.1 * 3705 + 7410 = 26305.5
5.1 * 3706 + 7412 = 26312.6
5.1 * 3707 + 7414 = 26319.7
5.1 * 3708 + 7416 = 26326.8
5.1 * 3709 + 7418 = 26333.9
5.1 * 3710 + 7420 = 26341
5.1 * 3711 + 7422 = 26348.1
5.1 * 3712 + 7424 = 26355.2
5.1 * 3713 + 7426 = 26362.3
5.1 * 3714 + 7428 = 26369.4
5.1 * 3715 + 7430 = 26376.5
5.1 * 3716 + 7432 = 26383.6
5.1 * 3717 + 7434 = 26390.7
5.1 * 3718 + 7436 = 26397.8
5.1 * 3719 + 7438 = 26404.9
5.1 * 3720 + 7440 = 26412
5.1 * 3721 + 7442 = 26419.1
5.1 * 3722 + 7444 = 26426.2
5.1 * 3723 + 7446 = 26433.3
5.1 * 3724 + 7448 = 26440.4
5.1 * 3725 + 7450 = 26447.5
5.1 * 3726 + 7452 = 26454.6
5.1 * 3727 + 7454 = 26461.7
5.1 * 3728 + 7456 = 26468.8
5.1 * 3729 + 7458 = 26475.9
5.1 * 3730 + 7460 = 26483
5.1 * 3731 + 7462 = 26490.1
5.1 * 3732 + 7464 = 26497.2
5.1 * 3733 + 7466 = 26504.3
5.1 * 3734 + 7468 = 26511.4
5.1 * 3735 + 7470 = 26518.5
5.1 * 3736 + 7472 = 26525.6
5.1 * 3737 + 7474 = 26532.7
5.1 * 3738 + 7476 = 26539.8
5.1 * 3739 + 7478 = 26546.9
5.1 * 3740 + 7480 = 26554
5.1 * 3741 + 7482 = 26561.1
5.1 * 3742 + 7484 = 26568.2
5.1 * 3743 + 7486 = 26575.3
5.1 * 3744 + 7488 = 26582.4
5.1 * 3745 + 7490 = 26589.5
5.1 * 3746 + 7492 = 26596.6
5.1 * 3747 + 7494 = 26603.7
5.1 * 3748 + 7496 = 26610.8
5.1 * 3749 + 7498 = 26617.9
5.1 * 3750 + 7500 = 26625
5.1 * 3751 + 7502 = 26632.1
5.1 * 3752 + 7504 = 26639.2
5.1 * 3753 + 7506 = 26646.3
5.1 * 3754 + 7508 = 26653.4
5.1 * 3755 + 7510 = 26660.5
5.1 * 3756 + 7512 = 26667.6
5.1 * 3757 + 7514 = 26674.7
5.1 * 3758 + 7516 = 26681.8
5.1 * 3759 + 7518 = 26688.9
5.1 * 3760 + 7520 = 26696
5.1 * 3761 + 7522 = 26703.1
5.1 * 3762 + 7524 = 26710.2
5.1 * 3763 + 7526 = 26717.3
5.1 * 3764 + 7528 = 26724.4
5.1 * 3765 + 7530 = 26731.5
5.1 * 3766 + 7532 = 26738.6
5.1 * 3767 + 7534 = 26745.7
5.1 * 3768 + 7536 = 26752.8
5.1 * 3769 + 7538 = 26759.9
5.1 * 3770 + 7540 = 26767
5.1 * 3771 + 7542 = 26774.1
5.1 * 3772 + 7544 = 26781.2
5.1 * 3773 + 7546 = 26788.3
5.1 * 3774 + 7548 = 26795.4
5.1 * 3775 + 7550 = 26802.5
5.1 * 3776 + 7552 = 26809.6
5.1 * 3777 + 7554 = 26816.7
5.1 * 3778 + 7556 = 26823.8
5.1 * 3779 + 7558 = 26830.9
5.1 * 3780 + 7560 = 26838
5.1 * 3781 + 7562 = 26845.1
5.1 * 3782 + 7564 = 26852.2
5.1 * 3783 + 7566 = 26859.3
5.1 * 3784 + 7568 = 26866.4
5.1 * 3785 + 7570 = 26873.5
5.1 * 3786 + 7572 = 26880.6
5.1 * 3787 + 7574 = 26887.7
5.1 * 3788 + 7576 = 26894.8
5.1 * 3789 + 7578 = 26901.9
5.1 * 3790 + 7580 = 26909
5.1 * 3791 + 7582 = 26916.1
5.1 * 3792 + 7584 = 26923.2
5.1 * 3793 + 7586 = 26930.3
5.1 * 3794 + 7588 = 26937.4
5.1 * 3795 + 7590 = 26944.5
5.1 * 3796 + 7592 = 26951.6
5.1 * 3797 + 7594 = 26958.7
5.1 * 3798 + 7596 = 26965.8
5.1 * 3799 + 7598 = 26972.9
5.1 * 3800 + 7600 = 26980
5.1 * 3801 + 7602 = 26987.1
5.1 * 3802 + 7604 = 26994.2
5.1 * 3803 + 7606 = 27001.3
5.1 * 3804 + 7608 = 27008.4
5.1 * 3805 + 7610 = 27015.5
5.1 * 3806 + 7612 = 27022.6
5.1 * 3807 + 7614 = 27029.7
5.1 * 3808 + 7616 = 27036.8
5.1 * 3809 + 7618 = 27043.9
5.1 * 3810 + 7620 = 27051
5.1 * 3811 + 7622 = 27058.1
5.1 * 3812 + 7624 = 27065.2
5.1 * 3813 + 7626 = 27072.3
5.1 * 3814 + 7628 = 27079.4
5.1 * 3815 + 7630 = 27086.5
5.1 * 3816 + 7632 = 27093.6
5.1 * 3817 + 7634 = 27100.7
5.1 * 3818 + 7636 = 27107.8
5.1 * 3819 + 7638 = 27114.9
5.1 * 3820 + 7640 = 27122
5.1 * 3821 + 7642 = 27129.1
5.1 * 3822 + 7644 = 27136.2
5.1 * 3823 + 7646 = 27143.3
5.1 * 3824 + 7648 = 27150.4
5.1 * 3825 + 7650 = 27157.5
5.1 * 3826 + 7652 = 27164.6
5.1 * 3827 + 7654 = 27171.7
5.1 * 3828 + 7656 = 27178.8
5.1 * 3829 + 7658 = 27185.9
5.1 * 3830 + 7660 = 27193
5.1 * 3831 + 7662 = 27200.1
5.1 * 3832 + 7664 = 27207.2
5.1 * 3833 + 7666 = 27214.3
5.1 * 3834 + 7668 = 27221.4
5.1 * 3835 + 7670 = 27228.5
5.1 * 3836 + 7672 = 27235.6
5.1 * 3837 + 7674 = 27242.7
5.1 * 3838 + 7676 = 27249.8
5.1 * 3839 + 7678 = 27256.9
5.1 * 3840 + 7680 = 27264
5.1 * 3841 + 7682 = 27271.1
5.1 * 3842 + 7684 = 27278.2
5.1 * 3843 + 7686 = 27285.3
5.1 * 3844 + 7688 = 27292.4
5.1 * 3845 + 7690 = 27299.5
5.1 * 3846 + 7692 = 27306.6
5.1 * 3847 + 7694 = 27313.7
5.1 * 3848 + 7696 = 27320.8
5.1 * 3849 + 7698 = 27327.9
5.1 * 3850 + 7700 = 27335
5.1 * 3851 + 7702 = 27342.1
5.1 * 3852 + 7704 = 27349.2
5.1 * 3853 + 7706 = 27356.3
5.1 * 3854 + 7708 = 27363.4
5.1 * 3855 + 7710 = 27370.5
5.1 * 3856 + 7712 = 27377.6
5.1 * 3857 + 7714 = 27384.7
5.1 * 3858 + 7716 = 27391.8
5.1 * 3859 + 7718 = 27398.9
5.1 * 3860 + 7720 = 27406
5.1 * 3861 + 7722 = 27413.1
5.1 * 3862 + 7724 = 27420.2
5.1 * 3863 + 7726 = 27427.3
5.1 * 3864 + 7728 = 27434.4
5.1 * 3865 + 7730 = 27441.5
5.1 * 3866 + 7732 = 27448.6
5.1 * 3867 + 7734 = 27455.7
5.1 * 3868 + 7736 = 27462.8
5.1 * 3869 + 7738 = 27469.9
5.1 * 3870 + 7740 = 27477
5.1 * 3871 + 7742 = 27484.1
5.1 * 3872 + 7744 = 27491.2
5.1 * 3873 + 7746 = 27498.3
5.1 * 3874 + 7748 = 27505.4
5.1 * 3875 + 7750 = 27512.5
5.1 * 3876 + 7752 = 27519.6
5.1 * 3877 + 7754 = 27526.7
5.1 * 3878 + 7756 = 27533.8
5.1 * 3879 + 7758 = 27540.9
5.1 * 3880 + 7760 = 27548
5.1 * 3881 + 7762 = 27555.1
5.1 * 3882 + 7764 = 27562.2
5.1 * 3883 + 7766 = 27569.3
5.1 * 3884 + 7768 = 27576.4
5.1 * 3885 + 7770 = 27583.5
5.1 * 3886 + 7772 = 27590.6
5.1 * 3887 + 7774 = 27597.7
5.1 * 3888 + 7776 = 27604.8
5.1 * 3889 + 7778 = 27611.9
5.1 * 3890 + 7780 = 27619
5.1 * 3891 + 7782 = 27626.1
5.1 * 3892 + 7784 = 27633.2
5.1 * 3893 + 7786 = 27640.3
5.1 * 3894 + 7788 = 27647.4
5.1 * 3895 + 7790 = 27654.5
5.1 * 3896 + 7792 = 27661.6
5.1 * 3897 + 7794 = 27668.7
5.1 * 3898 + 7796 = 27675.8
5.1 * 3899 + 7798 = 27682.9
5.1 * 3900 + 7800 = 27690
5.1 * 3901 + 7802 = 27697.1
5.1 * 3902 + 7804 = 27704.2
5.1 * 3903 + 7806 = 27711.3
5.1 * 3904 + 7808 = 27718.4
5.1 * 3905 + 7810 = 27725.5
5.1 * 3906 + 7812 = 27732.6
5.1 * 3907 + 7814 = 27739.7
5.1 * 3908 + 7816 = 27746.8
5.1 * 3909 + 7818 = 27753.9
5.1 * 3910 + 7820 = 27761
5.1 * 3911 + 7822 = 27768.1
5.1 * 3912 + 7824 = 27775.2
5.1 * 3913 + 7826 = 27782.3
5.1 * 3914 + 7828 = 27789.4
5.1 * 3915 + 7830 = 27796.5
5.1 * 3916 + 7832 = 27803.6
5.1 * 3917 + 7834 = 27810.7
5.1 * 3918 + 7836 = 27817.8
5.1 * 3919 + 7838 = 27824.9
5.1 * 3920 + 7840 = 27832
5.1 * 3921 + 7842 = 27839.1
5.1 * 3922 + 7844 = 27846.2
5.1 * 3923 + 7846 = 27853.3
5.1 * 3924 + 7848 = 27860.4
5.1 * 3925 + 7850 = 27867.5
5.1 * 3926 + 7852 = 27874.6
5.1 * 3927 + 7854 = 27881.7
5.1 * 3928 + 7856 = 27888.8
5.1 * 3929 + 7858 = 27895.9
5.1 * 3930 + 7860 = 27903
5.1 * 3931 + 7862 = 27910.1
5.1 * 3932 + 7864 = 27917.2
5.1 * 3933 + 7866 = 27924.3
5.1 * 3934 + 7868 = 27931.4
5.1 * 3935 + 7870 = 27938.5
5.1 * 3936 + 7872 = 27945.6
5.1 * 3937 + 7874 = 27952.7
5.1 * 3938 + 7876 = 27959.8
5.1 * 3939 + 7878 = 27966.9
5.1 * 3940 + 7880 = 27974
5.1 * 3941 + 7882 = 27981.1
5.1 * 3942 + 7884 = 27988.2
5.1 * 3943 + 7886 = 27995.3
5.1 * 3944 + 7888 = 28002.4
5.1 * 3945 + 7890 = 28009.5
5.1 * 3946 + 7892 = 28016.6
5.1 * 3947 + 7894 = 28023.7
5.1 * 3948 + 7896 = 28030.8
5.1 * 3949 + 7898 = 28037.9
5.1 * 3950 + 7900 = 28045
5.1 * 3951 + 7902 = 28052.1
5.1 * 3952 + 7904 = 28059.2
5.1 * 3953 + 7906 = 28066.3
5.1 * 3954 + 7908 = 28073.4
5.1 * 3955 + 7910 = 28080.5
5.1 * 3956 + 7912 = 28087.6
5.1 * 3957 + 7914 = 28094.7
5.1 * 3958 + 7916 = 28101.8
5.1 * 3959 + 7918 = 28108.9
5.1 * 3960 + 7920 = 28116
5.1 * 3961 + 7922 = 28123.1
5.1 * 3962 + 7924 = 28130.2
5.1 * 3963 + 7926 = 28137.3
5.1 * 3964 + 7928 = 28144.4
5.1 * 3965 + 7930 = 28151.5
5.1 * 3966 + 7932 = 28158.6
5.1 * 3967 + 7934 = 28165.7
5.1 * 3968 + 7936 = 28172.8
5.1 * 3969 + 7938 = 28179.9
5.1 * 3970 + 7940 = 28187
5.1 * 3971 + 7942 = 28194.1
5.1 * 3972 + 7944 = 28201.2
5.1 * 3973 + 7946 = 28208.3
5.1 * 3974 + 7948 = 28215.4
5.1 * 3975 + 7950 = 28222.5
5.1 * 3976 + 7952 = 28229.6
5.1 * 3977 + 7954 = 28236.7
5.1 * 3978 + 7956 = 28243.8
5.1 * 3979 + 7958 = 28250.9
5.1 * 3980 + 7960 = 28258
5.1 * 3981 + 7962 = 28265.1
5.1 * 3982 + 7964 = 28272.2
5.1 * 3983 + 7966 = 28279.3
5.1 * 3984 + 7968 = 28286.4
5.1 * 3985 + 7970 = 28293.5
5.1 * 3986 + 7972 = 28300.6
5.1 * 3987 + 7974 = 28307.7
5.1 * 3988 + 7976 = 28314.8
5.1 * 3989 + 7978 = 28321.9
5.1 * 3990 + 7980 = 28329
5.1 * 3991 + 7982 = 28336.1
5.1 * 3992 + 7984 = 28343.2
5.1 * 3993 + 7986 = 28350.3
5.1 * 3994 + 7988 = 28357.4
5.1 * 3995 + 7990 = 28364.5
5.1 * 3996 + 7992 = 28371.6
5.1 * 3997 + 7994 = 28378.7
5.1 * 3998 + 7996 = 28385.8
5.1 * 3999 + 7998 = 28392.9
5.1 * 4000 + 8000 = 28400
5.1 * 4001 + 8002 = 28407.1
5.1 * 4002 + 8004 = 28414.2
5.1 * 4003 + 8006 = 28421.3
5.1 * 4004 + 8008 = 28428.4
5.1 * 4005 + 8010 = 28435.5
5.1 * 4006 + 8012 = 28442.6
5.1 * 4007 + 8014 = 28449.7
5.1 * 4008 + 8016 = 28456.8
5.1 * 4009 + 8018 = 28463.9
5.1 * 4010 + 8020 = 28471
5.1 * 4011 + 8022 = 28478.1
5.1 * 4012 + 8024 = 28485.2
5.1 * 4013 + 8026 = 28492.3
5.1 * 4014 + 8028 = 28499.4
5.1 * 4015 + 8030 = 28506.5
5.1 * 4016 + 8032 = 28513.6
5.1 * 4017 + 8034 = 28520.7
5.1 * 4018 + 8036 = 28527.8
5.1 * 4019 + 8038 = 28534.9
5.1 * 4020 + 8040 = 28542
5.1 * 4021 + 8042 = 28549.1
5.1 * 4022 + 8044 = 28556.2
5.1 * 4023 + 8046 = 28563.3
5.1 * 4024 + 8048 = 28570.4
5.1 * 4025 + 8050 = 28577.5
5.1 * 4026 + 8052 = 28584.6
5.1 * 4027 + 8054 = 28591.7
5.1 * 4028 + 8056 = 28598.8
5.1 * 4029 + 8058 = 28605.9
5.1 * 4030 + 8060 = 28613
5.1 * 4031 + 8062 = 28620.1
5.1 * 4032 + 8064 = 28627.2
5.1 * 4033 + 8066 = 28634.3
5.1 * 4034 + 8068 = 28641.4
5.1 * 4035 + 8070 = 28648.5
5.1 * 4036 + 8072 = 28655.6
5.1 * 4037 + 8074 = 28662.7
5.1 * 4038 + 8076 = 28669.8
5.1 * 4039 + 8078 = 28676.9
5.1 * 4040 + 8080 = 28684
5.1 * 4041 + 8082 = 28691.1
5.1 * 4042 + 8084 = 28698.2
5.1 * 4043 + 8086 = 28705.3
5.1 * 4044 + 8088 = 28712.4
5.1 * 4045 + 8090 = 28719.5
5.1 * 4046 + 8092 = 28726.6
5.1 * 4047 + 8094 = 28733.7
5.1 * 4048 + 8096 = 28740.8
5.1 * 4049 + 8098 = 28747.9
5.1 * 4050 + 8100 = 28755
5.1 * 4051 + 8102 = 28762.1
5.1 * 4052 + 8104 = 28769.2
5.1 * 4053 + 8106 = 28776.3
5.1 * 4054 + 8108 = 28783.4
5.1 * 4055 + 8110 = 28790.5
5.1 * 4056 + 8112 = 28797.6
5.1 * 4057 + 8114 = 28804.7
5.1 * 4058 + 8116 = 28811.8
5.1 * 4059 + 8118 = 28818.9
5.1 * 4060 + 8120 = 28826
5.1 * 4061 + 8122 = 28833.1
5.1 * 4062 + 8124 = 28840.2
5.1 * 4063 + 8126 = 28847.3
5.1 * 4064 + 8128 = 28854.4
5.1 * 4065 + 8130 = 28861.5
5.1 * 4066 + 8132 = 28868.6
5.1 * 4067 + 8134 = 28875.7
5.1 * 4068 + 8136 = 28882.8
5.1 * 4069 + 8138 = 28889.9
5.1 * 4070 + 8140 = 28897
5.1 * 4071 + 8142 = 28904.1
5.1 * 4072 + 8144 = 28911.2
5.1 * 4073 + 8146 = 28918.3
5.1 * 4074 + 8148 = 28925.4
5.1 * 4075 + 8150 = 28932.5
5.1 * 4076 + 8152 = 28939.6
5.1 * 4077 + 8154 = 28946.7
5.1 * 4078 + 8156 = 28953.8
5.1 * 4079 + 8158 = 28960.9
5.1 * 4080 + 8160 = 28968
5.1 * 4081 + 8162 = 28975.1
5.1 * 4082 + 8164 = 28982.2
5.1 * 4083 + 8166 = 28989.3
5.1 * 4084 + 8168 = 28996.4
5.1 * 4085 + 8170 = 29003.5
5.1 * 4086 + 8172 = 29010.6
5.1 * 4087 + 8174 = 29017.7
5.1 * 4088 + 8176 = 29024.8
5.1 * 4089 + 8178 = 29031.9
5.1 * 4090 + 8180 = 29039
5.1 * 4091 + 8182 = 29046.1
5.1 * 4092 + 8184 = 29053.2
5.1 * 4093 + 8186 = 29060.3
5.1 * 4094 + 8188 = 29067.4
5.1 * 4095 + 8190 = 29074.5
PASSED!
****************************************************************
################################################################
4_CUDA_Libraries/lineOfSight
################################################################
>>> GCC Version is greater or equal to 5.0.0 <<<
make: Nothing to be done for 'all'.
[./lineOfSight] - Starting...
GPU Device 0: "Ampere" with compute capability 8.6

Line of sight
Average time: 0.057710 ms

Test passed
****************************************************************
################################################################
4_CUDA_Libraries/matrixMulCUBLAS
################################################################
make: Nothing to be done for 'all'.
[Matrix Multiply CUBLAS] - Starting...
GPU Device 0: "Ampere" with compute capability 8.6

GPU Device 0: "NVIDIA GeForce RTX 3060" with compute capability 8.6

MatrixA(640,480), MatrixB(480,320), MatrixC(640,320)
Computing result using CUBLAS...done.
Performance= 3904.92 GFlop/s, Time= 0.050 msec, Size= 196608000 Ops
Computing result using host CPU...done.
Comparing CUBLAS Matrix Multiply with CPU results: PASS

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
****************************************************************
################################################################
4_CUDA_Libraries/nvJPEG
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Using GPU 0 (NVIDIA GeForce RTX 3060, 28 SMs, 1536 th/SM max, CC 8.6, ECC off)
Decoding images in directory: ./images/, total 8, batchsize 1
Processing: ./images/img1.jpg
Image is 3 channels.
Channel #0 size: 480 x 640
Channel #1 size: 240 x 320
Channel #2 size: 240 x 320
YUV 4:2:0 chroma subsampling
Processing: ./images/img3.jpg
Image is 3 channels.
Channel #0 size: 640 x 426
Channel #1 size: 320 x 213
Channel #2 size: 320 x 213
YUV 4:2:0 chroma subsampling
Processing: ./images/img8.jpg
Image is 3 channels.
Channel #0 size: 480 x 640
Channel #1 size: 240 x 320
Channel #2 size: 240 x 320
YUV 4:2:0 chroma subsampling
Processing: ./images/img6.jpg
Image is 3 channels.
Channel #0 size: 640 x 480
Channel #1 size: 320 x 240
Channel #2 size: 320 x 240
YUV 4:2:0 chroma subsampling
Processing: ./images/img7.jpg
Image is 3 channels.
Channel #0 size: 480 x 640
Channel #1 size: 240 x 320
Channel #2 size: 240 x 320
YUV 4:2:0 chroma subsampling
Processing: ./images/img4.jpg
Image is 3 channels.
Channel #0 size: 640 x 426
Channel #1 size: 320 x 213
Channel #2 size: 320 x 213
YUV 4:2:0 chroma subsampling
Processing: ./images/img2.jpg
Image is 3 channels.
Channel #0 size: 480 x 640
Channel #1 size: 240 x 320
Channel #2 size: 240 x 320
YUV 4:2:0 chroma subsampling
Processing: ./images/img5.jpg
Image is 3 channels.
Channel #0 size: 640 x 480
Channel #1 size: 320 x 240
Channel #2 size: 320 x 240
YUV 4:2:0 chroma subsampling
Total decoding time: 4.91069
Avg decoding time per image: 0.613836
Avg images per sec: 1.6291
Avg decoding time per batch: 0.613836
****************************************************************
################################################################
4_CUDA_Libraries/nvJPEG_encoder
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Using GPU 0 (NVIDIA GeForce RTX 3060, 28 SMs, 1536 th/SM max, CC 8.6, ECC off)
Processing file: ./images/img1.jpg
Image is 3 channels.
Channel #0 size: 480 x 640
Channel #1 size: 240 x 320
Channel #2 size: 240 x 320
YUV 4:2:0 chroma subsampling
Writing JPEG file: encode_output/img1.jpg
Processing file: ./images/img3.jpg
Image is 3 channels.
Channel #0 size: 640 x 426
Channel #1 size: 320 x 213
Channel #2 size: 320 x 213
YUV 4:2:0 chroma subsampling
Writing JPEG file: encode_output/img3.jpg
Processing file: ./images/img8.jpg
Image is 3 channels.
Channel #0 size: 480 x 640
Channel #1 size: 240 x 320
Channel #2 size: 240 x 320
YUV 4:2:0 chroma subsampling
Writing JPEG file: encode_output/img8.jpg
Processing file: ./images/img6.jpg
Image is 3 channels.
Channel #0 size: 640 x 480
Channel #1 size: 320 x 240
Channel #2 size: 320 x 240
YUV 4:2:0 chroma subsampling
Writing JPEG file: encode_output/img6.jpg
Processing file: ./images/img7.jpg
Image is 3 channels.
Channel #0 size: 480 x 640
Channel #1 size: 240 x 320
Channel #2 size: 240 x 320
YUV 4:2:0 chroma subsampling
Writing JPEG file: encode_output/img7.jpg
Processing file: ./images/img4.jpg
Image is 3 channels.
Channel #0 size: 640 x 426
Channel #1 size: 320 x 213
Channel #2 size: 320 x 213
YUV 4:2:0 chroma subsampling
Writing JPEG file: encode_output/img4.jpg
Processing file: ./images/img2.jpg
Image is 3 channels.
Channel #0 size: 480 x 640
Channel #1 size: 240 x 320
Channel #2 size: 240 x 320
YUV 4:2:0 chroma subsampling
Writing JPEG file: encode_output/img2.jpg
Processing file: ./images/img5.jpg
Image is 3 channels.
Channel #0 size: 640 x 480
Channel #1 size: 320 x 240
Channel #2 size: 320 x 240
YUV 4:2:0 chroma subsampling
Writing JPEG file: encode_output/img5.jpg
Total images processed: 8
Total time spent on encoding: 4.42307
Avg time/image: 0.552884
****************************************************************
################################################################
4_CUDA_Libraries/oceanFFT
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at oceanFFT.cpp:304 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer(&cuda_heightVB_resource, heightVertexBuffer, cudaGraphicsMapFlagsWriteDiscard)" 
NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

[CUDA FFT Ocean Simulation]

Left mouse button          - rotate
Middle mouse button        - pan
Right mouse button         - zoom
'w' key                    - toggle wireframe
[CUDA FFT Ocean Simulation] 
GPU Device 0: "Ampere" with compute capability 8.6

****************************************************************
################################################################
4_CUDA_Libraries/randomFog
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
Random Fog
==========

GPU Device 0: "Ampere" with compute capability 8.6

CURAND initialized

Random number visualization

On creation, randomFog generates 200,000 random coordinates in spherical coordinate space (radius, angle rho, angle theta) with curand's XORWOW algorithm. The coordinates are normalized for a uniform distribution through the sphere.

The X axis is drawn with blue in the negative direction and yellow positive.
The Y axis is drawn with green in the negative direction and magenta positive.
The Z axis is drawn with red in the negative direction and cyan positive.

The following keys can be used to control the output:

	s         Generate a new set of random numbers and display as spherical coordinates (Sphere)
	e         Generate a new set of random numbers and display on a spherical surface (shEll)
	b         Generate a new set of random numbers and display as cartesian coordinates (cuBe/Box)
	p         Generate a new set of random numbers and display on a cartesian plane (Plane)

	i,l,j     Rotate the negative Z-axis up, right, down and left respectively
	a         Toggle auto-rotation
	t         Toggle 10x zoom
	z         Toggle axes display

	x         Select XORWOW generator (default)
	c         Select Sobol' generator
	v         Select scrambled Sobol' generator
	r         Reset XORWOW (i.e. reset to initial seed) and regenerate
	]         Increment the number of Sobol' dimensions and regenerate
	[         Reset the number of Sobol' dimensions to 1 and regenerate

	+         Increment the number of displayed points by 8,000 (up to maximum 200,000)
	-         Decrement the number of displayed points by 8,000 (down to minimum 8,000)

	q/[ESC]   Quit the application.


****************************************************************
################################################################
4_CUDA_Libraries/simpleCUBLAS
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

simpleCUBLAS test running..
simpleCUBLAS test passed.
****************************************************************
################################################################
4_CUDA_Libraries/simpleCUBLASXT
################################################################
make: Nothing to be done for 'all'.
Using 1 GPUs
GPU ID = 0, Name = NVIDIA GeForce RTX 3060 
simpleCUBLASXT test running..
simpleCUBLASXT test passed.
****************************************************************
################################################################
4_CUDA_Libraries/simpleCUBLAS_LU
################################################################
make: Nothing to be done for 'all'.
> initializing..
GPU Device 0: "Ampere" with compute capability 8.6

> using DOUBLE precision..
> pivot ENABLED..
> generating random matrices..
> copying data from host memory to GPU memory..
> performing LU decomposition..
> copying data from GPU memory to host memory..
> verifying the result..
> TEST SUCCESSFUL, with precision: 1e-15
****************************************************************
################################################################
4_CUDA_Libraries/simpleCUFFT
################################################################
make: Nothing to be done for 'all'.
[simpleCUFFT] is starting...
GPU Device 0: "Ampere" with compute capability 8.6

Temporary buffer size 8 bytes
Transforming signal cufftExecC2C
Launching ComplexPointwiseMulAndScale<<< >>>
Transforming signal back cufftExecC2C
****************************************************************
################################################################
4_CUDA_Libraries/simpleCUFFT_2d_MGPU
################################################################
make: Nothing to be done for 'all'.

Poisson equation using CUFFT library on Multiple GPUs is starting...

No. of GPU on node 1
Two GPUs are required to run simpleCUFFT_2d_MGPU sample code
****************************************************************
################################################################
4_CUDA_Libraries/simpleCUFFT_MGPU
################################################################
make: Nothing to be done for 'all'.

[simpleCUFFT_MGPU] is starting...

No. of GPU on node 1
Two GPUs are required to run simpleCUFFT_MGPU sample code
****************************************************************
################################################################
4_CUDA_Libraries/simpleCUFFT_callback
################################################################
>>> GCC Version is greater or equal to  5.3.0 <<<
make: Nothing to be done for 'all'.
[simpleCUFFT_callback] is starting...
GPU Device 0: "Ampere" with compute capability 8.6

Transforming signal cufftExecC2C
Transforming signal back cufftExecC2C
****************************************************************
################################################################
4_CUDA_Libraries/watershedSegmentationNPP
################################################################
make: Nothing to be done for 'all'.
NPP Library Version 12.3.1
CUDA Driver  Version: 12.6
CUDA Runtime Version: 12.6

Input file load succeeded.
teapot_Segments_8Way_512x512_8u succeeded.
teapot_CompressedSegmentLabels_8Way_512x512_32u succeeded.
teapot_SegmentBoundaries_8Way_512x512_8u succeeded.
teapot_SegmentsWithContrastingBoundaries_8Way_512x512_8u succeeded.
Input file load succeeded.
CT_Skull_Segments_8Way_512x512_8u succeeded.
CT_Skull_CompressedSegmentLabels_8Way_512x512_32u succeeded.
CT_Skull_SegmentBoundaries_8Way_512x512_8u succeeded.
CT_Skull_SegmentsWithContrastingBoundaries_8Way_512x512_8u succeeded.
Input file load succeeded.
Rocks_Segments_8Way_512x512_8u succeeded.
Rocks_CompressedSegmentLabels_8Way_512x512_32u succeeded.
Rocks_SegmentBoundaries_8Way_512x512_8u succeeded.
Rocks_SegmentsWithContrastingBoundaries_8Way_512x512_8u succeeded.
****************************************************************
################################################################
5_Domain_Specific/BlackScholes
################################################################
make: Nothing to be done for 'all'.
[./BlackScholes] - Starting...
GPU Device 0: "Ampere" with compute capability 8.6

Initializing data...
...allocating CPU memory for options.
...allocating GPU memory for options.
...generating input data in CPU mem.
...copying input data to GPU mem.
Data init done.

Executing Black-Scholes GPU kernel (512 iterations)...
Options count             : 8000000     
BlackScholesGPU() time    : 0.241480 msec
Effective memory bandwidth: 331.289732 GB/s
Gigaoptions per second    : 33.128973     

BlackScholes, Throughput = 33.1290 GOptions/s, Time = 0.00024 s, Size = 8000000 options, NumDevsUsed = 1, Workgroup = 128

Reading back GPU results...
Checking the results...
...running CPU calculations.

Comparing the results...
L1 norm: 1.741792E-07
Max absolute error: 1.192093E-05

Shutting down...
...releasing GPU memory.
...releasing CPU memory.
Shutdown done.

[BlackScholes] - Test Summary

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

Test passed
****************************************************************
################################################################
5_Domain_Specific/BlackScholes_nvrtc
################################################################
make: Nothing to be done for 'all'.
[./BlackScholes_nvrtc] - Starting...
Initializing data...
...allocating CPU memory for options.
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> GPU Device has SM 8.6 compute capability
...allocating GPU memory for options.
...generating input data in CPU mem.
...copying input data to GPU mem.
Data init done.

Executing Black-Scholes GPU kernel (512 iterations)...
Options count             : 8000000     
BlackScholesGPU() time    : 0.245461 msec
Effective memory bandwidth: 325.917432 GB/s
Gigaoptions per second    : 32.591743     

BlackScholes, Throughput = 32.5917 GOptions/s, Time = 0.00025 s, Size = 8000000 options, NumDevsUsed = 1, Workgroup = 128

Reading back GPU results...
Checking the results...
...running CPU calculations.

Comparing the results...
L1 norm: 1.741792E-07
Max absolute error: 1.192093E-05

Shutting down...
...releasing GPU memory.
...releasing CPU memory.
Shutdown done.

[./BlackScholes_nvrtc] - Test Summary
Test passed
****************************************************************
################################################################
5_Domain_Specific/FDTD3d
################################################################
make: Nothing to be done for 'all'.
./FDTD3d Starting...

Set-up, based upon target device GMEM size...
 getTargetDeviceGlobalMemSize
 cudaGetDeviceCount
GPU Device 0: "Ampere" with compute capability 8.6

 cudaGetDeviceProperties
 generateRandomData

FDTD on 376 x 376 x 376 volume with symmetric filter radius 4 for 5 timesteps...

fdtdReference...
 calloc intermediate
 Host FDTD loop
	t = 0
	t = 1
	t = 2
	t = 3
	t = 4

fdtdReference complete
fdtdGPU...
GPU Device 0: "Ampere" with compute capability 8.6

 set block size to 32x16
 set grid size to 12x24
 GPU FDTD loop
	t = 0 launch kernel
	t = 1 launch kernel
	t = 2 launch kernel
	t = 3 launch kernel
	t = 4 launch kernel

fdtdGPU complete

CompareData (tolerance 0.000100)...
****************************************************************
################################################################
5_Domain_Specific/HSOpticalFlow
################################################################
make: Nothing to be done for 'all'.
HSOpticalFlow Starting...

GPU Device 0: "Ampere" with compute capability 8.6

Loading "frame10.ppm" ...
Loading "frame11.ppm" ...
Computing optical flow on CPU...
Computing optical flow on GPU...
L1 error : 0.044308
****************************************************************
################################################################
5_Domain_Specific/Mandelbrot
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at Mandelbrot.cpp:916 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer( &cuda_pbo_resource, gl_PBO, cudaGraphicsMapFlagsWriteDiscard)" 
[CUDA Mandelbrot/Julia Set] - Starting...
GPU Device 0: "Ampere" with compute capability 8.6

Data initialization done.
Initializing GLUT...
OpenGL window created.
Creating GL texture...
Texture created.
Creating PBO...
****************************************************************
################################################################
5_Domain_Specific/MonteCarloMultiGPU
################################################################
make: Nothing to be done for 'all'.
./MonteCarloMultiGPU Starting...

Using single CPU thread for multiple GPUs
MonteCarloMultiGPU
==================
Parallelization method  = streamed
Problem scaling         = weak
Number of GPUs          = 1
Total number of options = 8192
Number of paths         = 262144
main(): generating input data...
main(): starting 1 host threads...
main(): GPU statistics, streamed
GPU Device #0: NVIDIA GeForce RTX 3060
Options         : 8192
Simulation paths: 262144

Total time (ms.): 18.851999
	Note: This is elapsed time for all to compute.
Options per sec.: 434542.770615
main(): comparing Monte Carlo and Black-Scholes results...
Shutting down...
Test Summary...
L1 norm        : 4.833895E-04
Average reserve: 13.966141

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

Test passed
****************************************************************
################################################################
5_Domain_Specific/NV12toBGRandResize
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6


TEST#1:
  CUDA resize nv12(1920x1080 --> 640x480), batch: 24, average time: 0.230 ms ==> 0.010 ms/frame
  CUDA convert nv12(640x480) to bgr(640x480), batch: 24, average time: 0.313 ms ==> 0.013 ms/frame

TEST#2:
  CUDA convert nv12(1920x1080) to bgr(1920x1080), batch: 24, average time: 2.102 ms ==> 0.088 ms/frame
  CUDA resize bgr(1920x1080 --> 640x480), batch: 24, average time: 1.789 ms ==> 0.075 ms/frame
****************************************************************
################################################################
5_Domain_Specific/SLID3D10Texture
################################################################
>>> WARNING - SLID3D10Texture is not supported on Linux - waiving sample <<<
make: Nothing to be done for 'all'.
./all_projects.sh: line 1310: ./SLID3D10Texture: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/SobelFilter
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at SobelFilter.cpp:307 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer( &cuda_pbo_resource, pbo_buffer, cudaGraphicsMapFlagsWriteDiscard)" 
CUDA Sobel Edge-Detection Starting...

GPU Device 0: "Ampere" with compute capability 8.6

Reading image: teapot.pgm
****************************************************************
################################################################
5_Domain_Specific/SobolQRNG
################################################################
make: Nothing to be done for 'all'.
Sobol Quasi-Random Number Generator Starting...

> number of vectors = 100000
> number of dimensions = 100
GPU Device 0: "Ampere" with compute capability 8.6

Allocating CPU memory...
Allocating GPU memory...
Initializing direction numbers...
Copying direction numbers to device...
Executing QRNG on GPU...
Gsamples/s: 3.5868
Reading results from GPU...

Executing QRNG on CPU...
Gsamples/s: 0.22817
Checking results...
L1-Error: 0
Shutting down...
****************************************************************
################################################################
5_Domain_Specific/VFlockingD3D10
################################################################
>>> WARNING - VFlockingD3D10 is not supported on Linux - waiving sample <<<
make: Nothing to be done for 'all'.
./all_projects.sh: line 1334: ./VFlockingD3D10: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/bicubicTexture
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at bicubicTexture.cpp:462 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer( &cuda_pbo_resource, pbo, cudaGraphicsMapFlagsWriteDiscard)" 
CUDA error at bicubicTexture_cuda.cu:88 code=4(cudaErrorCudartUnloading) "cudaDestroyTextureObject(texObjPoint)" 
Starting bicubicTexture
[CUDA BicubicTexture] (OpenGL Mode)
GPU Device 0: "Ampere" with compute capability 8.6

CUDA device [NVIDIA GeForce RTX 3060] has 28 Multi-Processors
Loaded 'teapot512.pgm', 512 x 512 pixels

	Controls
	=/- : Zoom in/out
	b   : Run Benchmark g_FilterMode
	c   : Draw Bicubic Spline Curve
	[esc] - Quit

	Press number keys to change filtering g_FilterMode:

	1 : nearest filtering
	2 : bilinear filtering
	3 : bicubic filtering
	4 : fast bicubic filtering
	5 : Catmull-Rom filtering

****************************************************************
################################################################
5_Domain_Specific/bilateralFilter
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at bilateralFilter.cpp:393 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer( &cuda_pbo_resource, pbo, cudaGraphicsMapFlagsWriteDiscard)" 
./bilateralFilter Starting...

Loading ./data/nature_monte.bmp...
BMP width: 640
BMP height: 480
BMP file loaded successfully!
Loaded './data/nature_monte.bmp', 640 x 480 pixels

GPU Device 0: "Ampere" with compute capability 8.6


****************************************************************
################################################################
5_Domain_Specific/binomialOptions
################################################################
make: Nothing to be done for 'all'.
[./binomialOptions] - Starting...
GPU Device 0: "Ampere" with compute capability 8.6

Generating input data...
Running GPU binomial tree...
Options count            : 1024     
Time steps               : 2048     
binomialOptionsGPU() time: 7.907000 msec
Options per second       : 129505.500392     
Running CPU binomial tree...
Comparing the results...
GPU binomial vs. Black-Scholes
L1 norm: 2.220214E-04
CPU binomial vs. Black-Scholes
L1 norm: 2.220922E-04
CPU binomial vs. GPU binomial
L1 norm: 7.997008E-07
Shutting down...

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

Test passed
****************************************************************
################################################################
5_Domain_Specific/binomialOptions_nvrtc
################################################################
make: Nothing to be done for 'all'.
[./binomialOptions_nvrtc] - Starting...
Generating input data...
Running GPU binomial tree...
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> GPU Device has SM 8.6 compute capability
Options count            : 1024     
Time steps               : 2048     
binomialOptionsGPU() time: 495.190002 msec
Options per second       : 2067.893122     
Running CPU binomial tree...
Comparing the results...
GPU binomial vs. Black-Scholes
L1 norm: 2.216577E-04
CPU binomial vs. Black-Scholes
L1 norm: 9.435265E-05
CPU binomial vs. GPU binomial
L1 norm: 1.513570E-04
Shutting down...

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

Test passed
****************************************************************
################################################################
5_Domain_Specific/convolutionFFT2D
################################################################
make: Nothing to be done for 'all'.
[./convolutionFFT2D] - Starting...
GPU Device 0: "Ampere" with compute capability 8.6

Testing built-in R2C / C2R FFT-based convolution
...allocating memory
...generating random input data
...creating R2C & C2R FFT plans for 2048 x 2048
...uploading to GPU and padding convolution kernel and input data
...transforming convolution kernel
...running GPU FFT convolution: 5333.333333 MPix/s (0.750000 ms)
...reading back GPU convolution results
...running reference CPU convolution
...comparing the results: rel L2 = 7.909606E-08 (max delta = 7.551771E-07)
L2norm Error OK
...shutting down
Testing custom R2C / C2R FFT-based convolution
...allocating memory
...generating random input data
...creating C2C FFT plan for 2048 x 1024
...uploading to GPU and padding convolution kernel and input data
...transforming convolution kernel
...running GPU FFT convolution: 4439.511541 MPix/s (0.901000 ms)
...reading back GPU FFT results
...running reference CPU convolution
...comparing the results: rel L2 = 9.417901E-08 (max delta = 7.642629E-07)
L2norm Error OK
...shutting down
Testing updated custom R2C / C2R FFT-based convolution
...allocating memory
...generating random input data
...creating C2C FFT plan for 2048 x 1024
...uploading to GPU and padding convolution kernel and input data
...transforming convolution kernel
...running GPU FFT convolution: 2172.732257 MPix/s (1.841000 ms)
...reading back GPU FFT results
...running reference CPU convolution
...comparing the results: rel L2 = 9.391089E-08 (max delta = 7.696280E-07)
L2norm Error OK
...shutting down
Test Summary: 0 errors
Test passed
****************************************************************
################################################################
5_Domain_Specific/dwtHaar1D
################################################################
make: Nothing to be done for 'all'.
./dwtHaar1D Starting...

GPU Device 0: "Ampere" with compute capability 8.6

source file    = "./data/signal.dat"
reference file = "result.dat"
gold file      = "./data/regression.gold.dat"
Reading signal from "./data/signal.dat"
Writing result to "result.dat"
Reading reference result from "./data/regression.gold.dat"
Test success!
****************************************************************
################################################################
5_Domain_Specific/dxtc
################################################################
make: Nothing to be done for 'all'.
./dxtc Starting...

GPU Device 0: "Ampere" with compute capability 8.6

Image Loaded './data/teapot512_std.ppm', 512 x 512 pixels

Running DXT Compression on 512 x 512 image...

16384 Blocks, 64 Threads per Block, 1048576 Threads in Grid...

dxtc, Throughput = 161.6178 MPixels/s, Time = 0.00162 s, Size = 262144 Pixels, NumDevsUsed = 1, Workgroup = 64

Checking accuracy...
RMS(reference, result) = 0.000000

Test passed
****************************************************************
################################################################
5_Domain_Specific/fastWalshTransform
################################################################
make: Nothing to be done for 'all'.
./fastWalshTransform Starting...

GPU Device 0: "Ampere" with compute capability 8.6

Initializing data...
...allocating CPU memory
...allocating GPU memory
...generating data
Data length: 8388608; kernel length: 128
Running GPU dyadic convolution using Fast Walsh Transform...
GPU time: 7.187000 ms; GOP/s: 40.268121
Reading back GPU results...
Running straightforward CPU dyadic convolution...
Comparing the results...
Shutting down...
L2 norm: 1.021579E-07
Test passed
****************************************************************
################################################################
5_Domain_Specific/fluidsD3D9
################################################################
>>> WARNING - fluidsD3D9 is not supported on Linux - waiving sample <<<
make: Nothing to be done for 'all'.
./all_projects.sh: line 1406: ./fluidsD3D9: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/fluidsGL
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at fluidsGL.cpp:467 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer(&cuda_vbo_resource, vbo, cudaGraphicsMapFlagsNone)" 
fluidsGL Starting...

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

GPU Device 0: "Ampere" with compute capability 8.6

CUDA device [NVIDIA GeForce RTX 3060] has 28 Multi-Processors
****************************************************************
################################################################
5_Domain_Specific/fluidsGLES
################################################################
>>> WARNING - fluidsGLES is not supported on Linux x86_64 - waiving sample <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o fluidsGLES.o -c fluidsGLES.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o fluidsGLES_kernels.o -c fluidsGLES_kernels.cu
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o fluidsGLES fluidsGLES.o fluidsGLES_kernels.o -lGLESv2 -lEGL -lX11 -lcufft
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp fluidsGLES ../../../bin/x86_64/linux/release
./all_projects.sh: line 1422: ./fluidsGLES: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/marchingCubes
################################################################
>>> GCC Version is greater or equal to 5.0.0 <<<
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at marchingCubes.cpp:476 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer( &cuda_posvbo_resource, posVbo, cudaGraphicsMapFlagsWriteDiscard)" 
[./marchingCubes] - Starting...
MarchingCubes
GPU Device 0: "Ampere" with compute capability 8.6

grid: 32 x 32 x 32 = 32768 voxels
max verts = 102400
Read './data/Bucky.raw', 32768 bytes
****************************************************************
################################################################
5_Domain_Specific/nbody
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at bodysystemcuda_impl.h:191 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer(&m_pGRes[i], m_pbo[i], cudaGraphicsMapFlagsNone)" 
Run "nbody -benchmark [-numbodies=<numBodies>]" to measure performance.
	-fullscreen       (run n-body simulation in fullscreen mode)
	-fp64             (use double precision floating point values for simulation)
	-hostmem          (stores simulation data in host memory)
	-benchmark        (run benchmark to measure performance) 
	-numbodies=<N>    (number of bodies (>= 1) to run in simulation) 
	-device=<d>       (where d=0,1,2.... for the CUDA device to use)
	-numdevices=<i>   (where i=(number of CUDA devices > 0) to use for simulation)
	-compare          (compares simulation results running once on the default GPU and once on the CPU)
	-cpu              (run n-body simulation on the CPU)
	-tipsy=<file.bin> (load a tipsy model file for simulation)

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

> Windowed mode
> Simulation data stored in video memory
> Single precision floating point simulation
> 1 Devices used for simulation
GPU Device 0: "Ampere" with compute capability 8.6

> Compute 8.6 CUDA device: [NVIDIA GeForce RTX 3060]
****************************************************************
################################################################
5_Domain_Specific/nbody_opengles
################################################################
>>> WARNING - nbody_opengles is not supported on Linux x86_64 - waiving sample <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 -ftz=true --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o bodysystemcuda.o -c bodysystemcuda.cu
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 -ftz=true --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o nbody_opengles.o -c nbody_opengles.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 -ftz=true --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o render_particles.o -c render_particles.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o nbody_opengles bodysystemcuda.o nbody_opengles.o render_particles.o -lGLESv2 -lEGL -lX11
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp nbody_opengles ../../../bin/x86_64/linux/release
./all_projects.sh: line 1446: ./nbody_opengles: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/nbody_screen
################################################################
>>> WARNING - nbody_screen is not supported on Linux x86_64 - waiving sample <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 -ftz=true --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o bodysystemcuda.o -c bodysystemcuda.cu
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 -ftz=true --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o nbody_screen.o -c nbody_screen.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 -ftz=true --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o render_particles.o -c render_particles.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o nbody_screen bodysystemcuda.o nbody_screen.o render_particles.o -lGLESv2 -lEGL -lscreen
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp nbody_screen ../../../bin/x86_64/linux/release
./all_projects.sh: line 1454: ./nbody_screen: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/p2pBandwidthLatencyTest
################################################################
make: Nothing to be done for 'all'.
[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]
Device: 0, NVIDIA GeForce RTX 3060, pciBusID: 1, pciDeviceID: 0, pciDomainID:0

***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.
So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.

P2P Connectivity Matrix
     D\D     0
     0	     1
Unidirectional P2P=Disabled Bandwidth Matrix (GB/s)
   D\D     0 
     0 329.99 
Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)
   D\D     0 
     0 331.81 
Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)
   D\D     0 
     0 273.79 
Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)
   D\D     0 
     0 273.31 
P2P=Disabled Latency Matrix (us)
   GPU     0 
     0   2.85 

   CPU     0 
     0   2.19 
P2P=Enabled Latency (P2P Writes) Matrix (us)
   GPU     0 
     0   2.39 

   CPU     0 
     0   2.04 

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
****************************************************************
################################################################
5_Domain_Specific/postProcessGL
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at main.cpp:264 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer(pbo_resource, *pbo, cudaGraphicsMapFlagsNone)" 
./postProcessGL Starting...

(Interactive OpenGL Demo)
GPU Device 0: "Ampere" with compute capability 8.6

****************************************************************
################################################################
5_Domain_Specific/quasirandomGenerator
################################################################
make: Nothing to be done for 'all'.
./quasirandomGenerator Starting...

Allocating GPU memory...
Allocating CPU memory...
Initializing QRNG tables...

Testing QRNG...

quasirandomGenerator, Throughput = 15.4127 GNumbers/s, Time = 0.00020 s, Size = 3145728 Numbers, NumDevsUsed = 1, Workgroup = 384

Reading GPU results...
Comparing to the CPU results...

L1 norm: 7.275964E-12

Testing inverseCNDgpu()...

quasirandomGenerator-inverse, Throughput = 66.4356 GNumbers/s, Time = 0.00005 s, Size = 3145728 Numbers, NumDevsUsed = 1, Workgroup = 128
Reading GPU results...

Comparing to the CPU results...
L1 norm: 9.439909E-08

Shutting down...
****************************************************************
################################################################
5_Domain_Specific/quasirandomGenerator_nvrtc
################################################################
make: Nothing to be done for 'all'.
./quasirandomGenerator_nvrtc Starting...

> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> Using CUDA Device [0]: NVIDIA GeForce RTX 3060
> GPU Device has SM 8.6 compute capability
Allocating GPU memory...
Allocating CPU memory...
Initializing QRNG tables...

Testing QRNG...

quasirandomGenerator, Throughput = 18.7581 GNumbers/s, Time = 0.00017 s, Size = 3145728 Numbers, NumDevsUsed = 1, Workgroup = 384

Reading GPU results...
Comparing to the CPU results...

L1 norm: 7.275964E-12

Testing inverseCNDgpu()...

quasirandomGenerator-inverse, Throughput = 30.8859 GNumbers/s, Time = 0.00010 s, Size = 3145728 Numbers, NumDevsUsed = 1, Workgroup = 128
Reading GPU results...

Comparing to the CPU results...
L1 norm: 9.439909E-08

Shutting down...
****************************************************************
################################################################
5_Domain_Specific/recursiveGaussian
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at recursiveGaussian.cpp:310 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer( &cuda_vbo_resource, pbo, cudaGraphicsRegisterFlagsWriteDiscard)" 
CUDA error at recursiveGaussian.cpp:210 code=400(cudaErrorInvalidResourceHandle) "cudaGraphicsUnregisterResource(cuda_vbo_resource)" 
CUDA Recursive Gaussian Starting...

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

Loaded './data/teapot512.ppm', 512 x 512 pixels
Press '+' and '-' to change filter width
0, 1, 2 - change filter order
GPU Device 0: "Ampere" with compute capability 8.6

****************************************************************
################################################################
5_Domain_Specific/simpleD3D10
################################################################
>>> WARNING - simpleD3D10 is not supported on Linux - waiving sample <<<
make: Nothing to be done for 'all'.
./all_projects.sh: line 1502: ./simpleD3D10: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/simpleD3D10RenderTarget
################################################################
>>> WARNING - simpleD3D10RenderTarget is not supported on Linux - waiving sample <<<
make: Nothing to be done for 'all'.
./all_projects.sh: line 1510: ./simpleD3D10RenderTarget: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/simpleD3D10Texture
################################################################
>>> WARNING - simpleD3D10Texture is not supported on Linux - waiving sample <<<
make: Nothing to be done for 'all'.
./all_projects.sh: line 1518: ./simpleD3D10Texture: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/simpleD3D11
################################################################
>>> WARNING - simpleD3D11 is not supported on Linux - waiving sample <<<
make: Nothing to be done for 'all'.
./all_projects.sh: line 1526: ./simpleD3D11: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/simpleD3D11Texture
################################################################
>>> WARNING - simpleD3D11Texture is not supported on Linux - waiving sample <<<
make: Nothing to be done for 'all'.
./all_projects.sh: line 1534: ./simpleD3D11Texture: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/simpleD3D12
################################################################
>>> WARNING - simpleD3D12 is not supported on Linux - waiving sample <<<
make: Nothing to be done for 'all'.
./all_projects.sh: line 1542: ./simpleD3D12: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/simpleD3D9
################################################################
>>> WARNING - simpleD3D9 is not supported on Linux - waiving sample <<<
make: Nothing to be done for 'all'.
./all_projects.sh: line 1550: ./simpleD3D9: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/simpleD3D9Texture
################################################################
>>> WARNING - simpleD3D9Texture is not supported on Linux - waiving sample <<<
make: Nothing to be done for 'all'.
./all_projects.sh: line 1558: ./simpleD3D9Texture: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/simpleGL
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at simpleGL.cu:421 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer(vbo_res, *vbo, vbo_res_flags)" 
CUDA error at simpleGL.cu:433 code=400(cudaErrorInvalidResourceHandle) "cudaGraphicsUnregisterResource(vbo_res)" 
simpleGL (VBO) starting...

GPU Device 0: "Ampere" with compute capability 8.6

****************************************************************
################################################################
5_Domain_Specific/simpleGLES
################################################################
>>> WARNING - simpleGLES is not supported on Linux x86_64 - waiving sample <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 -DUSE_CUDAINTEROP -DGRAPHICS_SETUP_EGL -DUSE_GLES --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o simpleGLES.o -c simpleGLES.cu
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o simpleGLES simpleGLES.o -lGLESv2 -lEGL -lX11
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp simpleGLES ../../../bin/x86_64/linux/release
./all_projects.sh: line 1574: ./simpleGLES: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/simpleGLES_EGLOutput
################################################################
>>> WARNING - simpleGLES_EGLOutput is not supported on Linux x86_64 - waiving sample <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 -DUSE_CUDAINTEROP -DGRAPHICS_SETUP_EGL -DUSE_GLES -I/usr/include/libdrm -I/usr/include/drm --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o simpleGLES_EGLOutput.o -c simpleGLES_EGLOutput.cu
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o simpleGLES_EGLOutput simpleGLES_EGLOutput.o -lGLESv2 -lEGL -ldrm
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp simpleGLES_EGLOutput ../../../bin/x86_64/linux/release
./all_projects.sh: line 1582: ./simpleGLES_EGLOutput: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/simpleGLES_screen
################################################################
>>> WARNING - simpleGLES_screen is not supported on Linux x86_64 - waiving sample <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 -DUSE_CUDAINTEROP -DGRAPHICS_SETUP_EGL -DUSE_GLES -DWIN_INTERFACE_CUSTOM --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o simpleGLES_screen.o -c simpleGLES_screen.cu
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o simpleGLES_screen simpleGLES_screen.o -lGLESv2 -lEGL -lscreen
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp simpleGLES_screen ../../../bin/x86_64/linux/release
./all_projects.sh: line 1590: ./simpleGLES_screen: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/simpleVulkan
################################################################
>>> WARNING - libvulkan.so not found, please install Vulkan SDK and pass VULKAN_SDK_PATH=<PATH_TO_VULKAN_SDK> <<<
>>> WARNING - vulkan.h not found, please install vulkan.h <<<
>>> GCC Version is greater or equal to 4.7.0 <<<
/bin/sh: 1: pkg-config: not found
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -I -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o SineWaveSimulation.o -c SineWaveSimulation.cu
/bin/sh: 1: pkg-config: not found
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -I -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o VulkanBaseApp.o -c VulkanBaseApp.cpp
/bin/sh: 1: pkg-config: not found
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -I -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o main.o -c main.cpp
/bin/sh: 1: pkg-config: not found
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o simpleVulkan SineWaveSimulation.o VulkanBaseApp.o main.o -L -lvulkan
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp simpleVulkan ../../../bin/x86_64/linux/release
./all_projects.sh: line 1598: ./simpleVulkan: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/simpleVulkanMMAP
################################################################
>>> WARNING - libvulkan.so not found, please install Vulkan SDK and pass VULKAN_SDK_PATH=<PATH_TO_VULKAN_SDK> <<<
>>> WARNING - vulkan.h not found, please install vulkan.h <<<
>>> GCC Version is greater or equal to 4.7.0 <<<
/bin/sh: 1: pkg-config: not found
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -I -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o helper_multiprocess.o -c ../../../Common/helper_multiprocess.cpp
/bin/sh: 1: pkg-config: not found
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -I -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o MonteCarloPi.o -c MonteCarloPi.cu
/bin/sh: 1: pkg-config: not found
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -I -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o VulkanBaseApp.o -c VulkanBaseApp.cpp
/bin/sh: 1: pkg-config: not found
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -I -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o main.o -c main.cpp
/bin/sh: 1: pkg-config: not found
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o simpleVulkanMMAP helper_multiprocess.o MonteCarloPi.o VulkanBaseApp.o main.o -L -lvulkan -L/usr/local/cuda/lib64/stubs -lcuda
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp simpleVulkanMMAP ../../../bin/x86_64/linux/release
./all_projects.sh: line 1606: ./simpleVulkanMMAP: No such file or directory
****************************************************************
################################################################
5_Domain_Specific/smokeParticles
################################################################
>>> GCC Version is greater or equal to 5.0.0 <<<
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
The following required OpenGL extensions missing:
	GL_ARB_multitexture
	GL_ARB_vertex_buffer_object
	GL_EXT_geometry_shader4.
CUDA Smoke Particles Starting...

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

****************************************************************
################################################################
5_Domain_Specific/stereoDisparity
################################################################
make: Nothing to be done for 'all'.
[stereoDisparity] Starting...

GPU Device 0: "Ampere" with compute capability 8.6

> GPU device has 28 Multi-Processors, SM 8.6 compute capabilities

Loaded <./data/stereo.im0.640x533.ppm> as image 0
Loaded <./data/stereo.im1.640x533.ppm> as image 1
Launching CUDA stereoDisparityKernel()
Input Size  [640x533], Kernel size [17x17], Disparities [-16:0]
GPU processing time : 0.8715 (ms)
Pixel throughput    : 391.437 Mpixels/sec
GPU Checksum = 4293895789, GPU image: <output_GPU.pgm>
Computing CPU reference...
CPU Checksum = 4293895789, CPU image: <output_CPU.pgm>
****************************************************************
################################################################
5_Domain_Specific/volumeFiltering
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at volumeFiltering.cpp:548 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer(&cuda_pbo_resource, pbo, cudaGraphicsMapFlagsWriteDiscard)" 
CUDA 3D Volume Filtering Starting...

GPU Device 0: "Ampere" with compute capability 8.6

Read './data/Bucky.raw', 32768 bytes
Press 
  'SPACE'     to toggle animation
  'p'         to toggle pre-integrated transfer function
  '+' and '-' to change density (0.01 increments)
  ']' and '[' to change brightness
  ';' and ''' to modify transfer function offset
  '.' and ',' to modify transfer function scale

****************************************************************
################################################################
5_Domain_Specific/volumeRender
################################################################
make: Nothing to be done for 'all'.
MESA: error: ZINK: failed to choose pdev
glx: failed to create drisw screen
CUDA error at volumeRender.cpp:424 code=304(cudaErrorOperatingSystem) "cudaGraphicsGLRegisterBuffer( &cuda_pbo_resource, pbo, cudaGraphicsMapFlagsWriteDiscard)" 
CUDA 3D Volume Render Starting...

GPU Device 0: "Ampere" with compute capability 8.6

Read './data/Bucky.raw', 32768 bytes
Press '+' and '-' to change density (0.01 increments)
      ']' and '[' to change brightness
      ';' and ''' to modify transfer function offset
      '.' and ',' to modify transfer function scale

****************************************************************
################################################################
5_Domain_Specific/vulkanImageCUDA
################################################################
>>> WARNING - libvulkan.so not found, please install Vulkan SDK and pass VULKAN_SDK_PATH=<PATH_TO_VULKAN_SDK> <<<
>>> WARNING - vulkan.h not found, please install vulkan.h <<<
>>> GCC Version is greater or equal to 4.7.0 <<<
/bin/sh: 1: pkg-config: not found
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -I -m64 --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o vulkanImageCUDA.o -c vulkanImageCUDA.cu
/bin/sh: 1: pkg-config: not found
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o vulkanImageCUDA vulkanImageCUDA.o -L -lvulkan
[@] mkdir -p ../../../bin/x86_64/linux/release
[@] cp vulkanImageCUDA ../../../bin/x86_64/linux/release
./all_projects.sh: line 1646: ./vulkanImageCUDA: No such file or directory
****************************************************************
################################################################
6_Performance/LargeKernelParameter
################################################################
make: Nothing to be done for 'all'.
Kernel 4KB parameter limit - time (us):157.97
Kernel 32,764 byte parameter limit - time (us):64.7604
Test passed!
****************************************************************
################################################################
6_Performance/UnifiedMemoryPerf
################################################################
make: Nothing to be done for 'all'.
GPU Device 0: "Ampere" with compute capability 8.6

Running ........................................................

Overall Time For matrixMultiplyPerf 

Printing Average of 20 measurements in (ms)
Size_KB	 UMhint	UMhntAs	 UMeasy	  0Copy	MemCopy	CpAsync	CpHpglk	CpPglAs
4	  0.091	  2.407	  0.102	  0.086	  0.167	  0.139	  0.287	  0.198
16	  0.103	  2.466	  0.211	  0.211	  0.703	  0.272	  0.617	  0.454
64	  0.240	  2.697	  0.276	  0.258	  0.739	  0.285	  0.635	  0.380
256	  0.899	  2.825	  0.993	  0.801	  0.707	  0.648	  0.601	  0.596
1024	  4.344	  5.854	  3.881	  4.132	  2.491	  2.872	  2.110	  2.969
4096	 27.808	 26.879	 24.054	 25.787	 10.747	  9.059	  6.683	  7.045
16384	206.211	204.759	249.723	194.749	 40.874	 42.306	 35.480	 34.845

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
****************************************************************
################################################################
6_Performance/alignedTypes
################################################################
make: Nothing to be done for 'all'.
[./alignedTypes] - Starting...
GPU Device 0: "Ampere" with compute capability 8.6

[NVIDIA GeForce RTX 3060] has 28 MP(s) x 128 (Cores/MP) = 3584 (Cores)
> Compute scaling value = 1.00
> Memory Size = 49999872
Allocating memory...
Generating host input data array...
Uploading input data to GPU memory...
Testing misaligned types...
uint8...
Avg. time: 1.070938 ms / Copy throughput: 43.481537 GB/s.
	TEST OK
uint16...
Avg. time: 0.520406 ms / Copy throughput: 89.480113 GB/s.
	TEST OK
RGBA8_misaligned...
Avg. time: 0.353125 ms / Copy throughput: 131.868343 GB/s.
	TEST OK
LA32_misaligned...
Avg. time: 0.311094 ms / Copy throughput: 149.684813 GB/s.
	TEST OK
RGB32_misaligned...
Avg. time: 0.306531 ms / Copy throughput: 151.912764 GB/s.
	TEST OK
RGBA32_misaligned...
Avg. time: 0.329406 ms / Copy throughput: 141.363462 GB/s.
	TEST OK
Testing aligned types...
RGBA8...
Avg. time: 0.429125 ms / Copy throughput: 108.513856 GB/s.
	TEST OK
I32...
Avg. time: 0.348906 ms / Copy throughput: 133.462813 GB/s.
	TEST OK
LA32...
Avg. time: 0.404687 ms / Copy throughput: 115.066589 GB/s.
	TEST OK
RGB32...
Avg. time: 0.388687 ms / Copy throughput: 119.803211 GB/s.
	TEST OK
RGBA32...
Avg. time: 0.300563 ms / Copy throughput: 154.929538 GB/s.
	TEST OK
RGBA32_2...
Avg. time: 0.323469 ms / Copy throughput: 143.958297 GB/s.
	TEST OK

[alignedTypes] -> Test Results: 0 Failures
Shutting down...
Test passed
****************************************************************
################################################################
6_Performance/cudaGraphsPerfScaling
################################################################
make: Nothing to be done for 'all'.
length, width, pattern, capture, instantiation, first_launch_api, first_launch_total, repeat_launch_api, repeat_launch_total, first_launch_device, blockingKernelTimeoutDetected, repeat_launch_device, blockingKernelTimeoutDetected, upload_api_time, updoad_device_time, blockingKernelTimeoutDetected, 
20, 1, 0, 15.906, 34.625, 11.735, 52.568, 3.006, 52.762, 15.360, 0.000, 14.336, 0.000, 16.391, 49.152, 0.000, 

****************************************************************
################################################################
6_Performance/transpose
################################################################
make: Nothing to be done for 'all'.
Transpose Starting...

GPU Device 0: "Ampere" with compute capability 8.6

> Device 0: "NVIDIA GeForce RTX 3060"
> SM Capability 8.6 detected:
> [NVIDIA GeForce RTX 3060] has 28 MP(s) x 128 (Cores/MP) = 3584 (Cores)
> Compute performance scaling factor = 1.00

Matrix size: 1024x1024 (64x64 tiles), tile size: 16x16, block size: 16x16

transpose simple copy       , Throughput = 277.3159 GB/s, Time = 0.02817 ms, Size = 1048576 fp32 elements, NumDevsUsed = 1, Workgroup = 256
transpose shared memory copy, Throughput = 272.1868 GB/s, Time = 0.02870 ms, Size = 1048576 fp32 elements, NumDevsUsed = 1, Workgroup = 256
transpose naive             , Throughput = 185.0896 GB/s, Time = 0.04221 ms, Size = 1048576 fp32 elements, NumDevsUsed = 1, Workgroup = 256
transpose coalesced         , Throughput = 264.9095 GB/s, Time = 0.02949 ms, Size = 1048576 fp32 elements, NumDevsUsed = 1, Workgroup = 256
transpose optimized         , Throughput = 269.1144 GB/s, Time = 0.02903 ms, Size = 1048576 fp32 elements, NumDevsUsed = 1, Workgroup = 256
transpose coarse-grained    , Throughput = 268.9247 GB/s, Time = 0.02905 ms, Size = 1048576 fp32 elements, NumDevsUsed = 1, Workgroup = 256
transpose fine-grained      , Throughput = 227.8792 GB/s, Time = 0.03428 ms, Size = 1048576 fp32 elements, NumDevsUsed = 1, Workgroup = 256
transpose diagonal          , Throughput = 238.9974 GB/s, Time = 0.03269 ms, Size = 1048576 fp32 elements, NumDevsUsed = 1, Workgroup = 256
Test passed
****************************************************************
################################################################
7_libNVVM
################################################################
-- Using CUDA_HOME: /usr/local/cuda
-- Using CUDA_LIB:  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so
-- Using LIBNVVM_HOME: /usr/local/cuda/nvvm
-- Using libnvvm header:      /usr/local/cuda/nvvm/include/nvvm.h
-- Using libnvvm header path: /usr/local/cuda/nvvm/include
-- Using libnvvm library:     /usr/local/cuda/nvvm/lib64/libnvvm.so
-- Using rpath: /usr/local/cuda/nvvm/lib64
-- Skipping the build of the cuda-c-linking sample.
-- Configuring done (2.5s)
-- Generating done (0.0s)
-- Build files have been written to: /home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/build
make[1]: Entering directory '/home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/build'
make[2]: Entering directory '/home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/build'
make[3]: Entering directory '/home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/build'
make[3]: Leaving directory '/home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/build'
[ 25%] Built target dsl
make[3]: Entering directory '/home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/build'
make[3]: Leaving directory '/home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/build'
[ 50%] Built target simple
make[3]: Entering directory '/home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/build'
make[3]: Leaving directory '/home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/build'
[ 75%] Built target ptxgen
make[3]: Entering directory '/home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/build'
make[3]: Leaving directory '/home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/build'
[100%] Built target uvmlite
make[2]: Leaving directory '/home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/build'
Install the project...
-- Install configuration: ""
-- Up-to-date: /home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/install/bin/dsl
-- Up-to-date: /home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/install/bin/dsl-gpu64.ll
-- Up-to-date: /home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/install/bin/simple
-- Up-to-date: /home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/install/bin/simple-gpu64.ll
-- Up-to-date: /home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/install/bin/ptxgen
-- Up-to-date: /home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/install/bin/uvmlite
-- Up-to-date: /home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/install/bin/uvmlite64.ll
make[1]: Leaving directory '/home/marco/code/marcoman/nvidia/cuda-samples/Samples/7_libNVVM/build'
****************************************************************
